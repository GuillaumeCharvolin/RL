_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.5926596522331238, Total_reward: 95.0
Epoch: 20, Loss: 0.9046231508255005, Total_reward: 105.0
Epoch: 30, Loss: 2.3695120811462402, Total_reward: 95.0
Epoch: 40, Loss: 3.6268913745880127, Total_reward: 97.0
Epoch: 50, Loss: 10.092339515686035, Total_reward: 132.0
Epoch: 60, Loss: 7.532546043395996, Total_reward: 95.0
Epoch: 70, Loss: 10.31145191192627, Total_reward: 106.0
Epoch: 80, Loss: 10.555920600891113, Total_reward: 98.0
Epoch: 90, Loss: 18.99883270263672, Total_reward: 100.0
Epoch: 100, Loss: 11.893534660339355, Total_reward: 136.0
Epoch: 110, Loss: 7.352075099945068, Total_reward: 118.0
Epoch: 120, Loss: 11.693535804748535, Total_reward: 118.0
Epoch: 130, Loss: 13.604312896728516, Total_reward: 133.0
Epoch: 140, Loss: 20.262983322143555, Total_reward: 143.0
Epoch: 150, Loss: 7.805176258087158, Total_reward: 125.0
Epoch: 160, Loss: 18.08725929260254, Total_reward: 109.0
Epoch: 170, Loss: 12.263310432434082, Total_reward: 126.0
Epoch: 180, Loss: 10.200242042541504, Total_reward: 120.0
Epoch: 190, Loss: 21.88523292541504, Total_reward: 122.0
Epoch: 200, Loss: 12.16706371307373, Total_reward: 96.0
Epoch: 210, Loss: 14.428800582885742, Total_reward: 146.0
Epoch: 220, Loss: 20.51673126220703, Total_reward: 112.0
Epoch: 230, Loss: 13.733467102050781, Total_reward: 127.0
Epoch: 240, Loss: 18.80301856994629, Total_reward: 116.0
Epoch: 250, Loss: 37.13246536254883, Total_reward: 116.0
Epoch: 260, Loss: 27.7069149017334, Total_reward: 116.0
Epoch: 270, Loss: 18.713624954223633, Total_reward: 113.0
Epoch: 280, Loss: 17.243791580200195, Total_reward: 150.0
Epoch: 290, Loss: 15.729381561279297, Total_reward: 103.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.08173807710409164, Total_reward: 110.0
Epoch: 20, Loss: 0.10929853469133377, Total_reward: 133.0
Epoch: 30, Loss: 0.08426307886838913, Total_reward: 123.0
Epoch: 40, Loss: 0.23213519155979156, Total_reward: 129.0
Epoch: 50, Loss: 1.0780847072601318, Total_reward: 130.0
Epoch: 60, Loss: 0.613964319229126, Total_reward: 124.0
Epoch: 70, Loss: 0.8987045288085938, Total_reward: 130.0
Epoch: 80, Loss: 0.7912489175796509, Total_reward: 123.0
Epoch: 90, Loss: 0.8299009799957275, Total_reward: 130.0
Epoch: 100, Loss: 0.44496315717697144, Total_reward: 165.0
Epoch: 110, Loss: 0.7012913823127747, Total_reward: 163.0
Epoch: 120, Loss: 1.2168227434158325, Total_reward: 116.0
Epoch: 130, Loss: 2.065253257751465, Total_reward: 121.0
Epoch: 140, Loss: 2.015172243118286, Total_reward: 134.0
Epoch: 150, Loss: 1.9866455793380737, Total_reward: 124.0
Epoch: 160, Loss: 0.9194236397743225, Total_reward: 126.0
Epoch: 170, Loss: 4.05479621887207, Total_reward: 115.0
Epoch: 180, Loss: 4.072221279144287, Total_reward: 149.0
Epoch: 190, Loss: 4.337039947509766, Total_reward: 117.0
Epoch: 200, Loss: 9.059784889221191, Total_reward: 109.0
Epoch: 210, Loss: 5.347538948059082, Total_reward: 127.0
Epoch: 220, Loss: 6.647027969360352, Total_reward: 127.0
Epoch: 230, Loss: 4.888329982757568, Total_reward: 126.0
Epoch: 240, Loss: 10.456143379211426, Total_reward: 113.0
Epoch: 250, Loss: 7.190518856048584, Total_reward: 87.0
Epoch: 260, Loss: 11.611855506896973, Total_reward: 101.0
Epoch: 270, Loss: 12.337302207946777, Total_reward: 121.0
Epoch: 280, Loss: 6.396993637084961, Total_reward: 141.0
Epoch: 290, Loss: 11.475783348083496, Total_reward: 128.0
Average reward over 10 runs (invisible): 11.1
result : 11.1
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.007641373202204704, Total_reward: 127.0
Epoch: 20, Loss: 0.5833156704902649, Total_reward: 127.0
Epoch: 30, Loss: 0.9557532072067261, Total_reward: 90.0
Epoch: 40, Loss: 3.9908580780029297, Total_reward: 150.0
Epoch: 50, Loss: 2.693298578262329, Total_reward: 142.0
Epoch: 60, Loss: 5.623165130615234, Total_reward: 132.0
Epoch: 70, Loss: 5.572511196136475, Total_reward: 148.0
Epoch: 80, Loss: 7.863890171051025, Total_reward: 133.0
Epoch: 90, Loss: 8.86758041381836, Total_reward: 152.0
Epoch: 100, Loss: 5.286463260650635, Total_reward: 130.0
Epoch: 110, Loss: 13.027395248413086, Total_reward: 132.0
Epoch: 120, Loss: 9.584129333496094, Total_reward: 108.0
Epoch: 130, Loss: 13.927385330200195, Total_reward: 91.0
Epoch: 140, Loss: 11.165024757385254, Total_reward: 113.0
Epoch: 150, Loss: 19.50254249572754, Total_reward: 111.0
Epoch: 160, Loss: 24.1702823638916, Total_reward: 107.0
Epoch: 170, Loss: 13.139644622802734, Total_reward: 152.0
Epoch: 180, Loss: 4.1442365646362305, Total_reward: 145.0
Epoch: 190, Loss: 10.930710792541504, Total_reward: 138.0
Epoch: 200, Loss: 13.090729713439941, Total_reward: 98.0
Epoch: 210, Loss: 18.82267951965332, Total_reward: 105.0
Epoch: 220, Loss: 9.469467163085938, Total_reward: 102.0
Epoch: 230, Loss: 21.796329498291016, Total_reward: 97.0
Epoch: 240, Loss: 26.13730239868164, Total_reward: 95.0
Epoch: 250, Loss: 8.270744323730469, Total_reward: 98.0
Epoch: 260, Loss: 18.976011276245117, Total_reward: 116.0
Epoch: 270, Loss: 22.58431053161621, Total_reward: 81.0
Epoch: 280, Loss: 17.19118881225586, Total_reward: 126.0
Epoch: 290, Loss: 16.728254318237305, Total_reward: 158.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.08398228138685226, Total_reward: 147.0
Epoch: 20, Loss: 0.0048484522849321365, Total_reward: 128.0
Epoch: 30, Loss: 0.08772429078817368, Total_reward: 135.0
Epoch: 40, Loss: 0.24809017777442932, Total_reward: 129.0
Epoch: 50, Loss: 0.59941166639328, Total_reward: 128.0
Epoch: 60, Loss: 0.30362191796302795, Total_reward: 134.0
Epoch: 70, Loss: 0.8240479230880737, Total_reward: 121.0
Epoch: 80, Loss: 0.8767616748809814, Total_reward: 105.0
Epoch: 90, Loss: 1.1304776668548584, Total_reward: 136.0
Epoch: 100, Loss: 0.977975070476532, Total_reward: 112.0
Epoch: 110, Loss: 2.376530170440674, Total_reward: 120.0
Epoch: 120, Loss: 2.7955739498138428, Total_reward: 100.0
Epoch: 130, Loss: 3.426588535308838, Total_reward: 95.0
Epoch: 140, Loss: 1.8836215734481812, Total_reward: 104.0
Epoch: 150, Loss: 3.9058609008789062, Total_reward: 104.0
Epoch: 160, Loss: 3.137068510055542, Total_reward: 119.0
Epoch: 170, Loss: 5.283267974853516, Total_reward: 92.0
Epoch: 180, Loss: 4.030765056610107, Total_reward: 85.0
Epoch: 190, Loss: 4.035441875457764, Total_reward: 112.0
Epoch: 200, Loss: 6.087578773498535, Total_reward: 118.0
Epoch: 210, Loss: 7.630935192108154, Total_reward: 111.0
Epoch: 220, Loss: 4.758863925933838, Total_reward: 111.0
Epoch: 230, Loss: 4.413689613342285, Total_reward: 138.0
Epoch: 240, Loss: 5.520891189575195, Total_reward: 120.0
Epoch: 250, Loss: 6.126522541046143, Total_reward: 115.0
Epoch: 260, Loss: 4.640512943267822, Total_reward: 102.0
Epoch: 270, Loss: 5.383736610412598, Total_reward: 126.0
Epoch: 280, Loss: 2.9352059364318848, Total_reward: 128.0
Epoch: 290, Loss: 8.536349296569824, Total_reward: 85.0
Average reward over 10 runs (invisible): 9.3
result : 9.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.25824716687202454, Total_reward: 112.0
Epoch: 20, Loss: 1.180192470550537, Total_reward: 130.0
Epoch: 30, Loss: 2.9175093173980713, Total_reward: 109.0
Epoch: 40, Loss: 6.507699489593506, Total_reward: 122.0
Epoch: 50, Loss: 5.565006256103516, Total_reward: 119.0
Epoch: 60, Loss: 6.876035213470459, Total_reward: 146.0
Epoch: 70, Loss: 8.459677696228027, Total_reward: 112.0
Epoch: 80, Loss: 11.520359992980957, Total_reward: 84.0
Epoch: 90, Loss: 8.690996170043945, Total_reward: 106.0
Epoch: 100, Loss: 18.37759017944336, Total_reward: 141.0
Epoch: 110, Loss: 5.196537017822266, Total_reward: 144.0
Epoch: 120, Loss: 7.486794471740723, Total_reward: 120.0
Epoch: 130, Loss: 7.571139335632324, Total_reward: 139.0
Epoch: 140, Loss: 12.590850830078125, Total_reward: 148.0
Epoch: 150, Loss: 13.034188270568848, Total_reward: 200.0
Epoch: 160, Loss: 6.893970966339111, Total_reward: 148.0
Epoch: 170, Loss: 11.736653327941895, Total_reward: 122.0
Epoch: 180, Loss: 17.647306442260742, Total_reward: 153.0
Epoch: 190, Loss: 12.036420822143555, Total_reward: 173.0
Epoch: 200, Loss: 18.658395767211914, Total_reward: 137.0
Epoch: 210, Loss: 16.46224594116211, Total_reward: 121.0
Epoch: 220, Loss: 5.89065408706665, Total_reward: 125.0
Epoch: 230, Loss: 17.71213150024414, Total_reward: 119.0
Epoch: 240, Loss: 26.37164306640625, Total_reward: 116.0
Epoch: 250, Loss: 28.17433738708496, Total_reward: 106.0
Epoch: 260, Loss: 32.520389556884766, Total_reward: 85.0
Epoch: 270, Loss: 30.030057907104492, Total_reward: 88.0
Epoch: 280, Loss: 31.044607162475586, Total_reward: 124.0
Epoch: 290, Loss: 25.78569221496582, Total_reward: 141.0
Average reward over 10 runs (invisible): 81.5
result : 81.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.34997016191482544, Total_reward: 111.0
Epoch: 20, Loss: 0.3419050872325897, Total_reward: 98.0
Epoch: 30, Loss: 0.5413953065872192, Total_reward: 174.0
Epoch: 40, Loss: 1.2234036922454834, Total_reward: 120.0
Epoch: 50, Loss: 1.6346620321273804, Total_reward: 187.0
Epoch: 60, Loss: 1.335775375366211, Total_reward: 191.0
Epoch: 70, Loss: 1.5710482597351074, Total_reward: 145.0
Epoch: 80, Loss: 1.2963733673095703, Total_reward: 119.0
Epoch: 90, Loss: 2.8356313705444336, Total_reward: 260.0
Epoch: 100, Loss: 2.2836155891418457, Total_reward: 168.0
Epoch: 110, Loss: 2.9317264556884766, Total_reward: 161.0
Epoch: 120, Loss: 2.63057017326355, Total_reward: 174.0
Epoch: 130, Loss: 2.0997016429901123, Total_reward: 149.0
Epoch: 140, Loss: 4.366497993469238, Total_reward: 160.0
Epoch: 150, Loss: 2.6268601417541504, Total_reward: 98.0
Epoch: 160, Loss: 7.158560752868652, Total_reward: 88.0
Epoch: 170, Loss: 5.15081262588501, Total_reward: 115.0
Epoch: 180, Loss: 6.092906951904297, Total_reward: 148.0
Epoch: 190, Loss: 5.879751205444336, Total_reward: 116.0
Epoch: 200, Loss: 5.667992115020752, Total_reward: 237.0
Epoch: 210, Loss: 2.18477463722229, Total_reward: 156.0
Epoch: 220, Loss: 3.037710428237915, Total_reward: 159.0
Epoch: 230, Loss: 7.113635063171387, Total_reward: 98.0
Epoch: 240, Loss: 9.032687187194824, Total_reward: 113.0
Epoch: 250, Loss: 16.288249969482422, Total_reward: 117.0
Epoch: 260, Loss: 8.521629333496094, Total_reward: 110.0
Epoch: 270, Loss: 13.015865325927734, Total_reward: 95.0
Epoch: 280, Loss: 10.662123680114746, Total_reward: 89.0
Epoch: 290, Loss: 13.45464038848877, Total_reward: 83.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.12578870356082916, Total_reward: 130.0
Epoch: 20, Loss: 0.27738821506500244, Total_reward: 147.0
Epoch: 30, Loss: 1.056828260421753, Total_reward: 157.0
Epoch: 40, Loss: 3.631157875061035, Total_reward: 131.0
Epoch: 50, Loss: 4.95841646194458, Total_reward: 134.0
Epoch: 60, Loss: 4.296313285827637, Total_reward: 135.0
Epoch: 70, Loss: 3.900223731994629, Total_reward: 125.0
Epoch: 80, Loss: 6.055506706237793, Total_reward: 117.0
Epoch: 90, Loss: 6.833912372589111, Total_reward: 144.0
Epoch: 100, Loss: 16.03713607788086, Total_reward: 126.0
Epoch: 110, Loss: 21.09220314025879, Total_reward: 87.0
Epoch: 120, Loss: 15.031906127929688, Total_reward: 89.0
Epoch: 130, Loss: 17.79195213317871, Total_reward: 84.0
Epoch: 140, Loss: 9.821555137634277, Total_reward: 85.0
Epoch: 150, Loss: 15.955190658569336, Total_reward: 85.0
Epoch: 160, Loss: 11.715715408325195, Total_reward: 88.0
Epoch: 170, Loss: 9.405405044555664, Total_reward: 88.0
Epoch: 180, Loss: 15.41379451751709, Total_reward: 94.0
Epoch: 190, Loss: 15.193754196166992, Total_reward: 118.0
Epoch: 200, Loss: 14.78476619720459, Total_reward: 138.0
Epoch: 210, Loss: 7.663544178009033, Total_reward: 219.0
Epoch: 220, Loss: 13.635388374328613, Total_reward: 154.0
Epoch: 230, Loss: 8.956489562988281, Total_reward: 170.0
Epoch: 240, Loss: 27.134889602661133, Total_reward: 133.0
Epoch: 250, Loss: 33.89947509765625, Total_reward: 118.0
Epoch: 260, Loss: 20.16997528076172, Total_reward: 150.0
Epoch: 270, Loss: 12.22396183013916, Total_reward: 167.0
Epoch: 280, Loss: 24.371801376342773, Total_reward: 133.0
Epoch: 290, Loss: 13.489258766174316, Total_reward: 146.0
Average reward over 10 runs (invisible): 11.1
result : 11.1
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.04131004586815834, Total_reward: 99.0
Epoch: 20, Loss: 0.007147952914237976, Total_reward: 201.0
Epoch: 30, Loss: 0.23526586592197418, Total_reward: 148.0
Epoch: 40, Loss: 0.45758387446403503, Total_reward: 155.0
Epoch: 50, Loss: 0.6648004055023193, Total_reward: 114.0
Epoch: 60, Loss: 0.8280408978462219, Total_reward: 109.0
Epoch: 70, Loss: 2.1434812545776367, Total_reward: 105.0
Epoch: 80, Loss: 1.2539650201797485, Total_reward: 97.0
Epoch: 90, Loss: 2.093628168106079, Total_reward: 112.0
Epoch: 100, Loss: 2.4886634349823, Total_reward: 131.0
Epoch: 110, Loss: 2.0774033069610596, Total_reward: 97.0
Epoch: 120, Loss: 2.1813368797302246, Total_reward: 114.0
Epoch: 130, Loss: 3.003657102584839, Total_reward: 215.0
Epoch: 140, Loss: 2.913325786590576, Total_reward: 153.0
Epoch: 150, Loss: 0.9489047527313232, Total_reward: 147.0
Epoch: 160, Loss: 2.8501405715942383, Total_reward: 155.0
Epoch: 170, Loss: 4.711094856262207, Total_reward: 214.0
Epoch: 180, Loss: 1.8796402215957642, Total_reward: 163.0
Epoch: 190, Loss: 3.989025115966797, Total_reward: 112.0
Epoch: 200, Loss: 4.687406539916992, Total_reward: 107.0
Epoch: 210, Loss: 5.667159080505371, Total_reward: 190.0
Epoch: 220, Loss: 5.064326286315918, Total_reward: 123.0
Epoch: 230, Loss: 6.294386863708496, Total_reward: 108.0
Epoch: 240, Loss: 5.64865779876709, Total_reward: 174.0
Epoch: 250, Loss: 4.960618495941162, Total_reward: 121.0
Epoch: 260, Loss: 2.060340166091919, Total_reward: 121.0
Epoch: 270, Loss: 9.130062103271484, Total_reward: 129.0
Epoch: 280, Loss: 8.975875854492188, Total_reward: 137.0
Epoch: 290, Loss: 4.709165573120117, Total_reward: 133.0
Average reward over 10 runs (invisible): 10.0
result : 10.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1301339715719223, Total_reward: 112.0
Epoch: 20, Loss: 0.3939085900783539, Total_reward: 139.0
Epoch: 30, Loss: 2.1306850910186768, Total_reward: 97.0
Epoch: 40, Loss: 4.907532691955566, Total_reward: 100.0
Epoch: 50, Loss: 1.9427381753921509, Total_reward: 110.0
Epoch: 60, Loss: 4.973625659942627, Total_reward: 105.0
Epoch: 70, Loss: 3.7346625328063965, Total_reward: 123.0
Epoch: 80, Loss: 6.261460304260254, Total_reward: 109.0
Epoch: 90, Loss: 2.7609081268310547, Total_reward: 120.0
Epoch: 100, Loss: 12.6521577835083, Total_reward: 116.0
Epoch: 110, Loss: 12.404013633728027, Total_reward: 117.0
Epoch: 120, Loss: 7.129008769989014, Total_reward: 126.0
Epoch: 130, Loss: 6.046640396118164, Total_reward: 102.0
Epoch: 140, Loss: 22.200946807861328, Total_reward: 113.0
Epoch: 150, Loss: 22.39313507080078, Total_reward: 122.0
Epoch: 160, Loss: 10.389447212219238, Total_reward: 123.0
Epoch: 170, Loss: 11.202957153320312, Total_reward: 97.0
Epoch: 180, Loss: 24.381731033325195, Total_reward: 99.0
Epoch: 190, Loss: 30.498310089111328, Total_reward: 132.0
Epoch: 200, Loss: 23.87099266052246, Total_reward: 119.0
Epoch: 210, Loss: 19.782020568847656, Total_reward: 153.0
Epoch: 220, Loss: 16.0354061126709, Total_reward: 118.0
Epoch: 230, Loss: 8.200174331665039, Total_reward: 149.0
Epoch: 240, Loss: 9.091479301452637, Total_reward: 151.0
Epoch: 250, Loss: 26.588979721069336, Total_reward: 106.0
Epoch: 260, Loss: 23.671560287475586, Total_reward: 103.0
Epoch: 270, Loss: 23.939172744750977, Total_reward: 143.0
Epoch: 280, Loss: 14.701637268066406, Total_reward: 107.0
Epoch: 290, Loss: 9.78817367553711, Total_reward: 113.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1808384358882904, Total_reward: 104.0
Epoch: 20, Loss: 0.19163620471954346, Total_reward: 88.0
Epoch: 30, Loss: 0.3867030143737793, Total_reward: 89.0
Epoch: 40, Loss: 0.8692868947982788, Total_reward: 99.0
Epoch: 50, Loss: 1.4373996257781982, Total_reward: 112.0
Epoch: 60, Loss: 1.1688802242279053, Total_reward: 128.0
Epoch: 70, Loss: 1.0435676574707031, Total_reward: 111.0
Epoch: 80, Loss: 2.3004114627838135, Total_reward: 98.0
Epoch: 90, Loss: 1.4566069841384888, Total_reward: 96.0
Epoch: 100, Loss: 3.414098024368286, Total_reward: 101.0
Epoch: 110, Loss: 4.092767715454102, Total_reward: 134.0
Epoch: 120, Loss: 3.4826478958129883, Total_reward: 124.0
Epoch: 130, Loss: 2.361253261566162, Total_reward: 152.0
Epoch: 140, Loss: 1.4667048454284668, Total_reward: 143.0
Epoch: 150, Loss: 7.288454055786133, Total_reward: 129.0
Epoch: 160, Loss: 4.522151947021484, Total_reward: 128.0
Epoch: 170, Loss: 2.0935161113739014, Total_reward: 131.0
Epoch: 180, Loss: 2.1110033988952637, Total_reward: 130.0
Epoch: 190, Loss: 6.277563095092773, Total_reward: 113.0
Epoch: 200, Loss: 4.689752578735352, Total_reward: 127.0
Epoch: 210, Loss: 11.165559768676758, Total_reward: 110.0
Epoch: 220, Loss: 3.9653096199035645, Total_reward: 114.0
Epoch: 230, Loss: 4.756641387939453, Total_reward: 110.0
Epoch: 240, Loss: 9.713997840881348, Total_reward: 111.0
Epoch: 250, Loss: 7.456376552581787, Total_reward: 112.0
Epoch: 260, Loss: 5.9804487228393555, Total_reward: 115.0
Epoch: 270, Loss: 2.591294288635254, Total_reward: 88.0
Epoch: 280, Loss: 8.873793601989746, Total_reward: 113.0
Epoch: 290, Loss: 3.6940464973449707, Total_reward: 121.0
Average reward over 10 runs (invisible): 10.9
result : 10.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.036907847970724106, Total_reward: 114.0
Epoch: 20, Loss: 0.6796267032623291, Total_reward: 106.0
Epoch: 30, Loss: 0.6666885018348694, Total_reward: 125.0
Epoch: 40, Loss: 1.2304487228393555, Total_reward: 117.0
Epoch: 50, Loss: 1.4582459926605225, Total_reward: 105.0
Epoch: 60, Loss: 4.169795989990234, Total_reward: 138.0
Epoch: 70, Loss: 7.320902347564697, Total_reward: 140.0
Epoch: 80, Loss: 11.224857330322266, Total_reward: 123.0
Epoch: 90, Loss: 3.4877805709838867, Total_reward: 143.0
Epoch: 100, Loss: 13.68552303314209, Total_reward: 115.0
Epoch: 110, Loss: 17.65241050720215, Total_reward: 126.0
Epoch: 120, Loss: 17.917850494384766, Total_reward: 131.0
Epoch: 130, Loss: 28.90442657470703, Total_reward: 142.0
Epoch: 140, Loss: 12.890103340148926, Total_reward: 104.0
Epoch: 150, Loss: 18.21544075012207, Total_reward: 140.0
Epoch: 160, Loss: 16.97918701171875, Total_reward: 151.0
Epoch: 170, Loss: 15.226375579833984, Total_reward: 130.0
Epoch: 180, Loss: 8.573869705200195, Total_reward: 120.0
Epoch: 190, Loss: 17.585357666015625, Total_reward: 113.0
Epoch: 200, Loss: 24.97099494934082, Total_reward: 98.0
Epoch: 210, Loss: 14.578508377075195, Total_reward: 91.0
Epoch: 220, Loss: 24.222087860107422, Total_reward: 124.0
Epoch: 230, Loss: 11.214879989624023, Total_reward: 120.0
Epoch: 240, Loss: 31.74332046508789, Total_reward: 94.0
Epoch: 250, Loss: 30.285371780395508, Total_reward: 120.0
Epoch: 260, Loss: 9.909603118896484, Total_reward: 99.0
Epoch: 270, Loss: 35.79680633544922, Total_reward: 105.0
Epoch: 280, Loss: 26.600509643554688, Total_reward: 86.0
Epoch: 290, Loss: 34.453651428222656, Total_reward: 113.0
Average reward over 10 runs (invisible): 9.8
result : 9.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.07265818864107132, Total_reward: 102.0
Epoch: 20, Loss: 0.05204221233725548, Total_reward: 87.0
Epoch: 30, Loss: 0.47110843658447266, Total_reward: 96.0
Epoch: 40, Loss: 0.4016703963279724, Total_reward: 86.0
Epoch: 50, Loss: 1.0115926265716553, Total_reward: 108.0
Epoch: 60, Loss: 0.819459855556488, Total_reward: 151.0
Epoch: 70, Loss: 1.9348640441894531, Total_reward: 128.0
Epoch: 80, Loss: 2.274826765060425, Total_reward: 166.0
Epoch: 90, Loss: 2.3956074714660645, Total_reward: 131.0
Epoch: 100, Loss: 0.5515097379684448, Total_reward: 175.0
Epoch: 110, Loss: 0.7344127893447876, Total_reward: 136.0
Epoch: 120, Loss: 2.522498846054077, Total_reward: 144.0
Epoch: 130, Loss: 2.441016435623169, Total_reward: 115.0
Epoch: 140, Loss: 1.6109931468963623, Total_reward: 90.0
Epoch: 150, Loss: 2.121933937072754, Total_reward: 92.0
Epoch: 160, Loss: 2.917996883392334, Total_reward: 143.0
Epoch: 170, Loss: 5.605911731719971, Total_reward: 123.0
Epoch: 180, Loss: 8.05473518371582, Total_reward: 119.0
Epoch: 190, Loss: 5.4987101554870605, Total_reward: 114.0
Epoch: 200, Loss: 6.442551612854004, Total_reward: 94.0
Epoch: 210, Loss: 5.438299179077148, Total_reward: 112.0
Epoch: 220, Loss: 13.096216201782227, Total_reward: 99.0
Epoch: 230, Loss: 5.6275177001953125, Total_reward: 162.0
Epoch: 240, Loss: 3.0482983589172363, Total_reward: 160.0
Epoch: 250, Loss: 9.051098823547363, Total_reward: 142.0
Epoch: 260, Loss: 4.753895282745361, Total_reward: 131.0
Epoch: 270, Loss: 9.878395080566406, Total_reward: 120.0
Epoch: 280, Loss: 1.578356385231018, Total_reward: 142.0
Epoch: 290, Loss: 10.653861999511719, Total_reward: 139.0
Average reward over 10 runs (invisible): 9.7
result : 9.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.015923546627163887, Total_reward: 156.0
Epoch: 20, Loss: 0.7630902528762817, Total_reward: 173.0
Epoch: 30, Loss: 1.2397557497024536, Total_reward: 135.0
Epoch: 40, Loss: 2.2214441299438477, Total_reward: 108.0
Epoch: 50, Loss: 5.799449443817139, Total_reward: 124.0
Epoch: 60, Loss: 8.713678359985352, Total_reward: 134.0
Epoch: 70, Loss: 2.458726644515991, Total_reward: 107.0
Epoch: 80, Loss: 9.869379997253418, Total_reward: 97.0
Epoch: 90, Loss: 5.77886438369751, Total_reward: 127.0
Epoch: 100, Loss: 7.59359073638916, Total_reward: 112.0
Epoch: 110, Loss: 15.131043434143066, Total_reward: 121.0
Epoch: 120, Loss: 21.635107040405273, Total_reward: 115.0
Epoch: 130, Loss: 9.272462844848633, Total_reward: 126.0
Epoch: 140, Loss: 16.201122283935547, Total_reward: 102.0
Epoch: 150, Loss: 14.600265502929688, Total_reward: 100.0
Epoch: 160, Loss: 8.518346786499023, Total_reward: 88.0
Epoch: 170, Loss: 14.39218521118164, Total_reward: 101.0
Epoch: 180, Loss: 15.470697402954102, Total_reward: 102.0
Epoch: 190, Loss: 11.343855857849121, Total_reward: 104.0
Epoch: 200, Loss: 18.53212547302246, Total_reward: 95.0
Epoch: 210, Loss: 6.960644721984863, Total_reward: 151.0
Epoch: 220, Loss: 9.030055046081543, Total_reward: 120.0
Epoch: 230, Loss: 23.144933700561523, Total_reward: 116.0
Epoch: 240, Loss: 15.131722450256348, Total_reward: 182.0
Epoch: 250, Loss: 6.530388832092285, Total_reward: 195.0
Epoch: 260, Loss: 16.004243850708008, Total_reward: 99.0
Epoch: 270, Loss: 14.04738712310791, Total_reward: 143.0
Epoch: 280, Loss: 16.746353149414062, Total_reward: 208.0
Epoch: 290, Loss: 30.078245162963867, Total_reward: 146.0
Average reward over 10 runs (invisible): 13.0
result : 13.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.012609736993908882, Total_reward: 117.0
Epoch: 20, Loss: 0.026602577418088913, Total_reward: 100.0
Epoch: 30, Loss: 0.3468601107597351, Total_reward: 141.0
Epoch: 40, Loss: 0.2566036880016327, Total_reward: 131.0
Epoch: 50, Loss: 0.44406604766845703, Total_reward: 118.0
Epoch: 60, Loss: 0.49017956852912903, Total_reward: 131.0
Epoch: 70, Loss: 2.1885032653808594, Total_reward: 113.0
Epoch: 80, Loss: 1.6844687461853027, Total_reward: 108.0
Epoch: 90, Loss: 1.5968501567840576, Total_reward: 83.0
Epoch: 100, Loss: 2.658731698989868, Total_reward: 128.0
Epoch: 110, Loss: 3.6556777954101562, Total_reward: 206.0
Epoch: 120, Loss: 1.8738831281661987, Total_reward: 177.0
Epoch: 130, Loss: 1.2618889808654785, Total_reward: 100.0
Epoch: 140, Loss: 3.364938497543335, Total_reward: 150.0
Epoch: 150, Loss: 2.1024763584136963, Total_reward: 200.0
Epoch: 160, Loss: 4.8831257820129395, Total_reward: 128.0
Epoch: 170, Loss: 4.691494464874268, Total_reward: 112.0
Epoch: 180, Loss: 6.299795150756836, Total_reward: 122.0
Epoch: 190, Loss: 3.194356679916382, Total_reward: 92.0
Epoch: 200, Loss: 4.30417013168335, Total_reward: 164.0
Epoch: 210, Loss: 4.657588481903076, Total_reward: 173.0
Epoch: 220, Loss: 2.6382994651794434, Total_reward: 171.0
Epoch: 230, Loss: 4.285375595092773, Total_reward: 120.0
Epoch: 240, Loss: 5.471925258636475, Total_reward: 157.0
Epoch: 250, Loss: 7.185230255126953, Total_reward: 106.0
Epoch: 260, Loss: 7.51716423034668, Total_reward: 101.0
Epoch: 270, Loss: 4.866909503936768, Total_reward: 123.0
Epoch: 280, Loss: 4.317758083343506, Total_reward: 177.0
Epoch: 290, Loss: 6.492917537689209, Total_reward: 148.0
Average reward over 10 runs (invisible): 25.9
result : 25.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.19040539860725403, Total_reward: 111.0
Epoch: 20, Loss: 0.9290274381637573, Total_reward: 101.0
Epoch: 30, Loss: 1.966705560684204, Total_reward: 97.0
Epoch: 40, Loss: 4.3896074295043945, Total_reward: 113.0
Epoch: 50, Loss: 4.537196159362793, Total_reward: 98.0
Epoch: 60, Loss: 4.150622844696045, Total_reward: 97.0
Epoch: 70, Loss: 9.887017250061035, Total_reward: 147.0
Epoch: 80, Loss: 6.699246406555176, Total_reward: 114.0
Epoch: 90, Loss: 14.745763778686523, Total_reward: 123.0
Epoch: 100, Loss: 5.913265228271484, Total_reward: 155.0
Epoch: 110, Loss: 8.468914985656738, Total_reward: 163.0
Epoch: 120, Loss: 10.536459922790527, Total_reward: 90.0
Epoch: 130, Loss: 3.0503480434417725, Total_reward: 99.0
Epoch: 140, Loss: 9.39228343963623, Total_reward: 109.0
Epoch: 150, Loss: 9.275726318359375, Total_reward: 105.0
Epoch: 160, Loss: 11.707602500915527, Total_reward: 109.0
Epoch: 170, Loss: 15.291576385498047, Total_reward: 103.0
Epoch: 180, Loss: 9.691065788269043, Total_reward: 105.0
Epoch: 190, Loss: 9.715533256530762, Total_reward: 102.0
Epoch: 200, Loss: 7.264885902404785, Total_reward: 134.0
Epoch: 210, Loss: 16.258285522460938, Total_reward: 121.0
Epoch: 220, Loss: 10.368988990783691, Total_reward: 112.0
Epoch: 230, Loss: 10.523420333862305, Total_reward: 117.0
Epoch: 240, Loss: 13.553430557250977, Total_reward: 130.0
Epoch: 250, Loss: 18.773845672607422, Total_reward: 123.0
Epoch: 260, Loss: 19.574121475219727, Total_reward: 106.0
Epoch: 270, Loss: 12.336259841918945, Total_reward: 104.0
Epoch: 280, Loss: 17.304443359375, Total_reward: 114.0
Epoch: 290, Loss: 8.051734924316406, Total_reward: 123.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.05080556124448776, Total_reward: 150.0
Epoch: 20, Loss: 0.051824357360601425, Total_reward: 110.0
Epoch: 30, Loss: 0.11906389147043228, Total_reward: 141.0
Epoch: 40, Loss: 0.13949066400527954, Total_reward: 158.0
Epoch: 50, Loss: 0.7986177802085876, Total_reward: 177.0
Epoch: 60, Loss: 1.1910412311553955, Total_reward: 134.0
Epoch: 70, Loss: 2.0862720012664795, Total_reward: 117.0
Epoch: 80, Loss: 0.5984324216842651, Total_reward: 136.0
Epoch: 90, Loss: 1.294824242591858, Total_reward: 172.0
Epoch: 100, Loss: 2.9791259765625, Total_reward: 133.0
Epoch: 110, Loss: 0.6670511364936829, Total_reward: 111.0
Epoch: 120, Loss: 1.4048256874084473, Total_reward: 94.0
Epoch: 130, Loss: 5.2981390953063965, Total_reward: 180.0
Epoch: 140, Loss: 1.3892650604248047, Total_reward: 126.0
Epoch: 150, Loss: 3.7046563625335693, Total_reward: 112.0
Epoch: 160, Loss: 2.8806424140930176, Total_reward: 133.0
Epoch: 170, Loss: 5.222566604614258, Total_reward: 215.0
Epoch: 180, Loss: 4.406479835510254, Total_reward: 119.0
Epoch: 190, Loss: 2.5195534229278564, Total_reward: 107.0
Epoch: 200, Loss: 6.24385929107666, Total_reward: 106.0
Epoch: 210, Loss: 3.2949304580688477, Total_reward: 97.0
Epoch: 220, Loss: 3.2782669067382812, Total_reward: 190.0
Epoch: 230, Loss: 9.90723705291748, Total_reward: 157.0
Epoch: 240, Loss: 8.052145957946777, Total_reward: 154.0
Epoch: 250, Loss: 3.7721168994903564, Total_reward: 123.0
Epoch: 260, Loss: 5.486726760864258, Total_reward: 161.0
Epoch: 270, Loss: 2.071389675140381, Total_reward: 143.0
Epoch: 280, Loss: 17.270946502685547, Total_reward: 178.0
Epoch: 290, Loss: 6.264971733093262, Total_reward: 147.0
Average reward over 10 runs (invisible): 10.1
result : 10.1
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.3665428161621094, Total_reward: 116.0
Epoch: 20, Loss: 0.7871460914611816, Total_reward: 98.0
Epoch: 30, Loss: 2.4891011714935303, Total_reward: 99.0
Epoch: 40, Loss: 2.2400095462799072, Total_reward: 112.0
Epoch: 50, Loss: 6.671859264373779, Total_reward: 88.0
Epoch: 60, Loss: 3.106220006942749, Total_reward: 175.0
Epoch: 70, Loss: 8.044933319091797, Total_reward: 120.0
Epoch: 80, Loss: 3.71915602684021, Total_reward: 151.0
Epoch: 90, Loss: 3.6091718673706055, Total_reward: 157.0
Epoch: 100, Loss: 4.617094039916992, Total_reward: 154.0
Epoch: 110, Loss: 11.516250610351562, Total_reward: 157.0
Epoch: 120, Loss: 11.431140899658203, Total_reward: 136.0
Epoch: 130, Loss: 6.358811378479004, Total_reward: 145.0
Epoch: 140, Loss: 11.078856468200684, Total_reward: 217.0
Epoch: 150, Loss: 15.420926094055176, Total_reward: 151.0
Epoch: 160, Loss: 11.375892639160156, Total_reward: 179.0
Epoch: 170, Loss: 16.60577392578125, Total_reward: 99.0
Epoch: 180, Loss: 14.76512336730957, Total_reward: 174.0
Epoch: 190, Loss: 19.334657669067383, Total_reward: 153.0
Epoch: 200, Loss: 28.198184967041016, Total_reward: 110.0
Epoch: 210, Loss: 47.37835693359375, Total_reward: 110.0
Epoch: 220, Loss: 21.538593292236328, Total_reward: 129.0
Epoch: 230, Loss: 35.63892364501953, Total_reward: 170.0
Epoch: 240, Loss: 15.300118446350098, Total_reward: 122.0
Epoch: 250, Loss: 21.006731033325195, Total_reward: 127.0
Epoch: 260, Loss: 29.588346481323242, Total_reward: 160.0
Epoch: 270, Loss: 26.42557144165039, Total_reward: 118.0
Epoch: 280, Loss: 3.4493892192840576, Total_reward: 144.0
Epoch: 290, Loss: 23.11994171142578, Total_reward: 105.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.13168829679489136, Total_reward: 99.0
Epoch: 20, Loss: 0.356555700302124, Total_reward: 94.0
Epoch: 30, Loss: 0.780703067779541, Total_reward: 112.0
Epoch: 40, Loss: 1.0888054370880127, Total_reward: 86.0
Epoch: 50, Loss: 1.5260109901428223, Total_reward: 174.0
Epoch: 60, Loss: 1.6300801038742065, Total_reward: 165.0
Epoch: 70, Loss: 1.3614646196365356, Total_reward: 140.0
Epoch: 80, Loss: 2.0127158164978027, Total_reward: 133.0
Epoch: 90, Loss: 4.262111663818359, Total_reward: 92.0
Epoch: 100, Loss: 2.868694543838501, Total_reward: 164.0
Epoch: 110, Loss: 3.050394296646118, Total_reward: 137.0
Epoch: 120, Loss: 3.015089988708496, Total_reward: 165.0
Epoch: 130, Loss: 0.9188519716262817, Total_reward: 114.0
Epoch: 140, Loss: 1.553895354270935, Total_reward: 157.0
Epoch: 150, Loss: 3.670436382293701, Total_reward: 127.0
Epoch: 160, Loss: 5.09168815612793, Total_reward: 141.0
Epoch: 170, Loss: 3.7481343746185303, Total_reward: 145.0
Epoch: 180, Loss: 5.901541709899902, Total_reward: 171.0
Epoch: 190, Loss: 5.499026775360107, Total_reward: 142.0
Epoch: 200, Loss: 7.18858528137207, Total_reward: 120.0
Epoch: 210, Loss: 7.357810974121094, Total_reward: 146.0
Epoch: 220, Loss: 6.082342624664307, Total_reward: 201.0
Epoch: 230, Loss: 5.366450309753418, Total_reward: 125.0
Epoch: 240, Loss: 6.754233360290527, Total_reward: 134.0
Epoch: 250, Loss: 9.36568832397461, Total_reward: 171.0
Epoch: 260, Loss: 5.977267742156982, Total_reward: 156.0
Epoch: 270, Loss: 2.9441306591033936, Total_reward: 112.0
Epoch: 280, Loss: 11.64217758178711, Total_reward: 110.0
Epoch: 290, Loss: 8.801587104797363, Total_reward: 181.0
Average reward over 10 runs (invisible): 10.6
result : 10.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1000729352235794, Total_reward: 126.0
Epoch: 20, Loss: 1.8280956745147705, Total_reward: 156.0
Epoch: 30, Loss: 1.4476542472839355, Total_reward: 173.0
Epoch: 40, Loss: 2.0908892154693604, Total_reward: 196.0
Epoch: 50, Loss: 2.136749505996704, Total_reward: 198.0
Epoch: 60, Loss: 4.304736137390137, Total_reward: 170.0
Epoch: 70, Loss: 1.7520546913146973, Total_reward: 131.0
Epoch: 80, Loss: 6.733725547790527, Total_reward: 204.0
Epoch: 90, Loss: 7.850327014923096, Total_reward: 129.0
Epoch: 100, Loss: 9.901870727539062, Total_reward: 201.0
Epoch: 110, Loss: 6.245734691619873, Total_reward: 188.0
Epoch: 120, Loss: 16.14183235168457, Total_reward: 181.0
Epoch: 130, Loss: 13.314682006835938, Total_reward: 179.0
Epoch: 140, Loss: 20.358287811279297, Total_reward: 126.0
Epoch: 150, Loss: 11.391692161560059, Total_reward: 104.0
Epoch: 160, Loss: 35.568138122558594, Total_reward: 107.0
Epoch: 170, Loss: 39.08856201171875, Total_reward: 93.0
Epoch: 180, Loss: 9.561750411987305, Total_reward: 103.0
Epoch: 190, Loss: 17.428226470947266, Total_reward: 92.0
Epoch: 200, Loss: 39.178733825683594, Total_reward: 97.0
Epoch: 210, Loss: 19.964664459228516, Total_reward: 102.0
Epoch: 220, Loss: 13.76767349243164, Total_reward: 164.0
Epoch: 230, Loss: 5.31416654586792, Total_reward: 116.0
Epoch: 240, Loss: 24.075176239013672, Total_reward: 93.0
Epoch: 250, Loss: 19.11983871459961, Total_reward: 117.0
Epoch: 260, Loss: 22.351879119873047, Total_reward: 117.0
Epoch: 270, Loss: 18.398216247558594, Total_reward: 93.0
Epoch: 280, Loss: 13.632158279418945, Total_reward: 106.0
Epoch: 290, Loss: 9.770111083984375, Total_reward: 100.0
Average reward over 10 runs (invisible): 9.8
result : 9.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.06064590439200401, Total_reward: 126.0
Epoch: 20, Loss: 0.08034542202949524, Total_reward: 159.0
Epoch: 30, Loss: 0.5285051465034485, Total_reward: 172.0
Epoch: 40, Loss: 0.2383825182914734, Total_reward: 145.0
Epoch: 50, Loss: 0.1750136762857437, Total_reward: 140.0
Epoch: 60, Loss: 0.6919547915458679, Total_reward: 168.0
Epoch: 70, Loss: 0.9980778694152832, Total_reward: 133.0
Epoch: 80, Loss: 0.5018085837364197, Total_reward: 99.0
Epoch: 90, Loss: 3.533515691757202, Total_reward: 130.0
Epoch: 100, Loss: 0.37361469864845276, Total_reward: 147.0
Epoch: 110, Loss: 2.244652509689331, Total_reward: 142.0
Epoch: 120, Loss: 2.526573419570923, Total_reward: 164.0
Epoch: 130, Loss: 1.9578908681869507, Total_reward: 105.0
Epoch: 140, Loss: 4.254167079925537, Total_reward: 148.0
Epoch: 150, Loss: 4.817529201507568, Total_reward: 179.0
Epoch: 160, Loss: 2.465569496154785, Total_reward: 145.0
Epoch: 170, Loss: 3.357178211212158, Total_reward: 186.0
Epoch: 180, Loss: 3.320918560028076, Total_reward: 135.0
Epoch: 190, Loss: 3.692636251449585, Total_reward: 128.0
Epoch: 200, Loss: 2.8106770515441895, Total_reward: 190.0
Epoch: 210, Loss: 6.9058942794799805, Total_reward: 131.0
Epoch: 220, Loss: 9.983799934387207, Total_reward: 127.0
Epoch: 230, Loss: 5.792116165161133, Total_reward: 170.0
Epoch: 240, Loss: 6.600522041320801, Total_reward: 141.0
Epoch: 250, Loss: 4.339128494262695, Total_reward: 137.0
Epoch: 260, Loss: 6.50167989730835, Total_reward: 134.0
Epoch: 270, Loss: 5.310046672821045, Total_reward: 150.0
Epoch: 280, Loss: 6.233864784240723, Total_reward: 105.0
Epoch: 290, Loss: 11.915225982666016, Total_reward: 145.0
Average reward over 10 runs (invisible): 9.7
result : 9.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.22493162751197815, Total_reward: 94.0
Epoch: 20, Loss: 2.878164291381836, Total_reward: 98.0
Epoch: 30, Loss: 2.9633450508117676, Total_reward: 129.0
Epoch: 40, Loss: 3.1358468532562256, Total_reward: 117.0
Epoch: 50, Loss: 2.9948158264160156, Total_reward: 140.0
Epoch: 60, Loss: 5.498044967651367, Total_reward: 135.0
Epoch: 70, Loss: 8.314414978027344, Total_reward: 135.0
Epoch: 80, Loss: 13.281327247619629, Total_reward: 96.0
Epoch: 90, Loss: 7.933595180511475, Total_reward: 108.0
Epoch: 100, Loss: 12.604717254638672, Total_reward: 111.0
Epoch: 110, Loss: 13.259729385375977, Total_reward: 131.0
Epoch: 120, Loss: 10.112871170043945, Total_reward: 117.0
Epoch: 130, Loss: 9.711833000183105, Total_reward: 145.0
Epoch: 140, Loss: 9.80411148071289, Total_reward: 132.0
Epoch: 150, Loss: 7.978012561798096, Total_reward: 171.0
Epoch: 160, Loss: 10.655492782592773, Total_reward: 108.0
Epoch: 170, Loss: 19.692045211791992, Total_reward: 139.0
Epoch: 180, Loss: 11.287762641906738, Total_reward: 180.0
Epoch: 190, Loss: 12.03158187866211, Total_reward: 123.0
Epoch: 200, Loss: 4.803761959075928, Total_reward: 183.0
Epoch: 210, Loss: 10.94133186340332, Total_reward: 203.0
Epoch: 220, Loss: 14.020856857299805, Total_reward: 209.0
Epoch: 230, Loss: 4.1163811683654785, Total_reward: 132.0
Epoch: 240, Loss: 25.78108787536621, Total_reward: 106.0
Epoch: 250, Loss: 11.633538246154785, Total_reward: 128.0
Epoch: 260, Loss: 20.347150802612305, Total_reward: 129.0
Epoch: 270, Loss: 36.3449592590332, Total_reward: 147.0
Epoch: 280, Loss: 29.714275360107422, Total_reward: 112.0
Epoch: 290, Loss: 6.283050537109375, Total_reward: 145.0
Average reward over 10 runs (invisible): 8.9
result : 8.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.12222295254468918, Total_reward: 166.0
Epoch: 20, Loss: 0.20803360641002655, Total_reward: 121.0
Epoch: 30, Loss: 1.1168279647827148, Total_reward: 168.0
Epoch: 40, Loss: 1.1626567840576172, Total_reward: 154.0
Epoch: 50, Loss: 0.7876818180084229, Total_reward: 122.0
Epoch: 60, Loss: 0.7640028595924377, Total_reward: 122.0
Epoch: 70, Loss: 1.3984181880950928, Total_reward: 125.0
Epoch: 80, Loss: 0.43216851353645325, Total_reward: 122.0
Epoch: 90, Loss: 1.3697432279586792, Total_reward: 105.0
Epoch: 100, Loss: 2.2190213203430176, Total_reward: 177.0
Epoch: 110, Loss: 1.9886811971664429, Total_reward: 120.0
Epoch: 120, Loss: 2.0242326259613037, Total_reward: 94.0
Epoch: 130, Loss: 3.6567177772521973, Total_reward: 120.0
Epoch: 140, Loss: 2.3233418464660645, Total_reward: 150.0
Epoch: 150, Loss: 1.6912199258804321, Total_reward: 145.0
Epoch: 160, Loss: 3.3267416954040527, Total_reward: 115.0
Epoch: 170, Loss: 6.653209209442139, Total_reward: 157.0
Epoch: 180, Loss: 2.1290247440338135, Total_reward: 124.0
Epoch: 190, Loss: 3.624568223953247, Total_reward: 101.0
Epoch: 200, Loss: 7.580697536468506, Total_reward: 143.0
Epoch: 210, Loss: 4.747400283813477, Total_reward: 113.0
Epoch: 220, Loss: 3.4809274673461914, Total_reward: 96.0
Epoch: 230, Loss: 4.791827201843262, Total_reward: 125.0
Epoch: 240, Loss: 7.981459617614746, Total_reward: 143.0
Epoch: 250, Loss: 11.712843894958496, Total_reward: 96.0
Epoch: 260, Loss: 3.227433681488037, Total_reward: 127.0
Epoch: 270, Loss: 11.16562557220459, Total_reward: 138.0
Epoch: 280, Loss: 12.525537490844727, Total_reward: 151.0
Epoch: 290, Loss: 7.472254276275635, Total_reward: 162.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.14910849928855896, Total_reward: 101.0
Epoch: 20, Loss: 0.31271111965179443, Total_reward: 89.0
Epoch: 30, Loss: 1.6049541234970093, Total_reward: 116.0
Epoch: 40, Loss: 2.7184810638427734, Total_reward: 134.0
Epoch: 50, Loss: 4.358372211456299, Total_reward: 163.0
Epoch: 60, Loss: 8.780637741088867, Total_reward: 157.0
Epoch: 70, Loss: 4.452054500579834, Total_reward: 149.0
Epoch: 80, Loss: 7.041436672210693, Total_reward: 133.0
Epoch: 90, Loss: 8.336657524108887, Total_reward: 143.0
Epoch: 100, Loss: 5.9554667472839355, Total_reward: 191.0
Epoch: 110, Loss: 5.872150897979736, Total_reward: 202.0
Epoch: 120, Loss: 6.819511413574219, Total_reward: 142.0
Epoch: 130, Loss: 11.526298522949219, Total_reward: 83.0
Epoch: 140, Loss: 2.8379788398742676, Total_reward: 112.0
Epoch: 150, Loss: 24.326759338378906, Total_reward: 87.0
Epoch: 160, Loss: 23.248441696166992, Total_reward: 87.0
Epoch: 170, Loss: 17.713502883911133, Total_reward: 93.0
Epoch: 180, Loss: 14.106951713562012, Total_reward: 86.0
Epoch: 190, Loss: 12.609228134155273, Total_reward: 107.0
Epoch: 200, Loss: 13.233341217041016, Total_reward: 106.0
Epoch: 210, Loss: 21.22136688232422, Total_reward: 97.0
Epoch: 220, Loss: 21.419158935546875, Total_reward: 129.0
Epoch: 230, Loss: 26.69068145751953, Total_reward: 96.0
Epoch: 240, Loss: 17.836977005004883, Total_reward: 130.0
Epoch: 250, Loss: 17.12661361694336, Total_reward: 104.0
Epoch: 260, Loss: 17.891080856323242, Total_reward: 124.0
Epoch: 270, Loss: 10.249722480773926, Total_reward: 131.0
Epoch: 280, Loss: 14.155253410339355, Total_reward: 100.0
Epoch: 290, Loss: 8.028303146362305, Total_reward: 116.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1337946653366089, Total_reward: 91.0
Epoch: 20, Loss: 0.19504320621490479, Total_reward: 88.0
Epoch: 30, Loss: 0.16500593721866608, Total_reward: 92.0
Epoch: 40, Loss: 0.4212992191314697, Total_reward: 90.0
Epoch: 50, Loss: 1.5635013580322266, Total_reward: 138.0
Epoch: 60, Loss: 0.915538489818573, Total_reward: 160.0
Epoch: 70, Loss: 1.065090537071228, Total_reward: 90.0
Epoch: 80, Loss: 2.1137161254882812, Total_reward: 98.0
Epoch: 90, Loss: 1.576782464981079, Total_reward: 137.0
Epoch: 100, Loss: 2.9096157550811768, Total_reward: 112.0
Epoch: 110, Loss: 3.531712532043457, Total_reward: 173.0
Epoch: 120, Loss: 1.0872598886489868, Total_reward: 156.0
Epoch: 130, Loss: 1.8083820343017578, Total_reward: 115.0
Epoch: 140, Loss: 3.7292027473449707, Total_reward: 159.0
Epoch: 150, Loss: 4.2918171882629395, Total_reward: 134.0
Epoch: 160, Loss: 2.1914074420928955, Total_reward: 119.0
Epoch: 170, Loss: 2.834474563598633, Total_reward: 157.0
Epoch: 180, Loss: 1.2365442514419556, Total_reward: 143.0
Epoch: 190, Loss: 3.418442726135254, Total_reward: 164.0
Epoch: 200, Loss: 3.4681646823883057, Total_reward: 167.0
Epoch: 210, Loss: 5.382196426391602, Total_reward: 133.0
Epoch: 220, Loss: 6.254212856292725, Total_reward: 127.0
Epoch: 230, Loss: 5.896154880523682, Total_reward: 234.0
Epoch: 240, Loss: 4.67264461517334, Total_reward: 144.0
Epoch: 250, Loss: 10.21919059753418, Total_reward: 124.0
Epoch: 260, Loss: 6.191714763641357, Total_reward: 177.0
Epoch: 270, Loss: 10.79720401763916, Total_reward: 114.0
Epoch: 280, Loss: 7.879476070404053, Total_reward: 145.0
Epoch: 290, Loss: 3.707460403442383, Total_reward: 204.0
Average reward over 10 runs (invisible): 15.8
result : 15.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.005840469617396593, Total_reward: 130.0
Epoch: 20, Loss: 0.8401727080345154, Total_reward: 115.0
Epoch: 30, Loss: 0.13832974433898926, Total_reward: 137.0
Epoch: 40, Loss: 4.223519325256348, Total_reward: 112.0
Epoch: 50, Loss: 3.676145076751709, Total_reward: 110.0
Epoch: 60, Loss: 13.979042053222656, Total_reward: 130.0
Epoch: 70, Loss: 15.522822380065918, Total_reward: 135.0
Epoch: 80, Loss: 18.69327163696289, Total_reward: 102.0
Epoch: 90, Loss: 20.247234344482422, Total_reward: 157.0
Epoch: 100, Loss: 18.272838592529297, Total_reward: 106.0
Epoch: 110, Loss: 12.794135093688965, Total_reward: 187.0
Epoch: 120, Loss: 13.347052574157715, Total_reward: 108.0
Epoch: 130, Loss: 16.10784339904785, Total_reward: 99.0
Epoch: 140, Loss: 17.612262725830078, Total_reward: 102.0
Epoch: 150, Loss: 12.497862815856934, Total_reward: 98.0
Epoch: 160, Loss: 18.408174514770508, Total_reward: 108.0
Epoch: 170, Loss: 26.531190872192383, Total_reward: 102.0
Epoch: 180, Loss: 18.79132080078125, Total_reward: 123.0
Epoch: 190, Loss: 29.161243438720703, Total_reward: 134.0
Epoch: 200, Loss: 18.81315040588379, Total_reward: 103.0
Epoch: 210, Loss: 21.407848358154297, Total_reward: 96.0
Epoch: 220, Loss: 17.24635887145996, Total_reward: 106.0
Epoch: 230, Loss: 21.916866302490234, Total_reward: 105.0
Epoch: 240, Loss: 39.6322021484375, Total_reward: 94.0
Epoch: 250, Loss: 19.09221839904785, Total_reward: 90.0
Epoch: 260, Loss: 19.05637550354004, Total_reward: 86.0
Epoch: 270, Loss: 25.345043182373047, Total_reward: 87.0
Epoch: 280, Loss: 19.30803871154785, Total_reward: 88.0
Epoch: 290, Loss: 29.469980239868164, Total_reward: 90.0
Average reward over 10 runs (invisible): 9.1
result : 9.1
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.041858576238155365, Total_reward: 97.0
Epoch: 20, Loss: 0.1584252119064331, Total_reward: 96.0
Epoch: 30, Loss: 0.23076918721199036, Total_reward: 89.0
Epoch: 40, Loss: 0.625283420085907, Total_reward: 93.0
Epoch: 50, Loss: 0.2939441502094269, Total_reward: 139.0
Epoch: 60, Loss: 1.2768229246139526, Total_reward: 140.0
Epoch: 70, Loss: 2.4755001068115234, Total_reward: 110.0
Epoch: 80, Loss: 0.787833571434021, Total_reward: 108.0
Epoch: 90, Loss: 2.6571128368377686, Total_reward: 105.0
Epoch: 100, Loss: 2.7702066898345947, Total_reward: 189.0
Epoch: 110, Loss: 3.598531723022461, Total_reward: 103.0
Epoch: 120, Loss: 2.2351675033569336, Total_reward: 99.0
Epoch: 130, Loss: 5.066770553588867, Total_reward: 94.0
Epoch: 140, Loss: 5.084104061126709, Total_reward: 110.0
Epoch: 150, Loss: 5.507940769195557, Total_reward: 100.0
Epoch: 160, Loss: 6.3289947509765625, Total_reward: 112.0
Epoch: 170, Loss: 5.946002006530762, Total_reward: 99.0
Epoch: 180, Loss: 8.563068389892578, Total_reward: 132.0
Epoch: 190, Loss: 6.794063091278076, Total_reward: 140.0
Epoch: 200, Loss: 7.709100723266602, Total_reward: 136.0
Epoch: 210, Loss: 8.434772491455078, Total_reward: 132.0
Epoch: 220, Loss: 8.79714298248291, Total_reward: 106.0
Epoch: 230, Loss: 8.411853790283203, Total_reward: 168.0
Epoch: 240, Loss: 7.087351322174072, Total_reward: 125.0
Epoch: 250, Loss: 7.037078857421875, Total_reward: 105.0
Epoch: 260, Loss: 8.754435539245605, Total_reward: 179.0
Epoch: 270, Loss: 9.439226150512695, Total_reward: 173.0
Epoch: 280, Loss: 7.471610069274902, Total_reward: 150.0
Epoch: 290, Loss: 11.826800346374512, Total_reward: 208.0
Average reward over 10 runs (invisible): 77.8
result : 77.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.00883497018367052, Total_reward: 159.0
Epoch: 20, Loss: 0.15635645389556885, Total_reward: 101.0
Epoch: 30, Loss: 1.5014194250106812, Total_reward: 110.0
Epoch: 40, Loss: 0.21486349403858185, Total_reward: 105.0
Epoch: 50, Loss: 7.220773696899414, Total_reward: 145.0
Epoch: 60, Loss: 5.351055145263672, Total_reward: 150.0
Epoch: 70, Loss: 12.994209289550781, Total_reward: 129.0
Epoch: 80, Loss: 8.449779510498047, Total_reward: 99.0
Epoch: 90, Loss: 8.76639461517334, Total_reward: 104.0
Epoch: 100, Loss: 6.062994480133057, Total_reward: 129.0
Epoch: 110, Loss: 6.4750285148620605, Total_reward: 106.0
Epoch: 120, Loss: 13.592622756958008, Total_reward: 151.0
Epoch: 130, Loss: 7.586085796356201, Total_reward: 112.0
Epoch: 140, Loss: 18.196346282958984, Total_reward: 137.0
Epoch: 150, Loss: 14.470449447631836, Total_reward: 114.0
Epoch: 160, Loss: 5.484740734100342, Total_reward: 124.0
Epoch: 170, Loss: 17.845657348632812, Total_reward: 114.0
Epoch: 180, Loss: 13.536709785461426, Total_reward: 131.0
Epoch: 190, Loss: 16.953493118286133, Total_reward: 125.0
Epoch: 200, Loss: 0.9714797735214233, Total_reward: 127.0
Epoch: 210, Loss: 10.268049240112305, Total_reward: 107.0
Epoch: 220, Loss: 7.283912658691406, Total_reward: 108.0
Epoch: 230, Loss: 27.08645248413086, Total_reward: 88.0
Epoch: 240, Loss: 1.4352301359176636, Total_reward: 140.0
Epoch: 250, Loss: 23.84467124938965, Total_reward: 101.0
Epoch: 260, Loss: 13.287492752075195, Total_reward: 96.0
Epoch: 270, Loss: 21.917980194091797, Total_reward: 98.0
Epoch: 280, Loss: 20.97629165649414, Total_reward: 103.0
Epoch: 290, Loss: 19.616806030273438, Total_reward: 91.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.06492157280445099, Total_reward: 114.0
Epoch: 20, Loss: 0.08171812444925308, Total_reward: 104.0
Epoch: 30, Loss: 0.3248923122882843, Total_reward: 91.0
Epoch: 40, Loss: 0.499950647354126, Total_reward: 91.0
Epoch: 50, Loss: 1.0306222438812256, Total_reward: 97.0
Epoch: 60, Loss: 0.7263105511665344, Total_reward: 101.0
Epoch: 70, Loss: 0.8233953714370728, Total_reward: 112.0
Epoch: 80, Loss: 3.306549072265625, Total_reward: 102.0
Epoch: 90, Loss: 1.6953070163726807, Total_reward: 96.0
Epoch: 100, Loss: 3.952141284942627, Total_reward: 139.0
Epoch: 110, Loss: 2.4971702098846436, Total_reward: 88.0
Epoch: 120, Loss: 2.813328981399536, Total_reward: 101.0
Epoch: 130, Loss: 3.6687676906585693, Total_reward: 139.0
Epoch: 140, Loss: 3.1146559715270996, Total_reward: 140.0
Epoch: 150, Loss: 5.222124099731445, Total_reward: 117.0
Epoch: 160, Loss: 7.675889492034912, Total_reward: 90.0
Epoch: 170, Loss: 4.211275100708008, Total_reward: 95.0
Epoch: 180, Loss: 2.9475505352020264, Total_reward: 130.0
Epoch: 190, Loss: 2.2985036373138428, Total_reward: 135.0
Epoch: 200, Loss: 4.8189167976379395, Total_reward: 153.0
Epoch: 210, Loss: 8.35653305053711, Total_reward: 181.0
Epoch: 220, Loss: 7.756977558135986, Total_reward: 190.0
Epoch: 230, Loss: 6.537962913513184, Total_reward: 140.0
Epoch: 240, Loss: 7.950451850891113, Total_reward: 124.0
Epoch: 250, Loss: 8.58407974243164, Total_reward: 118.0
Epoch: 260, Loss: 12.529740333557129, Total_reward: 128.0
Epoch: 270, Loss: 7.649178504943848, Total_reward: 98.0
Epoch: 280, Loss: 9.659319877624512, Total_reward: 88.0
Epoch: 290, Loss: 8.134117126464844, Total_reward: 119.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.25353550910949707, Total_reward: 130.0
Epoch: 20, Loss: 0.8649744987487793, Total_reward: 124.0
Epoch: 30, Loss: 2.2550289630889893, Total_reward: 129.0
Epoch: 40, Loss: 5.578312397003174, Total_reward: 112.0
Epoch: 50, Loss: 8.617269515991211, Total_reward: 104.0
Epoch: 60, Loss: 1.3133846521377563, Total_reward: 94.0
Epoch: 70, Loss: 9.242217063903809, Total_reward: 91.0
Epoch: 80, Loss: 6.883612632751465, Total_reward: 143.0
Epoch: 90, Loss: 9.133474349975586, Total_reward: 169.0
Epoch: 100, Loss: 7.713767051696777, Total_reward: 203.0
Epoch: 110, Loss: 19.542509078979492, Total_reward: 186.0
Epoch: 120, Loss: 5.747003078460693, Total_reward: 154.0
Epoch: 130, Loss: 16.85260009765625, Total_reward: 123.0
Epoch: 140, Loss: 12.675698280334473, Total_reward: 141.0
Epoch: 150, Loss: 7.53265380859375, Total_reward: 129.0
Epoch: 160, Loss: 27.84124755859375, Total_reward: 108.0
Epoch: 170, Loss: 13.014883995056152, Total_reward: 137.0
Epoch: 180, Loss: 7.554054260253906, Total_reward: 117.0
Epoch: 190, Loss: 40.335147857666016, Total_reward: 129.0
Epoch: 200, Loss: 23.35887336730957, Total_reward: 92.0
Epoch: 210, Loss: 4.671435832977295, Total_reward: 98.0
Epoch: 220, Loss: 19.069915771484375, Total_reward: 107.0
Epoch: 230, Loss: 17.075653076171875, Total_reward: 122.0
Epoch: 240, Loss: 14.855894088745117, Total_reward: 119.0
Epoch: 250, Loss: 13.080080032348633, Total_reward: 127.0
Epoch: 260, Loss: 19.799219131469727, Total_reward: 117.0
Epoch: 270, Loss: 15.240246772766113, Total_reward: 143.0
Epoch: 280, Loss: 23.014345169067383, Total_reward: 92.0
Epoch: 290, Loss: 26.742938995361328, Total_reward: 127.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.10549740493297577, Total_reward: 113.0
Epoch: 20, Loss: 0.19302164018154144, Total_reward: 92.0
Epoch: 30, Loss: 0.42598071694374084, Total_reward: 108.0
Epoch: 40, Loss: 0.4158402681350708, Total_reward: 113.0
Epoch: 50, Loss: 0.9890960454940796, Total_reward: 101.0
Epoch: 60, Loss: 2.5197794437408447, Total_reward: 97.0
Epoch: 70, Loss: 2.1987876892089844, Total_reward: 120.0
Epoch: 80, Loss: 2.8722074031829834, Total_reward: 102.0
Epoch: 90, Loss: 1.422571063041687, Total_reward: 102.0
Epoch: 100, Loss: 2.403803825378418, Total_reward: 109.0
Epoch: 110, Loss: 4.291291236877441, Total_reward: 166.0
Epoch: 120, Loss: 3.17437744140625, Total_reward: 120.0
Epoch: 130, Loss: 2.6828198432922363, Total_reward: 145.0
Epoch: 140, Loss: 2.6180994510650635, Total_reward: 137.0
Epoch: 150, Loss: 3.170902967453003, Total_reward: 109.0
Epoch: 160, Loss: 1.9386343955993652, Total_reward: 108.0
Epoch: 170, Loss: 1.9796977043151855, Total_reward: 125.0
Epoch: 180, Loss: 4.035745620727539, Total_reward: 166.0
Epoch: 190, Loss: 5.346842288970947, Total_reward: 87.0
Epoch: 200, Loss: 2.995134115219116, Total_reward: 120.0
Epoch: 210, Loss: 5.610456466674805, Total_reward: 104.0
Epoch: 220, Loss: 3.5128142833709717, Total_reward: 92.0
Epoch: 230, Loss: 5.477086544036865, Total_reward: 109.0
Epoch: 240, Loss: 4.704524040222168, Total_reward: 97.0
Epoch: 250, Loss: 4.674102306365967, Total_reward: 103.0
Epoch: 260, Loss: 7.20506477355957, Total_reward: 107.0
Epoch: 270, Loss: 6.593064785003662, Total_reward: 100.0
Epoch: 280, Loss: 11.290567398071289, Total_reward: 114.0
Epoch: 290, Loss: 6.823050022125244, Total_reward: 102.0
Average reward over 10 runs (invisible): 15.4
result : 15.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.023937931284308434, Total_reward: 130.0
Epoch: 20, Loss: 0.03194471821188927, Total_reward: 118.0
Epoch: 30, Loss: 1.574792504310608, Total_reward: 88.0
Epoch: 40, Loss: 1.4455597400665283, Total_reward: 103.0
Epoch: 50, Loss: 5.597259998321533, Total_reward: 178.0
Epoch: 60, Loss: 9.026105880737305, Total_reward: 150.0
Epoch: 70, Loss: 5.647164821624756, Total_reward: 141.0
Epoch: 80, Loss: 8.233589172363281, Total_reward: 142.0
Epoch: 90, Loss: 14.355350494384766, Total_reward: 156.0
Epoch: 100, Loss: 11.176925659179688, Total_reward: 146.0
Epoch: 110, Loss: 14.966253280639648, Total_reward: 153.0
Epoch: 120, Loss: 12.481122016906738, Total_reward: 147.0
Epoch: 130, Loss: 13.95161247253418, Total_reward: 125.0
Epoch: 140, Loss: 16.63363265991211, Total_reward: 132.0
Epoch: 150, Loss: 25.32261085510254, Total_reward: 110.0
Epoch: 160, Loss: 18.53107452392578, Total_reward: 100.0
Epoch: 170, Loss: 13.304617881774902, Total_reward: 130.0
Epoch: 180, Loss: 9.445988655090332, Total_reward: 116.0
Epoch: 190, Loss: 14.396490097045898, Total_reward: 115.0
Epoch: 200, Loss: 13.595695495605469, Total_reward: 112.0
Epoch: 210, Loss: 15.796712875366211, Total_reward: 117.0
Epoch: 220, Loss: 9.695228576660156, Total_reward: 119.0
Epoch: 230, Loss: 19.811521530151367, Total_reward: 161.0
Epoch: 240, Loss: 12.29092025756836, Total_reward: 167.0
Epoch: 250, Loss: 22.764793395996094, Total_reward: 152.0
Epoch: 260, Loss: 17.064645767211914, Total_reward: 159.0
Epoch: 270, Loss: 24.66839027404785, Total_reward: 110.0
Epoch: 280, Loss: 13.053467750549316, Total_reward: 94.0
Epoch: 290, Loss: 22.697654724121094, Total_reward: 115.0
Average reward over 10 runs (invisible): 9.0
result : 9.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.17532682418823242, Total_reward: 99.0
Epoch: 20, Loss: 0.12330812960863113, Total_reward: 94.0
Epoch: 30, Loss: 0.5011523962020874, Total_reward: 123.0
Epoch: 40, Loss: 0.3430617153644562, Total_reward: 127.0
Epoch: 50, Loss: 0.5966921448707581, Total_reward: 139.0
Epoch: 60, Loss: 1.1752476692199707, Total_reward: 194.0
Epoch: 70, Loss: 1.3660106658935547, Total_reward: 102.0
Epoch: 80, Loss: 0.4636378288269043, Total_reward: 123.0
Epoch: 90, Loss: 1.5036554336547852, Total_reward: 143.0
Epoch: 100, Loss: 1.4171148538589478, Total_reward: 158.0
Epoch: 110, Loss: 2.1927614212036133, Total_reward: 150.0
Epoch: 120, Loss: 1.7343573570251465, Total_reward: 132.0
Epoch: 130, Loss: 1.429329514503479, Total_reward: 160.0
Epoch: 140, Loss: 2.5413031578063965, Total_reward: 135.0
Epoch: 150, Loss: 3.820735216140747, Total_reward: 210.0
Epoch: 160, Loss: 1.8722093105316162, Total_reward: 167.0
Epoch: 170, Loss: 1.123522162437439, Total_reward: 110.0
Epoch: 180, Loss: 3.5241992473602295, Total_reward: 117.0
Epoch: 190, Loss: 4.176065444946289, Total_reward: 103.0
Epoch: 200, Loss: 2.316206216812134, Total_reward: 118.0
Epoch: 210, Loss: 2.7925736904144287, Total_reward: 151.0
Epoch: 220, Loss: 10.03632926940918, Total_reward: 151.0
Epoch: 230, Loss: 4.416506290435791, Total_reward: 105.0
Epoch: 240, Loss: 3.1536383628845215, Total_reward: 95.0
Epoch: 250, Loss: 8.69115161895752, Total_reward: 98.0
Epoch: 260, Loss: 6.128325939178467, Total_reward: 99.0
Epoch: 270, Loss: 10.691167831420898, Total_reward: 104.0
Epoch: 280, Loss: 16.450145721435547, Total_reward: 101.0
Epoch: 290, Loss: 9.111675262451172, Total_reward: 113.0
Average reward over 10 runs (invisible): 9.0
result : 9.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.10863891988992691, Total_reward: 131.0
Epoch: 20, Loss: 1.5867418050765991, Total_reward: 105.0
Epoch: 30, Loss: 2.346525192260742, Total_reward: 113.0
Epoch: 40, Loss: 5.271373748779297, Total_reward: 173.0
Epoch: 50, Loss: 3.545372724533081, Total_reward: 147.0
Epoch: 60, Loss: 4.219732284545898, Total_reward: 140.0
Epoch: 70, Loss: 6.431987285614014, Total_reward: 108.0
Epoch: 80, Loss: 11.318439483642578, Total_reward: 93.0
Epoch: 90, Loss: 6.574738502502441, Total_reward: 120.0
Epoch: 100, Loss: 10.789843559265137, Total_reward: 104.0
Epoch: 110, Loss: 23.138145446777344, Total_reward: 94.0
Epoch: 120, Loss: 17.40321922302246, Total_reward: 110.0
Epoch: 130, Loss: 18.078201293945312, Total_reward: 137.0
Epoch: 140, Loss: 13.156339645385742, Total_reward: 153.0
Epoch: 150, Loss: 19.25686264038086, Total_reward: 136.0
Epoch: 160, Loss: 10.737565994262695, Total_reward: 163.0
Epoch: 170, Loss: 8.491943359375, Total_reward: 158.0
Epoch: 180, Loss: 13.568659782409668, Total_reward: 144.0
Epoch: 190, Loss: 8.200778007507324, Total_reward: 150.0
Epoch: 200, Loss: 18.50831413269043, Total_reward: 142.0
Epoch: 210, Loss: 25.5128231048584, Total_reward: 116.0
Epoch: 220, Loss: 27.229022979736328, Total_reward: 101.0
Epoch: 230, Loss: 29.297977447509766, Total_reward: 119.0
Epoch: 240, Loss: 34.6488037109375, Total_reward: 124.0
Epoch: 250, Loss: 26.71044158935547, Total_reward: 147.0
Epoch: 260, Loss: 27.787315368652344, Total_reward: 128.0
Epoch: 270, Loss: 25.500503540039062, Total_reward: 153.0
Epoch: 280, Loss: 24.794240951538086, Total_reward: 122.0
Epoch: 290, Loss: 35.52241516113281, Total_reward: 117.0
Average reward over 10 runs (invisible): 12.4
result : 12.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.21692034602165222, Total_reward: 101.0
Epoch: 20, Loss: 0.12201877683401108, Total_reward: 92.0
Epoch: 30, Loss: 0.682619571685791, Total_reward: 139.0
Epoch: 40, Loss: 0.7886002659797668, Total_reward: 149.0
Epoch: 50, Loss: 0.6551830172538757, Total_reward: 174.0
Epoch: 60, Loss: 0.8712729215621948, Total_reward: 157.0
Epoch: 70, Loss: 0.554672360420227, Total_reward: 128.0
Epoch: 80, Loss: 1.0472060441970825, Total_reward: 107.0
Epoch: 90, Loss: 1.321643590927124, Total_reward: 125.0
Epoch: 100, Loss: 1.1223196983337402, Total_reward: 124.0
Epoch: 110, Loss: 3.9136409759521484, Total_reward: 127.0
Epoch: 120, Loss: 2.704730749130249, Total_reward: 134.0
Epoch: 130, Loss: 3.3320040702819824, Total_reward: 94.0
Epoch: 140, Loss: 3.2186837196350098, Total_reward: 129.0
Epoch: 150, Loss: 3.0914111137390137, Total_reward: 156.0
Epoch: 160, Loss: 3.835188150405884, Total_reward: 115.0
Epoch: 170, Loss: 2.5615837574005127, Total_reward: 122.0
Epoch: 180, Loss: 4.595071315765381, Total_reward: 116.0
Epoch: 190, Loss: 3.4016199111938477, Total_reward: 121.0
Epoch: 200, Loss: 3.956190824508667, Total_reward: 88.0
Epoch: 210, Loss: 5.332021713256836, Total_reward: 100.0
Epoch: 220, Loss: 6.307662487030029, Total_reward: 127.0
Epoch: 230, Loss: 5.76395845413208, Total_reward: 119.0
Epoch: 240, Loss: 6.66315221786499, Total_reward: 130.0
Epoch: 250, Loss: 7.338305950164795, Total_reward: 131.0
Epoch: 260, Loss: 8.535221099853516, Total_reward: 132.0
Epoch: 270, Loss: 5.269773483276367, Total_reward: 145.0
Epoch: 280, Loss: 4.052546977996826, Total_reward: 157.0
Epoch: 290, Loss: 3.2119877338409424, Total_reward: 169.0
Average reward over 10 runs (invisible): 17.4
result : 17.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.2133389413356781, Total_reward: 109.0
Epoch: 20, Loss: 1.5544450283050537, Total_reward: 99.0
Epoch: 30, Loss: 2.3860483169555664, Total_reward: 114.0
Epoch: 40, Loss: 3.392601728439331, Total_reward: 102.0
Epoch: 50, Loss: 8.49848747253418, Total_reward: 105.0
Epoch: 60, Loss: 8.255988121032715, Total_reward: 112.0
Epoch: 70, Loss: 4.433335304260254, Total_reward: 104.0
Epoch: 80, Loss: 11.285165786743164, Total_reward: 88.0
Epoch: 90, Loss: 11.939142227172852, Total_reward: 129.0
Epoch: 100, Loss: 20.06960105895996, Total_reward: 98.0
Epoch: 110, Loss: 10.614489555358887, Total_reward: 93.0
Epoch: 120, Loss: 12.705524444580078, Total_reward: 124.0
Epoch: 130, Loss: 11.32852840423584, Total_reward: 127.0
Epoch: 140, Loss: 6.686239242553711, Total_reward: 137.0
Epoch: 150, Loss: 13.234795570373535, Total_reward: 107.0
Epoch: 160, Loss: 15.074485778808594, Total_reward: 141.0
Epoch: 170, Loss: 11.39875316619873, Total_reward: 154.0
Epoch: 180, Loss: 7.444096088409424, Total_reward: 126.0
Epoch: 190, Loss: 15.79797649383545, Total_reward: 135.0
Epoch: 200, Loss: 22.395069122314453, Total_reward: 103.0
Epoch: 210, Loss: 12.943442344665527, Total_reward: 85.0
Epoch: 220, Loss: 33.953941345214844, Total_reward: 147.0
Epoch: 230, Loss: 19.50258445739746, Total_reward: 139.0
Epoch: 240, Loss: 16.05142593383789, Total_reward: 157.0
Epoch: 250, Loss: 13.268671989440918, Total_reward: 152.0
Epoch: 260, Loss: 26.854398727416992, Total_reward: 162.0
Epoch: 270, Loss: 15.486437797546387, Total_reward: 167.0
Epoch: 280, Loss: 13.966068267822266, Total_reward: 146.0
Epoch: 290, Loss: 9.671283721923828, Total_reward: 121.0
Average reward over 10 runs (invisible): 114.2
result : 114.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.08992164582014084, Total_reward: 111.0
Epoch: 20, Loss: 0.11606626957654953, Total_reward: 133.0
Epoch: 30, Loss: 0.22243210673332214, Total_reward: 151.0
Epoch: 40, Loss: 0.17305324971675873, Total_reward: 148.0
Epoch: 50, Loss: 0.3580428659915924, Total_reward: 159.0
Epoch: 60, Loss: 0.30339929461479187, Total_reward: 135.0
Epoch: 70, Loss: 1.2895636558532715, Total_reward: 153.0
Epoch: 80, Loss: 0.8966666460037231, Total_reward: 147.0
Epoch: 90, Loss: 1.149298071861267, Total_reward: 139.0
Epoch: 100, Loss: 1.6251659393310547, Total_reward: 151.0
Epoch: 110, Loss: 2.2465717792510986, Total_reward: 135.0
Epoch: 120, Loss: 2.1650917530059814, Total_reward: 142.0
Epoch: 130, Loss: 3.9030535221099854, Total_reward: 147.0
Epoch: 140, Loss: 2.822202444076538, Total_reward: 131.0
Epoch: 150, Loss: 4.318399429321289, Total_reward: 123.0
Epoch: 160, Loss: 1.9283177852630615, Total_reward: 160.0
Epoch: 170, Loss: 3.511089324951172, Total_reward: 149.0
Epoch: 180, Loss: 2.2445435523986816, Total_reward: 161.0
Epoch: 190, Loss: 3.225226402282715, Total_reward: 159.0
Epoch: 200, Loss: 3.192377805709839, Total_reward: 180.0
Epoch: 210, Loss: 2.5462379455566406, Total_reward: 130.0
Epoch: 220, Loss: 2.4544906616210938, Total_reward: 107.0
Epoch: 230, Loss: 5.348110675811768, Total_reward: 106.0
Epoch: 240, Loss: 3.954850435256958, Total_reward: 139.0
Epoch: 250, Loss: 3.4791574478149414, Total_reward: 163.0
Epoch: 260, Loss: 6.583349227905273, Total_reward: 158.0
Epoch: 270, Loss: 5.223413467407227, Total_reward: 133.0
Epoch: 280, Loss: 4.400052070617676, Total_reward: 94.0
Epoch: 290, Loss: 7.296370506286621, Total_reward: 104.0
Average reward over 10 runs (invisible): 10.0
result : 10.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.12373485416173935, Total_reward: 114.0
Epoch: 20, Loss: 0.5461982488632202, Total_reward: 128.0
Epoch: 30, Loss: 2.2791049480438232, Total_reward: 160.0
Epoch: 40, Loss: 3.3329033851623535, Total_reward: 145.0
Epoch: 50, Loss: 6.513568878173828, Total_reward: 159.0
Epoch: 60, Loss: 6.896751880645752, Total_reward: 205.0
Epoch: 70, Loss: 5.8489179611206055, Total_reward: 155.0
Epoch: 80, Loss: 6.52233362197876, Total_reward: 145.0
Epoch: 90, Loss: 8.753931045532227, Total_reward: 130.0
Epoch: 100, Loss: 11.54497241973877, Total_reward: 147.0
Epoch: 110, Loss: 20.3955078125, Total_reward: 111.0
Epoch: 120, Loss: 16.90426254272461, Total_reward: 172.0
Epoch: 130, Loss: 13.760030746459961, Total_reward: 193.0
Epoch: 140, Loss: 17.798015594482422, Total_reward: 157.0
Epoch: 150, Loss: 8.792389869689941, Total_reward: 102.0
Epoch: 160, Loss: 17.486099243164062, Total_reward: 119.0
Epoch: 170, Loss: 13.049057960510254, Total_reward: 133.0
Epoch: 180, Loss: 18.252429962158203, Total_reward: 105.0
Epoch: 190, Loss: 16.428190231323242, Total_reward: 149.0
Epoch: 200, Loss: 16.098133087158203, Total_reward: 102.0
Epoch: 210, Loss: 15.301950454711914, Total_reward: 148.0
Epoch: 220, Loss: 19.12019920349121, Total_reward: 106.0
Epoch: 230, Loss: 15.459657669067383, Total_reward: 109.0
Epoch: 240, Loss: 16.171113967895508, Total_reward: 91.0
Epoch: 250, Loss: 21.832176208496094, Total_reward: 96.0
Epoch: 260, Loss: 21.342281341552734, Total_reward: 127.0
Epoch: 270, Loss: 17.363557815551758, Total_reward: 158.0
Epoch: 280, Loss: 16.81321144104004, Total_reward: 147.0
Epoch: 290, Loss: 23.609905242919922, Total_reward: 132.0
Average reward over 10 runs (invisible): 9.0
result : 9.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.07618585973978043, Total_reward: 104.0
Epoch: 20, Loss: 0.08571195602416992, Total_reward: 95.0
Epoch: 30, Loss: 0.21122927963733673, Total_reward: 144.0
Epoch: 40, Loss: 0.400100439786911, Total_reward: 134.0
Epoch: 50, Loss: 0.7588778734207153, Total_reward: 158.0
Epoch: 60, Loss: 0.775749921798706, Total_reward: 165.0
Epoch: 70, Loss: 0.6129162311553955, Total_reward: 204.0
Epoch: 80, Loss: 0.3087327778339386, Total_reward: 222.0
Epoch: 90, Loss: 1.678109884262085, Total_reward: 257.0
Epoch: 100, Loss: 0.6089218258857727, Total_reward: 305.0
Epoch: 110, Loss: 1.2561209201812744, Total_reward: 155.0
Epoch: 120, Loss: 1.306839942932129, Total_reward: 149.0
Epoch: 130, Loss: 2.9371111392974854, Total_reward: 158.0
Epoch: 140, Loss: 2.5510284900665283, Total_reward: 180.0
Epoch: 150, Loss: 1.9261009693145752, Total_reward: 150.0
Epoch: 160, Loss: 2.2955949306488037, Total_reward: 205.0
Epoch: 170, Loss: 3.259666681289673, Total_reward: 137.0
Epoch: 180, Loss: 3.3087074756622314, Total_reward: 232.0
Epoch: 190, Loss: 4.180613994598389, Total_reward: 168.0
Epoch: 200, Loss: 2.201617956161499, Total_reward: 216.0
Epoch: 210, Loss: 3.8521316051483154, Total_reward: 165.0
Epoch: 220, Loss: 3.5790481567382812, Total_reward: 117.0
Epoch: 230, Loss: 7.281200885772705, Total_reward: 150.0
Epoch: 240, Loss: 6.166791915893555, Total_reward: 115.0
Epoch: 250, Loss: 4.162987232208252, Total_reward: 146.0
Epoch: 260, Loss: 5.924233913421631, Total_reward: 207.0
Epoch: 270, Loss: 9.845844268798828, Total_reward: 167.0
Epoch: 280, Loss: 5.124980926513672, Total_reward: 197.0
Epoch: 290, Loss: 5.744352340698242, Total_reward: 245.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1437457799911499, Total_reward: 183.0
Epoch: 20, Loss: 1.2139623165130615, Total_reward: 118.0
Epoch: 30, Loss: 2.6929383277893066, Total_reward: 157.0
Epoch: 40, Loss: 2.6222031116485596, Total_reward: 153.0
Epoch: 50, Loss: 5.5898518562316895, Total_reward: 186.0
Epoch: 60, Loss: 7.609347343444824, Total_reward: 153.0
Epoch: 70, Loss: 5.028120994567871, Total_reward: 141.0
Epoch: 80, Loss: 7.920121192932129, Total_reward: 135.0
Epoch: 90, Loss: 8.328347206115723, Total_reward: 146.0
Epoch: 100, Loss: 14.241273880004883, Total_reward: 105.0
Epoch: 110, Loss: 5.542317867279053, Total_reward: 158.0
Epoch: 120, Loss: 12.227375030517578, Total_reward: 113.0
Epoch: 130, Loss: 17.235008239746094, Total_reward: 139.0
Epoch: 140, Loss: 17.21691131591797, Total_reward: 106.0
Epoch: 150, Loss: 12.431580543518066, Total_reward: 92.0
Epoch: 160, Loss: 21.816946029663086, Total_reward: 113.0
Epoch: 170, Loss: 15.164376258850098, Total_reward: 181.0
Epoch: 180, Loss: 21.900951385498047, Total_reward: 168.0
Epoch: 190, Loss: 15.314906120300293, Total_reward: 160.0
Epoch: 200, Loss: 15.36227035522461, Total_reward: 131.0
Epoch: 210, Loss: 11.844155311584473, Total_reward: 162.0
Epoch: 220, Loss: 19.58230209350586, Total_reward: 123.0
Epoch: 230, Loss: 34.640655517578125, Total_reward: 88.0
Epoch: 240, Loss: 35.08058547973633, Total_reward: 98.0
Epoch: 250, Loss: 25.004987716674805, Total_reward: 123.0
Epoch: 260, Loss: 19.167085647583008, Total_reward: 163.0
Epoch: 270, Loss: 19.241586685180664, Total_reward: 160.0
Epoch: 280, Loss: 20.090290069580078, Total_reward: 189.0
Epoch: 290, Loss: 38.67790603637695, Total_reward: 181.0
Average reward over 10 runs (invisible): 45.9
result : 45.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.04693162441253662, Total_reward: 126.0
Epoch: 20, Loss: 0.09978219866752625, Total_reward: 90.0
Epoch: 30, Loss: 0.46677353978157043, Total_reward: 113.0
Epoch: 40, Loss: 0.5088741779327393, Total_reward: 108.0
Epoch: 50, Loss: 0.7021909356117249, Total_reward: 117.0
Epoch: 60, Loss: 1.0438786745071411, Total_reward: 127.0
Epoch: 70, Loss: 1.3111931085586548, Total_reward: 213.0
Epoch: 80, Loss: 0.9341715574264526, Total_reward: 166.0
Epoch: 90, Loss: 1.9099905490875244, Total_reward: 132.0
Epoch: 100, Loss: 1.8935126066207886, Total_reward: 123.0
Epoch: 110, Loss: 1.1797821521759033, Total_reward: 224.0
Epoch: 120, Loss: 1.170957088470459, Total_reward: 200.0
Epoch: 130, Loss: 2.6721506118774414, Total_reward: 175.0
Epoch: 140, Loss: 2.7255964279174805, Total_reward: 165.0
Epoch: 150, Loss: 2.2839853763580322, Total_reward: 141.0
Epoch: 160, Loss: 2.634881019592285, Total_reward: 198.0
Epoch: 170, Loss: 2.6181867122650146, Total_reward: 230.0
Epoch: 180, Loss: 3.527446985244751, Total_reward: 153.0
Epoch: 190, Loss: 4.262025833129883, Total_reward: 144.0
Epoch: 200, Loss: 1.7911183834075928, Total_reward: 113.0
Epoch: 210, Loss: 5.420414447784424, Total_reward: 88.0
Epoch: 220, Loss: 4.409394264221191, Total_reward: 111.0
Epoch: 230, Loss: 9.174362182617188, Total_reward: 96.0
Epoch: 240, Loss: 12.287275314331055, Total_reward: 96.0
Epoch: 250, Loss: 11.28525447845459, Total_reward: 131.0
Epoch: 260, Loss: 7.831462860107422, Total_reward: 152.0
Epoch: 270, Loss: 9.034512519836426, Total_reward: 202.0
Epoch: 280, Loss: 6.4087042808532715, Total_reward: 166.0
Epoch: 290, Loss: 5.217180252075195, Total_reward: 193.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.14464810490608215, Total_reward: 113.0
Epoch: 20, Loss: 0.5667590498924255, Total_reward: 100.0
Epoch: 30, Loss: 0.4351951479911804, Total_reward: 111.0
Epoch: 40, Loss: 1.6954596042633057, Total_reward: 103.0
Epoch: 50, Loss: 5.484277725219727, Total_reward: 103.0
Epoch: 60, Loss: 3.4758191108703613, Total_reward: 134.0
Epoch: 70, Loss: 6.494167804718018, Total_reward: 117.0
Epoch: 80, Loss: 9.482230186462402, Total_reward: 104.0
Epoch: 90, Loss: 6.523043155670166, Total_reward: 117.0
Epoch: 100, Loss: 11.398528099060059, Total_reward: 119.0
Epoch: 110, Loss: 15.046829223632812, Total_reward: 146.0
Epoch: 120, Loss: 13.821368217468262, Total_reward: 129.0
Epoch: 130, Loss: 15.338213920593262, Total_reward: 110.0
Epoch: 140, Loss: 13.93410587310791, Total_reward: 131.0
Epoch: 150, Loss: 12.79542064666748, Total_reward: 130.0
Epoch: 160, Loss: 20.785329818725586, Total_reward: 117.0
Epoch: 170, Loss: 14.60634708404541, Total_reward: 124.0
Epoch: 180, Loss: 21.600711822509766, Total_reward: 115.0
Epoch: 190, Loss: 18.555988311767578, Total_reward: 125.0
Epoch: 200, Loss: 27.022621154785156, Total_reward: 122.0
Epoch: 210, Loss: 16.451190948486328, Total_reward: 109.0
Epoch: 220, Loss: 26.065061569213867, Total_reward: 118.0
Epoch: 230, Loss: 16.662586212158203, Total_reward: 100.0
Epoch: 240, Loss: 16.26643180847168, Total_reward: 127.0
Epoch: 250, Loss: 19.43095588684082, Total_reward: 136.0
Epoch: 260, Loss: 22.04568099975586, Total_reward: 157.0
Epoch: 270, Loss: 29.171669006347656, Total_reward: 119.0
Epoch: 280, Loss: 19.586458206176758, Total_reward: 150.0
Epoch: 290, Loss: 15.819188117980957, Total_reward: 125.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.05241917073726654, Total_reward: 112.0
Epoch: 20, Loss: 0.16809943318367004, Total_reward: 86.0
Epoch: 30, Loss: 0.36727404594421387, Total_reward: 100.0
Epoch: 40, Loss: 0.3665218949317932, Total_reward: 92.0
Epoch: 50, Loss: 0.9207606315612793, Total_reward: 95.0
Epoch: 60, Loss: 0.7223303318023682, Total_reward: 112.0
Epoch: 70, Loss: 1.6075055599212646, Total_reward: 106.0
Epoch: 80, Loss: 1.558556318283081, Total_reward: 124.0
Epoch: 90, Loss: 1.9648317098617554, Total_reward: 145.0
Epoch: 100, Loss: 1.5925577878952026, Total_reward: 153.0
Epoch: 110, Loss: 2.112150192260742, Total_reward: 147.0
Epoch: 120, Loss: 2.38413667678833, Total_reward: 151.0
Epoch: 130, Loss: 3.229487419128418, Total_reward: 148.0
Epoch: 140, Loss: 2.619828224182129, Total_reward: 145.0
Epoch: 150, Loss: 2.9206697940826416, Total_reward: 149.0
Epoch: 160, Loss: 1.4968620538711548, Total_reward: 149.0
Epoch: 170, Loss: 2.4984302520751953, Total_reward: 153.0
Epoch: 180, Loss: 2.565612316131592, Total_reward: 145.0
Epoch: 190, Loss: 5.427062511444092, Total_reward: 156.0
Epoch: 200, Loss: 4.303203582763672, Total_reward: 145.0
Epoch: 210, Loss: 2.8388781547546387, Total_reward: 143.0
Epoch: 220, Loss: 3.6392922401428223, Total_reward: 144.0
Epoch: 230, Loss: 5.3180012702941895, Total_reward: 149.0
Epoch: 240, Loss: 6.704156398773193, Total_reward: 144.0
Epoch: 250, Loss: 4.426441192626953, Total_reward: 161.0
Epoch: 260, Loss: 6.668735504150391, Total_reward: 145.0
Epoch: 270, Loss: 8.166613578796387, Total_reward: 141.0
Epoch: 280, Loss: 6.378459930419922, Total_reward: 142.0
Epoch: 290, Loss: 5.563016414642334, Total_reward: 114.0
Average reward over 10 runs (invisible): 12.9
result : 12.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.19229808449745178, Total_reward: 129.0
Epoch: 20, Loss: 0.2192816287279129, Total_reward: 132.0
Epoch: 30, Loss: 0.3861292004585266, Total_reward: 148.0
Epoch: 40, Loss: 1.3139612674713135, Total_reward: 110.0
Epoch: 50, Loss: 2.254347562789917, Total_reward: 120.0
Epoch: 60, Loss: 5.600810527801514, Total_reward: 134.0
Epoch: 70, Loss: 4.166761875152588, Total_reward: 158.0
Epoch: 80, Loss: 3.8754618167877197, Total_reward: 145.0
Epoch: 90, Loss: 6.5931010246276855, Total_reward: 125.0
Epoch: 100, Loss: 10.254487991333008, Total_reward: 94.0
Epoch: 110, Loss: 16.21499252319336, Total_reward: 129.0
Epoch: 120, Loss: 13.432101249694824, Total_reward: 122.0
Epoch: 130, Loss: 13.42773723602295, Total_reward: 136.0
Epoch: 140, Loss: 9.205918312072754, Total_reward: 124.0
Epoch: 150, Loss: 21.904582977294922, Total_reward: 103.0
Epoch: 160, Loss: 17.229549407958984, Total_reward: 125.0
Epoch: 170, Loss: 18.887908935546875, Total_reward: 141.0
Epoch: 180, Loss: 16.673742294311523, Total_reward: 126.0
Epoch: 190, Loss: 12.002778053283691, Total_reward: 133.0
Epoch: 200, Loss: 13.830842971801758, Total_reward: 110.0
Epoch: 210, Loss: 24.719194412231445, Total_reward: 149.0
Epoch: 220, Loss: 26.945165634155273, Total_reward: 154.0
Epoch: 230, Loss: 18.316287994384766, Total_reward: 133.0
Epoch: 240, Loss: 21.49423599243164, Total_reward: 146.0
Epoch: 250, Loss: 23.77269172668457, Total_reward: 147.0
Epoch: 260, Loss: 7.813667297363281, Total_reward: 171.0
Epoch: 270, Loss: 19.63829231262207, Total_reward: 134.0
Epoch: 280, Loss: 16.699583053588867, Total_reward: 116.0
Epoch: 290, Loss: 40.62216567993164, Total_reward: 108.0
Average reward over 10 runs (invisible): 10.4
result : 10.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.05805967003107071, Total_reward: 123.0
Epoch: 20, Loss: 0.1416105479001999, Total_reward: 91.0
Epoch: 30, Loss: 0.37685537338256836, Total_reward: 99.0
Epoch: 40, Loss: 0.4121144413948059, Total_reward: 119.0
Epoch: 50, Loss: 0.6459932327270508, Total_reward: 167.0
Epoch: 60, Loss: 0.7715861201286316, Total_reward: 140.0
Epoch: 70, Loss: 0.8897587060928345, Total_reward: 137.0
Epoch: 80, Loss: 2.181593894958496, Total_reward: 114.0
Epoch: 90, Loss: 1.332203984260559, Total_reward: 139.0
Epoch: 100, Loss: 0.6670002937316895, Total_reward: 136.0
Epoch: 110, Loss: 2.742100715637207, Total_reward: 153.0
Epoch: 120, Loss: 1.189258337020874, Total_reward: 135.0
Epoch: 130, Loss: 3.3998990058898926, Total_reward: 121.0
Epoch: 140, Loss: 3.655230760574341, Total_reward: 143.0
Epoch: 150, Loss: 3.2383406162261963, Total_reward: 121.0
Epoch: 160, Loss: 1.931933045387268, Total_reward: 151.0
Epoch: 170, Loss: 2.6923301219940186, Total_reward: 127.0
Epoch: 180, Loss: 5.625515937805176, Total_reward: 149.0
Epoch: 190, Loss: 1.6951764822006226, Total_reward: 145.0
Epoch: 200, Loss: 3.60599422454834, Total_reward: 112.0
Epoch: 210, Loss: 5.098015308380127, Total_reward: 181.0
Epoch: 220, Loss: 4.31995153427124, Total_reward: 143.0
Epoch: 230, Loss: 4.141230583190918, Total_reward: 119.0
Epoch: 240, Loss: 3.325488805770874, Total_reward: 142.0
Epoch: 250, Loss: 8.148221969604492, Total_reward: 139.0
Epoch: 260, Loss: 5.696390628814697, Total_reward: 126.0
Epoch: 270, Loss: 6.048913478851318, Total_reward: 149.0
Epoch: 280, Loss: 6.060647487640381, Total_reward: 144.0
Epoch: 290, Loss: 6.1890106201171875, Total_reward: 163.0
Average reward over 10 runs (invisible): 21.0
result : 21.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.12993766367435455, Total_reward: 109.0
Epoch: 20, Loss: 1.0026159286499023, Total_reward: 95.0
Epoch: 30, Loss: 2.4454355239868164, Total_reward: 91.0
Epoch: 40, Loss: 2.312807559967041, Total_reward: 190.0
Epoch: 50, Loss: 5.3909993171691895, Total_reward: 147.0
Epoch: 60, Loss: 5.562941551208496, Total_reward: 130.0
Epoch: 70, Loss: 9.696427345275879, Total_reward: 138.0
Epoch: 80, Loss: 11.051490783691406, Total_reward: 167.0
Epoch: 90, Loss: 12.440308570861816, Total_reward: 155.0
Epoch: 100, Loss: 9.594606399536133, Total_reward: 105.0
Epoch: 110, Loss: 11.887672424316406, Total_reward: 155.0
Epoch: 120, Loss: 18.7149658203125, Total_reward: 242.0
Epoch: 130, Loss: 5.576474666595459, Total_reward: 122.0
Epoch: 140, Loss: 11.162355422973633, Total_reward: 146.0
Epoch: 150, Loss: 16.19520378112793, Total_reward: 103.0
Epoch: 160, Loss: 15.07656478881836, Total_reward: 119.0
Epoch: 170, Loss: 13.552072525024414, Total_reward: 153.0
Epoch: 180, Loss: 15.537452697753906, Total_reward: 139.0
Epoch: 190, Loss: 10.26974105834961, Total_reward: 118.0
Epoch: 200, Loss: 20.956357955932617, Total_reward: 159.0
Epoch: 210, Loss: 24.98731231689453, Total_reward: 163.0
Epoch: 220, Loss: 13.225713729858398, Total_reward: 129.0
Epoch: 230, Loss: 18.86064910888672, Total_reward: 146.0
Epoch: 240, Loss: 18.871929168701172, Total_reward: 106.0
Epoch: 250, Loss: 19.821640014648438, Total_reward: 90.0
Epoch: 260, Loss: 19.220579147338867, Total_reward: 162.0
Epoch: 270, Loss: 11.510451316833496, Total_reward: 179.0
Epoch: 280, Loss: 21.093564987182617, Total_reward: 122.0
Epoch: 290, Loss: 33.148406982421875, Total_reward: 120.0
Average reward over 10 runs (invisible): 9.7
result : 9.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.027217864990234375, Total_reward: 138.0
Epoch: 20, Loss: 0.05590811371803284, Total_reward: 182.0
Epoch: 30, Loss: 0.11842943727970123, Total_reward: 178.0
Epoch: 40, Loss: 0.3195432126522064, Total_reward: 106.0
Epoch: 50, Loss: 0.4322843849658966, Total_reward: 208.0
Epoch: 60, Loss: 0.8127657175064087, Total_reward: 197.0
Epoch: 70, Loss: 1.2097344398498535, Total_reward: 229.0
Epoch: 80, Loss: 1.7904129028320312, Total_reward: 127.0
Epoch: 90, Loss: 1.3116525411605835, Total_reward: 159.0
Epoch: 100, Loss: 1.0881245136260986, Total_reward: 145.0
Epoch: 110, Loss: 2.841920852661133, Total_reward: 95.0
Epoch: 120, Loss: 1.8567609786987305, Total_reward: 178.0
Epoch: 130, Loss: 2.325974464416504, Total_reward: 118.0
Epoch: 140, Loss: 2.997262716293335, Total_reward: 137.0
Epoch: 150, Loss: 4.295194625854492, Total_reward: 104.0
Epoch: 160, Loss: 4.055117607116699, Total_reward: 158.0
Epoch: 170, Loss: 3.597019910812378, Total_reward: 131.0
Epoch: 180, Loss: 2.9114346504211426, Total_reward: 178.0
Epoch: 190, Loss: 3.0819435119628906, Total_reward: 249.0
Epoch: 200, Loss: 2.2424721717834473, Total_reward: 192.0
Epoch: 210, Loss: 5.655517101287842, Total_reward: 158.0
Epoch: 220, Loss: 5.477200031280518, Total_reward: 131.0
Epoch: 230, Loss: 5.4914655685424805, Total_reward: 171.0
Epoch: 240, Loss: 4.328131675720215, Total_reward: 195.0
Epoch: 250, Loss: 4.559769630432129, Total_reward: 167.0
Epoch: 260, Loss: 5.400591850280762, Total_reward: 145.0
Epoch: 270, Loss: 6.835182189941406, Total_reward: 127.0
Epoch: 280, Loss: 8.030477523803711, Total_reward: 112.0
Epoch: 290, Loss: 8.31654167175293, Total_reward: 129.0
Average reward over 10 runs (invisible): 9.3
result : 9.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.06306488811969757, Total_reward: 153.0
Epoch: 20, Loss: 0.5002067685127258, Total_reward: 102.0
Epoch: 30, Loss: 1.7520900964736938, Total_reward: 144.0
Epoch: 40, Loss: 2.7936935424804688, Total_reward: 102.0
Epoch: 50, Loss: 4.032336711883545, Total_reward: 153.0
Epoch: 60, Loss: 8.252848625183105, Total_reward: 116.0
Epoch: 70, Loss: 4.753647327423096, Total_reward: 151.0
Epoch: 80, Loss: 10.372383117675781, Total_reward: 115.0
Epoch: 90, Loss: 11.808331489562988, Total_reward: 187.0
Epoch: 100, Loss: 11.684883117675781, Total_reward: 105.0
Epoch: 110, Loss: 12.069950103759766, Total_reward: 160.0
Epoch: 120, Loss: 14.347845077514648, Total_reward: 134.0
Epoch: 130, Loss: 12.729386329650879, Total_reward: 144.0
Epoch: 140, Loss: 21.961301803588867, Total_reward: 155.0
Epoch: 150, Loss: 16.996551513671875, Total_reward: 108.0
Epoch: 160, Loss: 8.578890800476074, Total_reward: 141.0
Epoch: 170, Loss: 23.814151763916016, Total_reward: 126.0
Epoch: 180, Loss: 15.77242374420166, Total_reward: 185.0
Epoch: 190, Loss: 6.584785461425781, Total_reward: 142.0
Epoch: 200, Loss: 15.862998962402344, Total_reward: 132.0
Epoch: 210, Loss: 13.229497909545898, Total_reward: 180.0
Epoch: 220, Loss: 22.35870361328125, Total_reward: 124.0
Epoch: 230, Loss: 10.944091796875, Total_reward: 144.0
Epoch: 240, Loss: 20.955833435058594, Total_reward: 137.0
Epoch: 250, Loss: 14.396223068237305, Total_reward: 186.0
Epoch: 260, Loss: 25.519729614257812, Total_reward: 196.0
Epoch: 270, Loss: 12.852827072143555, Total_reward: 224.0
Epoch: 280, Loss: 11.994067192077637, Total_reward: 138.0
Epoch: 290, Loss: 23.094820022583008, Total_reward: 125.0
Average reward over 10 runs (invisible): 12.0
result : 12.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.054249707609415054, Total_reward: 134.0
Epoch: 20, Loss: 0.03183887526392937, Total_reward: 125.0
Epoch: 30, Loss: 0.06281718611717224, Total_reward: 111.0
Epoch: 40, Loss: 0.4332119822502136, Total_reward: 158.0
Epoch: 50, Loss: 0.22935837507247925, Total_reward: 149.0
Epoch: 60, Loss: 0.5419710874557495, Total_reward: 109.0
Epoch: 70, Loss: 0.8217491507530212, Total_reward: 139.0
Epoch: 80, Loss: 1.8030788898468018, Total_reward: 130.0
Epoch: 90, Loss: 1.785346269607544, Total_reward: 91.0
Epoch: 100, Loss: 1.6398500204086304, Total_reward: 88.0
Epoch: 110, Loss: 1.8717299699783325, Total_reward: 136.0
Epoch: 120, Loss: 3.1363017559051514, Total_reward: 108.0
Epoch: 130, Loss: 2.370490312576294, Total_reward: 153.0
Epoch: 140, Loss: 1.6504496335983276, Total_reward: 118.0
Epoch: 150, Loss: 2.734422206878662, Total_reward: 103.0
Epoch: 160, Loss: 5.027999401092529, Total_reward: 114.0
Epoch: 170, Loss: 2.585789203643799, Total_reward: 112.0
Epoch: 180, Loss: 3.5984795093536377, Total_reward: 91.0
Epoch: 190, Loss: 5.491196155548096, Total_reward: 149.0
Epoch: 200, Loss: 3.747018814086914, Total_reward: 190.0
Epoch: 210, Loss: 3.1317057609558105, Total_reward: 156.0
Epoch: 220, Loss: 3.664828062057495, Total_reward: 100.0
Epoch: 230, Loss: 4.552513599395752, Total_reward: 114.0
Epoch: 240, Loss: 5.928749084472656, Total_reward: 149.0
Epoch: 250, Loss: 8.967521667480469, Total_reward: 152.0
Epoch: 260, Loss: 7.604941368103027, Total_reward: 130.0
Epoch: 270, Loss: 6.2658586502075195, Total_reward: 97.0
Epoch: 280, Loss: 10.520654678344727, Total_reward: 105.0
Epoch: 290, Loss: 4.7419514656066895, Total_reward: 185.0
Average reward over 10 runs (invisible): 59.8
result : 59.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.22339671850204468, Total_reward: 157.0
Epoch: 20, Loss: 0.9209335446357727, Total_reward: 118.0
Epoch: 30, Loss: 2.152000665664673, Total_reward: 89.0
Epoch: 40, Loss: 2.3486135005950928, Total_reward: 99.0
Epoch: 50, Loss: 4.254115104675293, Total_reward: 109.0
Epoch: 60, Loss: 5.398192405700684, Total_reward: 151.0
Epoch: 70, Loss: 4.525870323181152, Total_reward: 161.0
Epoch: 80, Loss: 5.620256423950195, Total_reward: 136.0
Epoch: 90, Loss: 7.688780784606934, Total_reward: 166.0
Epoch: 100, Loss: 8.185711860656738, Total_reward: 133.0
Epoch: 110, Loss: 12.231229782104492, Total_reward: 135.0
Epoch: 120, Loss: 7.062170028686523, Total_reward: 116.0
Epoch: 130, Loss: 10.764237403869629, Total_reward: 121.0
Epoch: 140, Loss: 14.299592971801758, Total_reward: 174.0
Epoch: 150, Loss: 18.835372924804688, Total_reward: 111.0
Epoch: 160, Loss: 14.878944396972656, Total_reward: 130.0
Epoch: 170, Loss: 15.316914558410645, Total_reward: 105.0
Epoch: 180, Loss: 17.750099182128906, Total_reward: 94.0
Epoch: 190, Loss: 18.266530990600586, Total_reward: 102.0
Epoch: 200, Loss: 23.19802474975586, Total_reward: 91.0
Epoch: 210, Loss: 14.139016151428223, Total_reward: 123.0
Epoch: 220, Loss: 12.979040145874023, Total_reward: 118.0
Epoch: 230, Loss: 9.879839897155762, Total_reward: 132.0
Epoch: 240, Loss: 13.905622482299805, Total_reward: 144.0
Epoch: 250, Loss: 14.850669860839844, Total_reward: 126.0
Epoch: 260, Loss: 15.985272407531738, Total_reward: 100.0
Epoch: 270, Loss: 16.438934326171875, Total_reward: 119.0
Epoch: 280, Loss: 18.312925338745117, Total_reward: 115.0
Epoch: 290, Loss: 9.107454299926758, Total_reward: 141.0
Average reward over 10 runs (invisible): 24.9
result : 24.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.140899658203125, Total_reward: 96.0
Epoch: 20, Loss: 0.13046878576278687, Total_reward: 95.0
Epoch: 30, Loss: 0.4157142639160156, Total_reward: 93.0
Epoch: 40, Loss: 0.577907145023346, Total_reward: 83.0
Epoch: 50, Loss: 1.5446438789367676, Total_reward: 103.0
Epoch: 60, Loss: 0.872616171836853, Total_reward: 84.0
Epoch: 70, Loss: 1.9095110893249512, Total_reward: 114.0
Epoch: 80, Loss: 1.4613052606582642, Total_reward: 180.0
Epoch: 90, Loss: 2.123598098754883, Total_reward: 186.0
Epoch: 100, Loss: 0.6770833730697632, Total_reward: 170.0
Epoch: 110, Loss: 1.9330109357833862, Total_reward: 175.0
Epoch: 120, Loss: 2.3101999759674072, Total_reward: 168.0
Epoch: 130, Loss: 3.0004501342773438, Total_reward: 110.0
Epoch: 140, Loss: 2.7138707637786865, Total_reward: 112.0
Epoch: 150, Loss: 3.876932382583618, Total_reward: 111.0
Epoch: 160, Loss: 5.449055194854736, Total_reward: 132.0
Epoch: 170, Loss: 4.581721782684326, Total_reward: 156.0
Epoch: 180, Loss: 2.8690390586853027, Total_reward: 179.0
Epoch: 190, Loss: 6.981017589569092, Total_reward: 175.0
Epoch: 200, Loss: 5.923212051391602, Total_reward: 170.0
Epoch: 210, Loss: 5.445460796356201, Total_reward: 126.0
Epoch: 220, Loss: 6.967010974884033, Total_reward: 140.0
Epoch: 230, Loss: 7.399476528167725, Total_reward: 98.0
Epoch: 240, Loss: 5.257700443267822, Total_reward: 151.0
Epoch: 250, Loss: 9.610331535339355, Total_reward: 132.0
Epoch: 260, Loss: 5.643069267272949, Total_reward: 143.0
Epoch: 270, Loss: 6.892864227294922, Total_reward: 146.0
Epoch: 280, Loss: 8.064695358276367, Total_reward: 127.0
Epoch: 290, Loss: 8.137651443481445, Total_reward: 110.0
Average reward over 10 runs (invisible): 9.7
result : 9.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.2190355360507965, Total_reward: 115.0
Epoch: 20, Loss: 1.1214778423309326, Total_reward: 171.0
Epoch: 30, Loss: 1.3069789409637451, Total_reward: 160.0
Epoch: 40, Loss: 1.599515676498413, Total_reward: 142.0
Epoch: 50, Loss: 4.827692031860352, Total_reward: 146.0
Epoch: 60, Loss: 3.572136640548706, Total_reward: 219.0
Epoch: 70, Loss: 8.96553897857666, Total_reward: 158.0
Epoch: 80, Loss: 8.728227615356445, Total_reward: 209.0
Epoch: 90, Loss: 8.97587776184082, Total_reward: 130.0
Epoch: 100, Loss: 5.020138740539551, Total_reward: 156.0
Epoch: 110, Loss: 12.687407493591309, Total_reward: 150.0
Epoch: 120, Loss: 7.538323879241943, Total_reward: 234.0
Epoch: 130, Loss: 12.882548332214355, Total_reward: 212.0
Epoch: 140, Loss: 8.57420825958252, Total_reward: 164.0
Epoch: 150, Loss: 8.433452606201172, Total_reward: 140.0
Epoch: 160, Loss: 17.375215530395508, Total_reward: 163.0
Epoch: 170, Loss: 21.59070587158203, Total_reward: 170.0
Epoch: 180, Loss: 14.379558563232422, Total_reward: 148.0
Epoch: 190, Loss: 16.72085189819336, Total_reward: 143.0
Epoch: 200, Loss: 19.136585235595703, Total_reward: 212.0
Epoch: 210, Loss: 27.54178237915039, Total_reward: 151.0
Epoch: 220, Loss: 30.951215744018555, Total_reward: 189.0
Epoch: 230, Loss: 27.114887237548828, Total_reward: 125.0
Epoch: 240, Loss: 41.54087829589844, Total_reward: 123.0
Epoch: 250, Loss: 24.86998748779297, Total_reward: 161.0
Epoch: 260, Loss: 22.820642471313477, Total_reward: 161.0
Epoch: 270, Loss: 23.006404876708984, Total_reward: 130.0
Epoch: 280, Loss: 34.15470886230469, Total_reward: 135.0
Epoch: 290, Loss: 30.115467071533203, Total_reward: 129.0
Average reward over 10 runs (invisible): 19.9
result : 19.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.12228712439537048, Total_reward: 166.0
Epoch: 20, Loss: 0.1027083545923233, Total_reward: 194.0
Epoch: 30, Loss: 0.407588392496109, Total_reward: 130.0
Epoch: 40, Loss: 0.234547957777977, Total_reward: 171.0
Epoch: 50, Loss: 0.6341606974601746, Total_reward: 139.0
Epoch: 60, Loss: 0.5683256387710571, Total_reward: 121.0
Epoch: 70, Loss: 0.6209670305252075, Total_reward: 152.0
Epoch: 80, Loss: 1.0385535955429077, Total_reward: 143.0
Epoch: 90, Loss: 2.2692413330078125, Total_reward: 155.0
Epoch: 100, Loss: 1.2012841701507568, Total_reward: 183.0
Epoch: 110, Loss: 2.572183847427368, Total_reward: 133.0
Epoch: 120, Loss: 1.5960317850112915, Total_reward: 144.0
Epoch: 130, Loss: 2.4642817974090576, Total_reward: 118.0
Epoch: 140, Loss: 2.0896482467651367, Total_reward: 127.0
Epoch: 150, Loss: 3.868853807449341, Total_reward: 152.0
Epoch: 160, Loss: 3.082118034362793, Total_reward: 158.0
Epoch: 170, Loss: 4.779616355895996, Total_reward: 118.0
Epoch: 180, Loss: 4.190455913543701, Total_reward: 170.0
Epoch: 190, Loss: 3.4654388427734375, Total_reward: 145.0
Epoch: 200, Loss: 7.0726518630981445, Total_reward: 136.0
Epoch: 210, Loss: 9.271656036376953, Total_reward: 95.0
Epoch: 220, Loss: 6.344043254852295, Total_reward: 148.0
Epoch: 230, Loss: 9.242976188659668, Total_reward: 143.0
Epoch: 240, Loss: 7.499940872192383, Total_reward: 131.0
Epoch: 250, Loss: 4.7485198974609375, Total_reward: 192.0
Epoch: 260, Loss: 7.498593330383301, Total_reward: 148.0
Epoch: 270, Loss: 4.6928558349609375, Total_reward: 134.0
Epoch: 280, Loss: 5.477953910827637, Total_reward: 194.0
Epoch: 290, Loss: 7.866828441619873, Total_reward: 160.0
Average reward over 10 runs (invisible): 31.7
result : 31.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.21927303075790405, Total_reward: 107.0
Epoch: 20, Loss: 0.3723914921283722, Total_reward: 198.0
Epoch: 30, Loss: 1.7612760066986084, Total_reward: 143.0
Epoch: 40, Loss: 1.3418855667114258, Total_reward: 97.0
Epoch: 50, Loss: 5.071018218994141, Total_reward: 131.0
Epoch: 60, Loss: 5.508615970611572, Total_reward: 122.0
Epoch: 70, Loss: 3.687861919403076, Total_reward: 219.0
Epoch: 80, Loss: 9.591828346252441, Total_reward: 216.0
Epoch: 90, Loss: 6.149604797363281, Total_reward: 223.0
Epoch: 100, Loss: 11.61385440826416, Total_reward: 105.0
Epoch: 110, Loss: 9.148323059082031, Total_reward: 156.0
Epoch: 120, Loss: 12.990839004516602, Total_reward: 128.0
Epoch: 130, Loss: 10.004416465759277, Total_reward: 118.0
Epoch: 140, Loss: 17.37330436706543, Total_reward: 156.0
Epoch: 150, Loss: 8.259966850280762, Total_reward: 173.0
Epoch: 160, Loss: 14.269852638244629, Total_reward: 162.0
Epoch: 170, Loss: 14.310051918029785, Total_reward: 138.0
Epoch: 180, Loss: 20.878040313720703, Total_reward: 99.0
Epoch: 190, Loss: 27.77351188659668, Total_reward: 104.0
Epoch: 200, Loss: 21.425334930419922, Total_reward: 125.0
Epoch: 210, Loss: 19.097862243652344, Total_reward: 174.0
Epoch: 220, Loss: 30.35157585144043, Total_reward: 276.0
Epoch: 230, Loss: 13.571698188781738, Total_reward: 203.0
Epoch: 240, Loss: 27.617900848388672, Total_reward: 167.0
Epoch: 250, Loss: 20.85073471069336, Total_reward: 161.0
Epoch: 260, Loss: 11.65383529663086, Total_reward: 138.0
Epoch: 270, Loss: 30.33912467956543, Total_reward: 150.0
Epoch: 280, Loss: 22.07497787475586, Total_reward: 115.0
Epoch: 290, Loss: 24.14209747314453, Total_reward: 146.0
Average reward over 10 runs (invisible): 9.9
result : 9.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.2322251945734024, Total_reward: 95.0
Epoch: 20, Loss: 0.25525522232055664, Total_reward: 97.0
Epoch: 30, Loss: 0.4629921615123749, Total_reward: 106.0
Epoch: 40, Loss: 0.797492265701294, Total_reward: 127.0
Epoch: 50, Loss: 0.8317877054214478, Total_reward: 116.0
Epoch: 60, Loss: 1.100966215133667, Total_reward: 145.0
Epoch: 70, Loss: 1.2568767070770264, Total_reward: 99.0
Epoch: 80, Loss: 2.0664799213409424, Total_reward: 160.0
Epoch: 90, Loss: 1.4078806638717651, Total_reward: 140.0
Epoch: 100, Loss: 0.8416743278503418, Total_reward: 131.0
Epoch: 110, Loss: 1.7598413228988647, Total_reward: 170.0
Epoch: 120, Loss: 1.750974178314209, Total_reward: 184.0
Epoch: 130, Loss: 2.0391457080841064, Total_reward: 134.0
Epoch: 140, Loss: 3.55700945854187, Total_reward: 102.0
Epoch: 150, Loss: 4.885624885559082, Total_reward: 106.0
Epoch: 160, Loss: 4.4826579093933105, Total_reward: 104.0
Epoch: 170, Loss: 4.140685558319092, Total_reward: 116.0
Epoch: 180, Loss: 3.1729507446289062, Total_reward: 97.0
Epoch: 190, Loss: 6.873345851898193, Total_reward: 94.0
Epoch: 200, Loss: 6.315948963165283, Total_reward: 104.0
Epoch: 210, Loss: 4.469693660736084, Total_reward: 267.0
Epoch: 220, Loss: 3.6834490299224854, Total_reward: 169.0
Epoch: 230, Loss: 1.609716773033142, Total_reward: 179.0
Epoch: 240, Loss: 4.395779132843018, Total_reward: 172.0
Epoch: 250, Loss: 2.256197929382324, Total_reward: 204.0
Epoch: 260, Loss: 6.792372226715088, Total_reward: 155.0
Epoch: 270, Loss: 6.741207599639893, Total_reward: 169.0
Epoch: 280, Loss: 9.0798921585083, Total_reward: 152.0
Epoch: 290, Loss: 6.89796781539917, Total_reward: 145.0
Average reward over 10 runs (invisible): 125.8
result : 125.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.24088767170906067, Total_reward: 106.0
Epoch: 20, Loss: 0.5345199108123779, Total_reward: 155.0
Epoch: 30, Loss: 0.864207923412323, Total_reward: 195.0
Epoch: 40, Loss: 3.1054508686065674, Total_reward: 148.0
Epoch: 50, Loss: 6.219714641571045, Total_reward: 103.0
Epoch: 60, Loss: 4.826169013977051, Total_reward: 157.0
Epoch: 70, Loss: 6.970318794250488, Total_reward: 136.0
Epoch: 80, Loss: 9.723328590393066, Total_reward: 130.0
Epoch: 90, Loss: 14.886672973632812, Total_reward: 192.0
Epoch: 100, Loss: 7.078213691711426, Total_reward: 181.0
Epoch: 110, Loss: 12.021111488342285, Total_reward: 183.0
Epoch: 120, Loss: 12.2650785446167, Total_reward: 145.0
Epoch: 130, Loss: 13.302042961120605, Total_reward: 157.0
Epoch: 140, Loss: 18.546768188476562, Total_reward: 206.0
Epoch: 150, Loss: 14.759222984313965, Total_reward: 194.0
Epoch: 160, Loss: 10.657702445983887, Total_reward: 150.0
Epoch: 170, Loss: 19.954959869384766, Total_reward: 137.0
Epoch: 180, Loss: 32.88680648803711, Total_reward: 111.0
Epoch: 190, Loss: 31.45928192138672, Total_reward: 149.0
Epoch: 200, Loss: 18.3646297454834, Total_reward: 229.0
Epoch: 210, Loss: 32.22902297973633, Total_reward: 206.0
Epoch: 220, Loss: 34.21438217163086, Total_reward: 190.0
Epoch: 230, Loss: 11.208162307739258, Total_reward: 170.0
Epoch: 240, Loss: 20.73356819152832, Total_reward: 157.0
Epoch: 250, Loss: 23.658676147460938, Total_reward: 159.0
Epoch: 260, Loss: 33.176025390625, Total_reward: 151.0
Epoch: 270, Loss: 32.410560607910156, Total_reward: 118.0
Epoch: 280, Loss: 46.879852294921875, Total_reward: 97.0
Epoch: 290, Loss: 33.9112663269043, Total_reward: 122.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.06320793181657791, Total_reward: 107.0
Epoch: 20, Loss: 0.050853580236434937, Total_reward: 92.0
Epoch: 30, Loss: 0.2278878092765808, Total_reward: 119.0
Epoch: 40, Loss: 0.4073896110057831, Total_reward: 101.0
Epoch: 50, Loss: 0.7996042966842651, Total_reward: 207.0
Epoch: 60, Loss: 0.5370811223983765, Total_reward: 194.0
Epoch: 70, Loss: 0.7755862474441528, Total_reward: 125.0
Epoch: 80, Loss: 0.9711240530014038, Total_reward: 164.0
Epoch: 90, Loss: 1.0030558109283447, Total_reward: 154.0
Epoch: 100, Loss: 1.144644856452942, Total_reward: 155.0
Epoch: 110, Loss: 2.0689845085144043, Total_reward: 230.0
Epoch: 120, Loss: 1.740930199623108, Total_reward: 185.0
Epoch: 130, Loss: 1.3776068687438965, Total_reward: 110.0
Epoch: 140, Loss: 2.243925094604492, Total_reward: 132.0
Epoch: 150, Loss: 2.702573537826538, Total_reward: 219.0
Epoch: 160, Loss: 1.3215961456298828, Total_reward: 211.0
Epoch: 170, Loss: 1.66489839553833, Total_reward: 215.0
Epoch: 180, Loss: 4.683537006378174, Total_reward: 142.0
Epoch: 190, Loss: 5.171600341796875, Total_reward: 108.0
Epoch: 200, Loss: 5.036489009857178, Total_reward: 209.0
Epoch: 210, Loss: 3.15952205657959, Total_reward: 200.0
Epoch: 220, Loss: 6.565199375152588, Total_reward: 169.0
Epoch: 230, Loss: 7.8438920974731445, Total_reward: 234.0
Epoch: 240, Loss: 3.8233768939971924, Total_reward: 149.0
Epoch: 250, Loss: 6.360640525817871, Total_reward: 143.0
Epoch: 260, Loss: 6.566427230834961, Total_reward: 149.0
Epoch: 270, Loss: 7.437119483947754, Total_reward: 133.0
Epoch: 280, Loss: 7.673656463623047, Total_reward: 135.0
Epoch: 290, Loss: 10.112360954284668, Total_reward: 137.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.08627044409513474, Total_reward: 108.0
Epoch: 20, Loss: 0.5148453712463379, Total_reward: 128.0
Epoch: 30, Loss: 0.6877330541610718, Total_reward: 127.0
Epoch: 40, Loss: 2.915877103805542, Total_reward: 110.0
Epoch: 50, Loss: 4.081839561462402, Total_reward: 132.0
Epoch: 60, Loss: 6.338559627532959, Total_reward: 146.0
Epoch: 70, Loss: 7.074501037597656, Total_reward: 164.0
Epoch: 80, Loss: 11.053006172180176, Total_reward: 162.0
Epoch: 90, Loss: 12.459083557128906, Total_reward: 163.0
Epoch: 100, Loss: 13.904451370239258, Total_reward: 170.0
Epoch: 110, Loss: 16.37299346923828, Total_reward: 142.0
Epoch: 120, Loss: 11.866327285766602, Total_reward: 154.0
Epoch: 130, Loss: 20.88324737548828, Total_reward: 147.0
Epoch: 140, Loss: 18.126249313354492, Total_reward: 139.0
Epoch: 150, Loss: 23.81502342224121, Total_reward: 117.0
Epoch: 160, Loss: 13.981130599975586, Total_reward: 179.0
Epoch: 170, Loss: 15.655377388000488, Total_reward: 143.0
Epoch: 180, Loss: 14.733875274658203, Total_reward: 121.0
Epoch: 190, Loss: 24.82030487060547, Total_reward: 131.0
Epoch: 200, Loss: 12.365139961242676, Total_reward: 167.0
Epoch: 210, Loss: 22.95685386657715, Total_reward: 134.0
Epoch: 220, Loss: 12.73788833618164, Total_reward: 132.0
Epoch: 230, Loss: 36.00277328491211, Total_reward: 126.0
Epoch: 240, Loss: 22.717308044433594, Total_reward: 109.0
Epoch: 250, Loss: 12.795369148254395, Total_reward: 131.0
Epoch: 260, Loss: 26.442100524902344, Total_reward: 139.0
Epoch: 270, Loss: 22.721445083618164, Total_reward: 138.0
Epoch: 280, Loss: 21.20792007446289, Total_reward: 112.0
Epoch: 290, Loss: 25.986825942993164, Total_reward: 128.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.08796840906143188, Total_reward: 99.0
Epoch: 20, Loss: 0.14142915606498718, Total_reward: 91.0
Epoch: 30, Loss: 0.6649460792541504, Total_reward: 119.0
Epoch: 40, Loss: 0.40992966294288635, Total_reward: 136.0
Epoch: 50, Loss: 0.5716578960418701, Total_reward: 100.0
Epoch: 60, Loss: 1.215406894683838, Total_reward: 87.0
Epoch: 70, Loss: 2.3844730854034424, Total_reward: 111.0
Epoch: 80, Loss: 1.4069666862487793, Total_reward: 87.0
Epoch: 90, Loss: 1.91304349899292, Total_reward: 104.0
Epoch: 100, Loss: 2.945702075958252, Total_reward: 86.0
Epoch: 110, Loss: 2.5474841594696045, Total_reward: 133.0
Epoch: 120, Loss: 2.6136634349823, Total_reward: 139.0
Epoch: 130, Loss: 2.528782844543457, Total_reward: 150.0
Epoch: 140, Loss: 3.492964744567871, Total_reward: 162.0
Epoch: 150, Loss: 2.8135604858398438, Total_reward: 164.0
Epoch: 160, Loss: 4.163396835327148, Total_reward: 238.0
Epoch: 170, Loss: 3.9673125743865967, Total_reward: 248.0
Epoch: 180, Loss: 4.121864318847656, Total_reward: 171.0
Epoch: 190, Loss: 4.746133804321289, Total_reward: 167.0
Epoch: 200, Loss: 2.413297414779663, Total_reward: 178.0
Epoch: 210, Loss: 6.599102020263672, Total_reward: 164.0
Epoch: 220, Loss: 2.3429782390594482, Total_reward: 185.0
Epoch: 230, Loss: 5.675323963165283, Total_reward: 134.0
Epoch: 240, Loss: 7.553313255310059, Total_reward: 159.0
Epoch: 250, Loss: 2.417860984802246, Total_reward: 190.0
Epoch: 260, Loss: 7.513168811798096, Total_reward: 159.0
Epoch: 270, Loss: 5.70468807220459, Total_reward: 196.0
Epoch: 280, Loss: 4.699684143066406, Total_reward: 141.0
Epoch: 290, Loss: 3.997690200805664, Total_reward: 181.0
Average reward over 10 runs (invisible): 40.3
result : 40.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1273379623889923, Total_reward: 135.0
Epoch: 20, Loss: 0.8556751012802124, Total_reward: 99.0
Epoch: 30, Loss: 1.9716925621032715, Total_reward: 121.0
Epoch: 40, Loss: 2.8637478351593018, Total_reward: 148.0
Epoch: 50, Loss: 4.0873494148254395, Total_reward: 119.0
Epoch: 60, Loss: 6.805509567260742, Total_reward: 97.0
Epoch: 70, Loss: 11.576159477233887, Total_reward: 121.0
Epoch: 80, Loss: 14.281885147094727, Total_reward: 133.0
Epoch: 90, Loss: 8.277392387390137, Total_reward: 96.0
Epoch: 100, Loss: 13.805511474609375, Total_reward: 108.0
Epoch: 110, Loss: 14.723943710327148, Total_reward: 198.0
Epoch: 120, Loss: 13.115714073181152, Total_reward: 124.0
Epoch: 130, Loss: 15.027624130249023, Total_reward: 127.0
Epoch: 140, Loss: 10.827335357666016, Total_reward: 196.0
Epoch: 150, Loss: 7.945009708404541, Total_reward: 150.0
Epoch: 160, Loss: 16.298826217651367, Total_reward: 178.0
Epoch: 170, Loss: 29.309236526489258, Total_reward: 126.0
Epoch: 180, Loss: 21.16290283203125, Total_reward: 135.0
Epoch: 190, Loss: 5.718477249145508, Total_reward: 155.0
Epoch: 200, Loss: 24.738126754760742, Total_reward: 130.0
Epoch: 210, Loss: 12.46910285949707, Total_reward: 166.0
Epoch: 220, Loss: 28.08684730529785, Total_reward: 178.0
Epoch: 230, Loss: 33.286338806152344, Total_reward: 155.0
Epoch: 240, Loss: 15.738661766052246, Total_reward: 136.0
Epoch: 250, Loss: 13.46993637084961, Total_reward: 193.0
Epoch: 260, Loss: 31.23208236694336, Total_reward: 130.0
Epoch: 270, Loss: 9.049541473388672, Total_reward: 154.0
Epoch: 280, Loss: 15.50020980834961, Total_reward: 135.0
Epoch: 290, Loss: 19.148433685302734, Total_reward: 111.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.019143924117088318, Total_reward: 149.0
Epoch: 20, Loss: 0.054246872663497925, Total_reward: 170.0
Epoch: 30, Loss: 0.14787256717681885, Total_reward: 132.0
Epoch: 40, Loss: 0.2262742817401886, Total_reward: 146.0
Epoch: 50, Loss: 0.5351680517196655, Total_reward: 148.0
Epoch: 60, Loss: 0.7332655191421509, Total_reward: 127.0
Epoch: 70, Loss: 1.5478599071502686, Total_reward: 105.0
Epoch: 80, Loss: 1.5652209520339966, Total_reward: 148.0
Epoch: 90, Loss: 1.9178147315979004, Total_reward: 140.0
Epoch: 100, Loss: 1.4882935285568237, Total_reward: 125.0
Epoch: 110, Loss: 3.360612154006958, Total_reward: 112.0
Epoch: 120, Loss: 2.121593475341797, Total_reward: 133.0
Epoch: 130, Loss: 4.488063812255859, Total_reward: 135.0
Epoch: 140, Loss: 2.1934518814086914, Total_reward: 103.0
Epoch: 150, Loss: 3.621609687805176, Total_reward: 159.0
Epoch: 160, Loss: 5.019475936889648, Total_reward: 101.0
Epoch: 170, Loss: 3.8435840606689453, Total_reward: 130.0
Epoch: 180, Loss: 3.824978828430176, Total_reward: 122.0
Epoch: 190, Loss: 3.689549446105957, Total_reward: 161.0
Epoch: 200, Loss: 3.0056135654449463, Total_reward: 127.0
Epoch: 210, Loss: 4.698178291320801, Total_reward: 137.0
Epoch: 220, Loss: 5.920952796936035, Total_reward: 112.0
Epoch: 230, Loss: 3.4976119995117188, Total_reward: 147.0
Epoch: 240, Loss: 5.4809184074401855, Total_reward: 238.0
Epoch: 250, Loss: 3.7182443141937256, Total_reward: 146.0
Epoch: 260, Loss: 5.654743194580078, Total_reward: 127.0
Epoch: 270, Loss: 5.580676555633545, Total_reward: 112.0
Epoch: 280, Loss: 4.8288469314575195, Total_reward: 152.0
Epoch: 290, Loss: 5.5899882316589355, Total_reward: 128.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.003565702121704817, Total_reward: 128.0
Epoch: 20, Loss: 0.365245521068573, Total_reward: 107.0
Epoch: 30, Loss: 0.32203832268714905, Total_reward: 103.0
Epoch: 40, Loss: 2.425236463546753, Total_reward: 99.0
Epoch: 50, Loss: 6.502769947052002, Total_reward: 107.0
Epoch: 60, Loss: 6.10038423538208, Total_reward: 124.0
Epoch: 70, Loss: 7.997631549835205, Total_reward: 108.0
Epoch: 80, Loss: 10.446702003479004, Total_reward: 93.0
Epoch: 90, Loss: 8.534042358398438, Total_reward: 128.0
Epoch: 100, Loss: 10.710762023925781, Total_reward: 147.0
Epoch: 110, Loss: 12.109399795532227, Total_reward: 119.0
Epoch: 120, Loss: 10.412517547607422, Total_reward: 99.0
Epoch: 130, Loss: 7.154172420501709, Total_reward: 110.0
Epoch: 140, Loss: 12.003432273864746, Total_reward: 102.0
Epoch: 150, Loss: 9.805673599243164, Total_reward: 114.0
Epoch: 160, Loss: 10.546952247619629, Total_reward: 130.0
Epoch: 170, Loss: 14.689132690429688, Total_reward: 85.0
Epoch: 180, Loss: 13.849958419799805, Total_reward: 121.0
Epoch: 190, Loss: 16.891183853149414, Total_reward: 118.0
Epoch: 200, Loss: 19.371591567993164, Total_reward: 117.0
Epoch: 210, Loss: 9.722373962402344, Total_reward: 125.0
Epoch: 220, Loss: 8.906673431396484, Total_reward: 86.0
Epoch: 230, Loss: 12.121284484863281, Total_reward: 123.0
Epoch: 240, Loss: 11.231454849243164, Total_reward: 136.0
Epoch: 250, Loss: 10.409664154052734, Total_reward: 107.0
Epoch: 260, Loss: 18.377859115600586, Total_reward: 115.0
Epoch: 270, Loss: 9.206157684326172, Total_reward: 135.0
Epoch: 280, Loss: 21.256330490112305, Total_reward: 111.0
Epoch: 290, Loss: 16.352041244506836, Total_reward: 89.0
Average reward over 10 runs (invisible): 9.3
result : 9.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.02010398358106613, Total_reward: 159.0
Epoch: 20, Loss: 0.03425878286361694, Total_reward: 129.0
Epoch: 30, Loss: 0.10710001736879349, Total_reward: 130.0
Epoch: 40, Loss: 0.32452264428138733, Total_reward: 121.0
Epoch: 50, Loss: 0.9678950905799866, Total_reward: 131.0
Epoch: 60, Loss: 0.6148781776428223, Total_reward: 132.0
Epoch: 70, Loss: 0.9831082224845886, Total_reward: 108.0
Epoch: 80, Loss: 0.8451402187347412, Total_reward: 172.0
Epoch: 90, Loss: 2.0896079540252686, Total_reward: 151.0
Epoch: 100, Loss: 2.1678247451782227, Total_reward: 114.0
Epoch: 110, Loss: 1.771765112876892, Total_reward: 127.0
Epoch: 120, Loss: 3.614955425262451, Total_reward: 114.0
Epoch: 130, Loss: 2.307812213897705, Total_reward: 154.0
Epoch: 140, Loss: 3.697758436203003, Total_reward: 121.0
Epoch: 150, Loss: 4.422025680541992, Total_reward: 126.0
Epoch: 160, Loss: 3.057643175125122, Total_reward: 126.0
Epoch: 170, Loss: 3.0760743618011475, Total_reward: 187.0
Epoch: 180, Loss: 4.7370781898498535, Total_reward: 197.0
Epoch: 190, Loss: 3.6757872104644775, Total_reward: 162.0
Epoch: 200, Loss: 3.615048885345459, Total_reward: 107.0
Epoch: 210, Loss: 1.6734694242477417, Total_reward: 202.0
Epoch: 220, Loss: 5.639190673828125, Total_reward: 165.0
Epoch: 230, Loss: 6.404516220092773, Total_reward: 132.0
Epoch: 240, Loss: 4.823437690734863, Total_reward: 166.0
Epoch: 250, Loss: 4.69140100479126, Total_reward: 183.0
Epoch: 260, Loss: 4.841712951660156, Total_reward: 144.0
Epoch: 270, Loss: 6.15239143371582, Total_reward: 146.0
Epoch: 280, Loss: 8.136775016784668, Total_reward: 213.0
Epoch: 290, Loss: 6.740566730499268, Total_reward: 173.0
Average reward over 10 runs (invisible): 30.3
result : 30.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.013414599001407623, Total_reward: 140.0
Epoch: 20, Loss: 0.8297368288040161, Total_reward: 121.0
Epoch: 30, Loss: 2.181217908859253, Total_reward: 122.0
Epoch: 40, Loss: 2.108243703842163, Total_reward: 180.0
Epoch: 50, Loss: 5.194294452667236, Total_reward: 136.0
Epoch: 60, Loss: 5.838357448577881, Total_reward: 123.0
Epoch: 70, Loss: 5.784240245819092, Total_reward: 106.0
Epoch: 80, Loss: 7.305065631866455, Total_reward: 165.0
Epoch: 90, Loss: 10.160943984985352, Total_reward: 168.0
Epoch: 100, Loss: 10.012993812561035, Total_reward: 180.0
Epoch: 110, Loss: 4.917997360229492, Total_reward: 225.0
Epoch: 120, Loss: 15.046089172363281, Total_reward: 154.0
Epoch: 130, Loss: 8.636330604553223, Total_reward: 135.0
Epoch: 140, Loss: 13.822452545166016, Total_reward: 198.0
Epoch: 150, Loss: 15.514457702636719, Total_reward: 195.0
Epoch: 160, Loss: 18.166841506958008, Total_reward: 202.0
Epoch: 170, Loss: 12.987122535705566, Total_reward: 190.0
Epoch: 180, Loss: 13.406078338623047, Total_reward: 212.0
Epoch: 190, Loss: 15.513224601745605, Total_reward: 155.0
Epoch: 200, Loss: 13.9319429397583, Total_reward: 257.0
Epoch: 210, Loss: 11.789978981018066, Total_reward: 141.0
Epoch: 220, Loss: 21.0089054107666, Total_reward: 171.0
Epoch: 230, Loss: 34.89894104003906, Total_reward: 168.0
Epoch: 240, Loss: 17.98202896118164, Total_reward: 175.0
Epoch: 250, Loss: 20.867523193359375, Total_reward: 188.0
Epoch: 260, Loss: 17.824684143066406, Total_reward: 206.0
Epoch: 270, Loss: 23.808439254760742, Total_reward: 134.0
Epoch: 280, Loss: 26.058351516723633, Total_reward: 140.0
Epoch: 290, Loss: 25.882064819335938, Total_reward: 177.0
Average reward over 10 runs (invisible): 16.3
result : 16.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.1, 'batch_size': 128, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.043103549629449844, Total_reward: 100.0
Epoch: 20, Loss: 0.08261001110076904, Total_reward: 90.0
Epoch: 30, Loss: 0.11329711973667145, Total_reward: 108.0
Epoch: 40, Loss: 0.1258198618888855, Total_reward: 119.0
Epoch: 50, Loss: 0.6325391530990601, Total_reward: 134.0
Epoch: 60, Loss: 0.6491274237632751, Total_reward: 203.0
Epoch: 70, Loss: 1.4866349697113037, Total_reward: 130.0
Epoch: 80, Loss: 0.5534363985061646, Total_reward: 122.0
Epoch: 90, Loss: 1.0711828470230103, Total_reward: 187.0
Epoch: 100, Loss: 0.7863477468490601, Total_reward: 189.0
Epoch: 110, Loss: 2.3408405780792236, Total_reward: 157.0
Epoch: 120, Loss: 2.6556828022003174, Total_reward: 163.0
Epoch: 130, Loss: 2.0380516052246094, Total_reward: 145.0
Epoch: 140, Loss: 2.632707357406616, Total_reward: 142.0
Epoch: 150, Loss: 2.0822978019714355, Total_reward: 146.0
Epoch: 160, Loss: 2.0350894927978516, Total_reward: 167.0
Epoch: 170, Loss: 3.597632646560669, Total_reward: 196.0
Epoch: 180, Loss: 3.34285569190979, Total_reward: 210.0
Epoch: 190, Loss: 2.2437121868133545, Total_reward: 192.0
Epoch: 200, Loss: 4.124354839324951, Total_reward: 177.0
Epoch: 210, Loss: 3.742474317550659, Total_reward: 140.0
Epoch: 220, Loss: 0.39675992727279663, Total_reward: 189.0
Epoch: 230, Loss: 4.8000664710998535, Total_reward: 211.0
Epoch: 240, Loss: 6.013682842254639, Total_reward: 143.0
Epoch: 250, Loss: 8.582404136657715, Total_reward: 134.0
Epoch: 260, Loss: 4.578857898712158, Total_reward: 137.0
Epoch: 270, Loss: 8.77172565460205, Total_reward: 155.0
Epoch: 280, Loss: 4.440090656280518, Total_reward: 174.0
Epoch: 290, Loss: 6.413161754608154, Total_reward: 136.0
Average reward over 10 runs (invisible): 55.8
result : 55.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.0466623455286026, Total_reward: 104.0
Epoch: 20, Loss: 0.6911602020263672, Total_reward: 103.0
Epoch: 30, Loss: 7.1213884353637695, Total_reward: 88.0
Epoch: 40, Loss: 3.516774892807007, Total_reward: 103.0
Epoch: 50, Loss: 1.7517685890197754, Total_reward: 106.0
Epoch: 60, Loss: 5.472151279449463, Total_reward: 110.0
Epoch: 70, Loss: 9.952553749084473, Total_reward: 124.0
Epoch: 80, Loss: 0.5535427331924438, Total_reward: 107.0
Epoch: 90, Loss: 4.875063896179199, Total_reward: 128.0
Epoch: 100, Loss: 17.039064407348633, Total_reward: 94.0
Epoch: 110, Loss: 10.770834922790527, Total_reward: 106.0
Epoch: 120, Loss: 28.746501922607422, Total_reward: 103.0
Epoch: 130, Loss: 17.019207000732422, Total_reward: 97.0
Epoch: 140, Loss: 14.80672836303711, Total_reward: 111.0
Epoch: 150, Loss: 20.31000328063965, Total_reward: 85.0
Epoch: 160, Loss: 11.543317794799805, Total_reward: 99.0
Epoch: 170, Loss: 13.07833194732666, Total_reward: 125.0
Epoch: 180, Loss: 20.92328453063965, Total_reward: 95.0
Epoch: 190, Loss: 23.879560470581055, Total_reward: 107.0
Epoch: 200, Loss: 10.383323669433594, Total_reward: 85.0
Epoch: 210, Loss: 20.78915023803711, Total_reward: 89.0
Epoch: 220, Loss: 13.610078811645508, Total_reward: 79.0
Epoch: 230, Loss: 8.900443077087402, Total_reward: 101.0
Epoch: 240, Loss: 16.340986251831055, Total_reward: 89.0
Epoch: 250, Loss: 14.577644348144531, Total_reward: 90.0
Epoch: 260, Loss: 18.626562118530273, Total_reward: 84.0
Epoch: 270, Loss: 16.475147247314453, Total_reward: 98.0
Epoch: 280, Loss: 19.379167556762695, Total_reward: 107.0
Epoch: 290, Loss: 13.205229759216309, Total_reward: 120.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.016710851341485977, Total_reward: 93.0
Epoch: 20, Loss: 0.024970252066850662, Total_reward: 98.0
Epoch: 30, Loss: 0.21531228721141815, Total_reward: 89.0
Epoch: 40, Loss: 0.3886682391166687, Total_reward: 89.0
Epoch: 50, Loss: 0.8218737840652466, Total_reward: 89.0
Epoch: 60, Loss: 0.9201823472976685, Total_reward: 88.0
Epoch: 70, Loss: 1.4653387069702148, Total_reward: 116.0
Epoch: 80, Loss: 0.6309466361999512, Total_reward: 138.0
Epoch: 90, Loss: 1.3587020635604858, Total_reward: 140.0
Epoch: 100, Loss: 1.1572362184524536, Total_reward: 136.0
Epoch: 110, Loss: 0.7701239585876465, Total_reward: 104.0
Epoch: 120, Loss: 1.2057127952575684, Total_reward: 107.0
Epoch: 130, Loss: 1.5743415355682373, Total_reward: 102.0
Epoch: 140, Loss: 2.268369197845459, Total_reward: 136.0
Epoch: 150, Loss: 1.9427590370178223, Total_reward: 129.0
Epoch: 160, Loss: 1.5339534282684326, Total_reward: 141.0
Epoch: 170, Loss: 2.3623831272125244, Total_reward: 135.0
Epoch: 180, Loss: 4.7088165283203125, Total_reward: 164.0
Epoch: 190, Loss: 4.844143867492676, Total_reward: 121.0
Epoch: 200, Loss: 3.3423337936401367, Total_reward: 114.0
Epoch: 210, Loss: 12.27769660949707, Total_reward: 122.0
Epoch: 220, Loss: 4.950926780700684, Total_reward: 141.0
Epoch: 230, Loss: 4.680669784545898, Total_reward: 169.0
Epoch: 240, Loss: 8.399064064025879, Total_reward: 140.0
Epoch: 250, Loss: 7.84501838684082, Total_reward: 128.0
Epoch: 260, Loss: 7.8888349533081055, Total_reward: 114.0
Epoch: 270, Loss: 8.165764808654785, Total_reward: 122.0
Epoch: 280, Loss: 8.469988822937012, Total_reward: 149.0
Epoch: 290, Loss: 4.292932987213135, Total_reward: 134.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.2881597876548767, Total_reward: 96.0
Epoch: 20, Loss: 1.0651825666427612, Total_reward: 95.0
Epoch: 30, Loss: 2.5438997745513916, Total_reward: 158.0
Epoch: 40, Loss: 2.547905683517456, Total_reward: 137.0
Epoch: 50, Loss: 1.2904750108718872, Total_reward: 156.0
Epoch: 60, Loss: 1.3734815120697021, Total_reward: 174.0
Epoch: 70, Loss: 6.987492561340332, Total_reward: 109.0
Epoch: 80, Loss: 11.491847038269043, Total_reward: 144.0
Epoch: 90, Loss: 7.700334548950195, Total_reward: 146.0
Epoch: 100, Loss: 6.059303283691406, Total_reward: 145.0
Epoch: 110, Loss: 4.738114356994629, Total_reward: 163.0
Epoch: 120, Loss: 4.459290027618408, Total_reward: 149.0
Epoch: 130, Loss: 13.468114852905273, Total_reward: 143.0
Epoch: 140, Loss: 15.888142585754395, Total_reward: 122.0
Epoch: 150, Loss: 16.653104782104492, Total_reward: 110.0
Epoch: 160, Loss: 6.957515716552734, Total_reward: 104.0
Epoch: 170, Loss: 10.690610885620117, Total_reward: 99.0
Epoch: 180, Loss: 26.375946044921875, Total_reward: 107.0
Epoch: 190, Loss: 45.21922302246094, Total_reward: 81.0
Epoch: 200, Loss: 37.685546875, Total_reward: 82.0
Epoch: 210, Loss: 34.7495002746582, Total_reward: 105.0
Epoch: 220, Loss: 47.115970611572266, Total_reward: 113.0
Epoch: 230, Loss: 27.728700637817383, Total_reward: 119.0
Epoch: 240, Loss: 14.474072456359863, Total_reward: 124.0
Epoch: 250, Loss: 11.18740463256836, Total_reward: 102.0
Epoch: 260, Loss: 13.020191192626953, Total_reward: 117.0
Epoch: 270, Loss: 16.1121826171875, Total_reward: 117.0
Epoch: 280, Loss: 29.460235595703125, Total_reward: 130.0
Epoch: 290, Loss: 10.865352630615234, Total_reward: 136.0
Average reward over 10 runs (invisible): 82.5
result : 82.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.03345700725913048, Total_reward: 113.0
Epoch: 20, Loss: 0.02977132424712181, Total_reward: 121.0
Epoch: 30, Loss: 0.32624515891075134, Total_reward: 97.0
Epoch: 40, Loss: 0.6340306401252747, Total_reward: 95.0
Epoch: 50, Loss: 0.5184137225151062, Total_reward: 99.0
Epoch: 60, Loss: 1.0447773933410645, Total_reward: 108.0
Epoch: 70, Loss: 1.4517420530319214, Total_reward: 105.0
Epoch: 80, Loss: 1.43295156955719, Total_reward: 110.0
Epoch: 90, Loss: 1.9262971878051758, Total_reward: 97.0
Epoch: 100, Loss: 2.014885425567627, Total_reward: 89.0
Epoch: 110, Loss: 2.4313347339630127, Total_reward: 90.0
Epoch: 120, Loss: 1.310907244682312, Total_reward: 90.0
Epoch: 130, Loss: 2.4538843631744385, Total_reward: 94.0
Epoch: 140, Loss: 3.4878220558166504, Total_reward: 100.0
Epoch: 150, Loss: 2.3979201316833496, Total_reward: 106.0
Epoch: 160, Loss: 1.7248908281326294, Total_reward: 95.0
Epoch: 170, Loss: 4.070056915283203, Total_reward: 88.0
Epoch: 180, Loss: 8.653207778930664, Total_reward: 122.0
Epoch: 190, Loss: 5.3176116943359375, Total_reward: 93.0
Epoch: 200, Loss: 1.7403537034988403, Total_reward: 94.0
Epoch: 210, Loss: 3.809314727783203, Total_reward: 101.0
Epoch: 220, Loss: 5.369577407836914, Total_reward: 98.0
Epoch: 230, Loss: 6.188473224639893, Total_reward: 103.0
Epoch: 240, Loss: 9.624497413635254, Total_reward: 111.0
Epoch: 250, Loss: 4.9378862380981445, Total_reward: 94.0
Epoch: 260, Loss: 4.684581756591797, Total_reward: 104.0
Epoch: 270, Loss: 3.586416721343994, Total_reward: 88.0
Epoch: 280, Loss: 5.601658821105957, Total_reward: 88.0
Epoch: 290, Loss: 10.706704139709473, Total_reward: 98.0
Average reward over 10 runs (invisible): 9.3
result : 9.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.14693842828273773, Total_reward: 94.0
Epoch: 20, Loss: 1.0381107330322266, Total_reward: 119.0
Epoch: 30, Loss: 2.037868022918701, Total_reward: 101.0
Epoch: 40, Loss: 6.379307270050049, Total_reward: 112.0
Epoch: 50, Loss: 3.6730549335479736, Total_reward: 153.0
Epoch: 60, Loss: 5.576332092285156, Total_reward: 224.0
Epoch: 70, Loss: 3.7633697986602783, Total_reward: 174.0
Epoch: 80, Loss: 3.9356563091278076, Total_reward: 135.0
Epoch: 90, Loss: 4.030529975891113, Total_reward: 125.0
Epoch: 100, Loss: 4.4498677253723145, Total_reward: 180.0
Epoch: 110, Loss: 18.3330020904541, Total_reward: 190.0
Epoch: 120, Loss: 10.7481689453125, Total_reward: 145.0
Epoch: 130, Loss: 17.29840660095215, Total_reward: 161.0
Epoch: 140, Loss: 14.184853553771973, Total_reward: 120.0
Epoch: 150, Loss: 14.168639183044434, Total_reward: 167.0
Epoch: 160, Loss: 12.5781831741333, Total_reward: 152.0
Epoch: 170, Loss: 12.580862998962402, Total_reward: 136.0
Epoch: 180, Loss: 12.517068862915039, Total_reward: 120.0
Epoch: 190, Loss: 29.234481811523438, Total_reward: 103.0
Epoch: 200, Loss: 17.422977447509766, Total_reward: 97.0
Epoch: 210, Loss: 26.232666015625, Total_reward: 113.0
Epoch: 220, Loss: 27.885684967041016, Total_reward: 192.0
Epoch: 230, Loss: 18.73729705810547, Total_reward: 228.0
Epoch: 240, Loss: 30.151683807373047, Total_reward: 179.0
Epoch: 250, Loss: 6.599900245666504, Total_reward: 109.0
Epoch: 260, Loss: 28.066242218017578, Total_reward: 139.0
Epoch: 270, Loss: 28.480743408203125, Total_reward: 188.0
Epoch: 280, Loss: 21.88514518737793, Total_reward: 127.0
Epoch: 290, Loss: 17.078277587890625, Total_reward: 185.0
Average reward over 10 runs (invisible): 15.4
result : 15.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.09574752300977707, Total_reward: 98.0
Epoch: 20, Loss: 0.19723278284072876, Total_reward: 87.0
Epoch: 30, Loss: 0.8514417409896851, Total_reward: 119.0
Epoch: 40, Loss: 0.7267583608627319, Total_reward: 142.0
Epoch: 50, Loss: 0.9194292426109314, Total_reward: 174.0
Epoch: 60, Loss: 1.384796142578125, Total_reward: 153.0
Epoch: 70, Loss: 1.375474214553833, Total_reward: 239.0
Epoch: 80, Loss: 1.2319011688232422, Total_reward: 135.0
Epoch: 90, Loss: 1.307436466217041, Total_reward: 166.0
Epoch: 100, Loss: 1.355375051498413, Total_reward: 138.0
Epoch: 110, Loss: 0.7592795491218567, Total_reward: 164.0
Epoch: 120, Loss: 1.2380019426345825, Total_reward: 251.0
Epoch: 130, Loss: 5.366661548614502, Total_reward: 164.0
Epoch: 140, Loss: 3.822878360748291, Total_reward: 168.0
Epoch: 150, Loss: 4.419981002807617, Total_reward: 142.0
Epoch: 160, Loss: 3.3545517921447754, Total_reward: 183.0
Epoch: 170, Loss: 4.277041435241699, Total_reward: 135.0
Epoch: 180, Loss: 7.310976028442383, Total_reward: 158.0
Epoch: 190, Loss: 2.0506811141967773, Total_reward: 106.0
Epoch: 200, Loss: 6.1970014572143555, Total_reward: 119.0
Epoch: 210, Loss: 4.782022953033447, Total_reward: 154.0
Epoch: 220, Loss: 6.134062767028809, Total_reward: 182.0
Epoch: 230, Loss: 2.3933358192443848, Total_reward: 173.0
Epoch: 240, Loss: 5.267340183258057, Total_reward: 177.0
Epoch: 250, Loss: 7.279115200042725, Total_reward: 135.0
Epoch: 260, Loss: 7.246262073516846, Total_reward: 153.0
Epoch: 270, Loss: 9.947131156921387, Total_reward: 101.0
Epoch: 280, Loss: 8.805007934570312, Total_reward: 157.0
Epoch: 290, Loss: 5.973354339599609, Total_reward: 89.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.5306077003479004, Total_reward: 100.0
Epoch: 20, Loss: 1.0162410736083984, Total_reward: 94.0
Epoch: 30, Loss: 3.103769540786743, Total_reward: 98.0
Epoch: 40, Loss: 4.902504920959473, Total_reward: 132.0
Epoch: 50, Loss: 1.036434531211853, Total_reward: 141.0
Epoch: 60, Loss: 4.8707427978515625, Total_reward: 136.0
Epoch: 70, Loss: 7.665122032165527, Total_reward: 149.0
Epoch: 80, Loss: 4.457724571228027, Total_reward: 167.0
Epoch: 90, Loss: 4.140839576721191, Total_reward: 157.0
Epoch: 100, Loss: 8.55889892578125, Total_reward: 143.0
Epoch: 110, Loss: 7.509708404541016, Total_reward: 162.0
Epoch: 120, Loss: 18.638673782348633, Total_reward: 112.0
Epoch: 130, Loss: 11.403336524963379, Total_reward: 192.0
Epoch: 140, Loss: 9.777222633361816, Total_reward: 141.0
Epoch: 150, Loss: 24.18035125732422, Total_reward: 125.0
Epoch: 160, Loss: 15.967864990234375, Total_reward: 94.0
Epoch: 170, Loss: 20.526487350463867, Total_reward: 105.0
Epoch: 180, Loss: 12.728056907653809, Total_reward: 175.0
Epoch: 190, Loss: 11.702473640441895, Total_reward: 112.0
Epoch: 200, Loss: 20.261608123779297, Total_reward: 106.0
Epoch: 210, Loss: 10.908156394958496, Total_reward: 173.0
Epoch: 220, Loss: 15.214662551879883, Total_reward: 159.0
Epoch: 230, Loss: 14.024497985839844, Total_reward: 130.0
Epoch: 240, Loss: 25.183866500854492, Total_reward: 114.0
Epoch: 250, Loss: 15.366776466369629, Total_reward: 133.0
Epoch: 260, Loss: 25.898120880126953, Total_reward: 117.0
Epoch: 270, Loss: 40.49833297729492, Total_reward: 118.0
Epoch: 280, Loss: 11.440916061401367, Total_reward: 100.0
Epoch: 290, Loss: 13.208850860595703, Total_reward: 107.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.012071467004716396, Total_reward: 130.0
Epoch: 20, Loss: 0.05155858397483826, Total_reward: 105.0
Epoch: 30, Loss: 0.32453399896621704, Total_reward: 87.0
Epoch: 40, Loss: 0.5652245283126831, Total_reward: 82.0
Epoch: 50, Loss: 1.0560075044631958, Total_reward: 109.0
Epoch: 60, Loss: 1.0007649660110474, Total_reward: 110.0
Epoch: 70, Loss: 1.2634084224700928, Total_reward: 174.0
Epoch: 80, Loss: 1.2855864763259888, Total_reward: 156.0
Epoch: 90, Loss: 1.0590112209320068, Total_reward: 136.0
Epoch: 100, Loss: 1.4628852605819702, Total_reward: 107.0
Epoch: 110, Loss: 0.9938284158706665, Total_reward: 145.0
Epoch: 120, Loss: 3.587938070297241, Total_reward: 124.0
Epoch: 130, Loss: 5.275049686431885, Total_reward: 116.0
Epoch: 140, Loss: 2.841982364654541, Total_reward: 199.0
Epoch: 150, Loss: 4.624355316162109, Total_reward: 152.0
Epoch: 160, Loss: 1.597233533859253, Total_reward: 197.0
Epoch: 170, Loss: 1.9880355596542358, Total_reward: 153.0
Epoch: 180, Loss: 5.4728899002075195, Total_reward: 139.0
Epoch: 190, Loss: 2.2432477474212646, Total_reward: 145.0
Epoch: 200, Loss: 2.2934985160827637, Total_reward: 145.0
Epoch: 210, Loss: 8.459077835083008, Total_reward: 99.0
Epoch: 220, Loss: 6.410993576049805, Total_reward: 99.0
Epoch: 230, Loss: 7.469065189361572, Total_reward: 166.0
Epoch: 240, Loss: 3.983847141265869, Total_reward: 234.0
Epoch: 250, Loss: 6.169069766998291, Total_reward: 169.0
Epoch: 260, Loss: 3.2683775424957275, Total_reward: 207.0
Epoch: 270, Loss: 3.612332820892334, Total_reward: 287.0
Epoch: 280, Loss: 9.265268325805664, Total_reward: 189.0
Epoch: 290, Loss: 1.9329009056091309, Total_reward: 175.0
Average reward over 10 runs (invisible): 9.1
result : 9.1
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.018949931487441063, Total_reward: 119.0
Epoch: 20, Loss: 0.31047746539115906, Total_reward: 99.0
Epoch: 30, Loss: 1.649943470954895, Total_reward: 82.0
Epoch: 40, Loss: 1.8703134059906006, Total_reward: 86.0
Epoch: 50, Loss: 3.7862184047698975, Total_reward: 108.0
Epoch: 60, Loss: 9.980883598327637, Total_reward: 125.0
Epoch: 70, Loss: 5.182943344116211, Total_reward: 111.0
Epoch: 80, Loss: 18.340803146362305, Total_reward: 112.0
Epoch: 90, Loss: 12.63628101348877, Total_reward: 132.0
Epoch: 100, Loss: 14.238933563232422, Total_reward: 130.0
Epoch: 110, Loss: 17.370508193969727, Total_reward: 121.0
Epoch: 120, Loss: 7.656351089477539, Total_reward: 111.0
Epoch: 130, Loss: 4.208423137664795, Total_reward: 126.0
Epoch: 140, Loss: 21.469863891601562, Total_reward: 136.0
Epoch: 150, Loss: 23.14917755126953, Total_reward: 160.0
Epoch: 160, Loss: 5.066042423248291, Total_reward: 157.0
Epoch: 170, Loss: 12.428922653198242, Total_reward: 138.0
Epoch: 180, Loss: 18.07583236694336, Total_reward: 145.0
Epoch: 190, Loss: 12.15231990814209, Total_reward: 127.0
Epoch: 200, Loss: 29.983963012695312, Total_reward: 132.0
Epoch: 210, Loss: 36.137115478515625, Total_reward: 131.0
Epoch: 220, Loss: 36.00304412841797, Total_reward: 152.0
Epoch: 230, Loss: 36.88888931274414, Total_reward: 132.0
Epoch: 240, Loss: 20.00944709777832, Total_reward: 169.0
Epoch: 250, Loss: 15.912666320800781, Total_reward: 147.0
Epoch: 260, Loss: 9.441292762756348, Total_reward: 148.0
Epoch: 270, Loss: 34.3531379699707, Total_reward: 138.0
Epoch: 280, Loss: 13.467894554138184, Total_reward: 141.0
Epoch: 290, Loss: 8.937501907348633, Total_reward: 125.0
Average reward over 10 runs (invisible): 9.7
result : 9.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.007714943028986454, Total_reward: 105.0
Epoch: 20, Loss: 0.07781778275966644, Total_reward: 83.0
Epoch: 30, Loss: 0.11122675240039825, Total_reward: 144.0
Epoch: 40, Loss: 0.11225935071706772, Total_reward: 126.0
Epoch: 50, Loss: 0.6225724220275879, Total_reward: 97.0
Epoch: 60, Loss: 0.5412964820861816, Total_reward: 84.0
Epoch: 70, Loss: 0.4229031801223755, Total_reward: 105.0
Epoch: 80, Loss: 0.8724244832992554, Total_reward: 120.0
Epoch: 90, Loss: 0.4634641408920288, Total_reward: 116.0
Epoch: 100, Loss: 1.6618365049362183, Total_reward: 156.0
Epoch: 110, Loss: 1.1516190767288208, Total_reward: 127.0
Epoch: 120, Loss: 3.099996328353882, Total_reward: 161.0
Epoch: 130, Loss: 0.3642232418060303, Total_reward: 145.0
Epoch: 140, Loss: 2.9528369903564453, Total_reward: 147.0
Epoch: 150, Loss: 2.7541732788085938, Total_reward: 148.0
Epoch: 160, Loss: 2.4333689212799072, Total_reward: 136.0
Epoch: 170, Loss: 2.235516309738159, Total_reward: 136.0
Epoch: 180, Loss: 2.029862403869629, Total_reward: 141.0
Epoch: 190, Loss: 5.563028335571289, Total_reward: 123.0
Epoch: 200, Loss: 4.634629249572754, Total_reward: 139.0
Epoch: 210, Loss: 5.645332336425781, Total_reward: 122.0
Epoch: 220, Loss: 7.223528861999512, Total_reward: 116.0
Epoch: 230, Loss: 3.258819580078125, Total_reward: 110.0
Epoch: 240, Loss: 3.4072794914245605, Total_reward: 110.0
Epoch: 250, Loss: 6.55928897857666, Total_reward: 114.0
Epoch: 260, Loss: 5.030082702636719, Total_reward: 96.0
Epoch: 270, Loss: 5.669149875640869, Total_reward: 124.0
Epoch: 280, Loss: 10.690646171569824, Total_reward: 117.0
Epoch: 290, Loss: 15.316591262817383, Total_reward: 92.0
Average reward over 10 runs (invisible): 25.6
result : 25.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.0071379998698830605, Total_reward: 114.0
Epoch: 20, Loss: 0.39649975299835205, Total_reward: 112.0
Epoch: 30, Loss: 1.5278887748718262, Total_reward: 146.0
Epoch: 40, Loss: 2.2759716510772705, Total_reward: 85.0
Epoch: 50, Loss: 3.06196665763855, Total_reward: 96.0
Epoch: 60, Loss: 2.681201696395874, Total_reward: 124.0
Epoch: 70, Loss: 6.075204849243164, Total_reward: 135.0
Epoch: 80, Loss: 4.656865119934082, Total_reward: 114.0
Epoch: 90, Loss: 25.698083877563477, Total_reward: 113.0
Epoch: 100, Loss: 5.833712577819824, Total_reward: 86.0
Epoch: 110, Loss: 15.360359191894531, Total_reward: 105.0
Epoch: 120, Loss: 17.509441375732422, Total_reward: 99.0
Epoch: 130, Loss: 15.854759216308594, Total_reward: 109.0
Epoch: 140, Loss: 21.32650375366211, Total_reward: 96.0
Epoch: 150, Loss: 16.759763717651367, Total_reward: 116.0
Epoch: 160, Loss: 9.625835418701172, Total_reward: 111.0
Epoch: 170, Loss: 18.844011306762695, Total_reward: 101.0
Epoch: 180, Loss: 16.809799194335938, Total_reward: 112.0
Epoch: 190, Loss: 7.216339588165283, Total_reward: 84.0
Epoch: 200, Loss: 17.47894859313965, Total_reward: 102.0
Epoch: 210, Loss: 13.539206504821777, Total_reward: 100.0
Epoch: 220, Loss: 20.971574783325195, Total_reward: 82.0
Epoch: 230, Loss: 27.448068618774414, Total_reward: 129.0
Epoch: 240, Loss: 24.19649887084961, Total_reward: 129.0
Epoch: 250, Loss: 32.780921936035156, Total_reward: 148.0
Epoch: 260, Loss: 25.824779510498047, Total_reward: 117.0
Epoch: 270, Loss: 31.22462272644043, Total_reward: 101.0
Epoch: 280, Loss: 26.727310180664062, Total_reward: 101.0
Epoch: 290, Loss: 31.433956146240234, Total_reward: 98.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.04592633619904518, Total_reward: 112.0
Epoch: 20, Loss: 0.12350768595933914, Total_reward: 90.0
Epoch: 30, Loss: 0.23124274611473083, Total_reward: 100.0
Epoch: 40, Loss: 0.013477003201842308, Total_reward: 126.0
Epoch: 50, Loss: 0.6535390615463257, Total_reward: 103.0
Epoch: 60, Loss: 0.6630170345306396, Total_reward: 133.0
Epoch: 70, Loss: 0.6387531757354736, Total_reward: 103.0
Epoch: 80, Loss: 0.7919731736183167, Total_reward: 102.0
Epoch: 90, Loss: 1.4916585683822632, Total_reward: 91.0
Epoch: 100, Loss: 1.9933022260665894, Total_reward: 90.0
Epoch: 110, Loss: 1.6887502670288086, Total_reward: 105.0
Epoch: 120, Loss: 3.9371957778930664, Total_reward: 124.0
Epoch: 130, Loss: 0.603153645992279, Total_reward: 97.0
Epoch: 140, Loss: 5.203000068664551, Total_reward: 132.0
Epoch: 150, Loss: 2.8784782886505127, Total_reward: 134.0
Epoch: 160, Loss: 2.479573965072632, Total_reward: 110.0
Epoch: 170, Loss: 4.555469036102295, Total_reward: 136.0
Epoch: 180, Loss: 1.9492424726486206, Total_reward: 112.0
Epoch: 190, Loss: 4.330577850341797, Total_reward: 137.0
Epoch: 200, Loss: 2.601163864135742, Total_reward: 112.0
Epoch: 210, Loss: 2.105888605117798, Total_reward: 86.0
Epoch: 220, Loss: 2.263662576675415, Total_reward: 113.0
Epoch: 230, Loss: 6.240954875946045, Total_reward: 97.0
Epoch: 240, Loss: 4.816211223602295, Total_reward: 106.0
Epoch: 250, Loss: 7.11004638671875, Total_reward: 128.0
Epoch: 260, Loss: 5.026029109954834, Total_reward: 113.0
Epoch: 270, Loss: 6.724800109863281, Total_reward: 126.0
Epoch: 280, Loss: 6.678168773651123, Total_reward: 102.0
Epoch: 290, Loss: 5.177935600280762, Total_reward: 121.0
Average reward over 10 runs (invisible): 9.1
result : 9.1
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.11915090680122375, Total_reward: 103.0
Epoch: 20, Loss: 0.36243510246276855, Total_reward: 86.0
Epoch: 30, Loss: 2.5085201263427734, Total_reward: 84.0
Epoch: 40, Loss: 3.4326963424682617, Total_reward: 93.0
Epoch: 50, Loss: 6.170257091522217, Total_reward: 115.0
Epoch: 60, Loss: 9.190847396850586, Total_reward: 139.0
Epoch: 70, Loss: 8.377218246459961, Total_reward: 144.0
Epoch: 80, Loss: 5.137275218963623, Total_reward: 167.0
Epoch: 90, Loss: 18.576316833496094, Total_reward: 220.0
Epoch: 100, Loss: 4.119178771972656, Total_reward: 245.0
Epoch: 110, Loss: 4.2916669845581055, Total_reward: 164.0
Epoch: 120, Loss: 27.646024703979492, Total_reward: 169.0
Epoch: 130, Loss: 13.618103981018066, Total_reward: 185.0
Epoch: 140, Loss: 19.512786865234375, Total_reward: 154.0
Epoch: 150, Loss: 12.673991203308105, Total_reward: 119.0
Epoch: 160, Loss: 13.69958209991455, Total_reward: 112.0
Epoch: 170, Loss: 25.02617645263672, Total_reward: 182.0
Epoch: 180, Loss: 15.787032127380371, Total_reward: 161.0
Epoch: 190, Loss: 18.290454864501953, Total_reward: 169.0
Epoch: 200, Loss: 17.868242263793945, Total_reward: 143.0
Epoch: 210, Loss: 16.229698181152344, Total_reward: 207.0
Epoch: 220, Loss: 17.460620880126953, Total_reward: 128.0
Epoch: 230, Loss: 16.87811279296875, Total_reward: 181.0
Epoch: 240, Loss: 11.597797393798828, Total_reward: 166.0
Epoch: 250, Loss: 24.893230438232422, Total_reward: 119.0
Epoch: 260, Loss: 25.535249710083008, Total_reward: 111.0
Epoch: 270, Loss: 28.916446685791016, Total_reward: 104.0
Epoch: 280, Loss: 23.899078369140625, Total_reward: 124.0
Epoch: 290, Loss: 19.264019012451172, Total_reward: 110.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.009762362577021122, Total_reward: 124.0
Epoch: 20, Loss: 0.11864567548036575, Total_reward: 90.0
Epoch: 30, Loss: 0.5689576864242554, Total_reward: 112.0
Epoch: 40, Loss: 0.46300143003463745, Total_reward: 90.0
Epoch: 50, Loss: 0.26015785336494446, Total_reward: 126.0
Epoch: 60, Loss: 0.9310842752456665, Total_reward: 93.0
Epoch: 70, Loss: 1.9918216466903687, Total_reward: 102.0
Epoch: 80, Loss: 1.6120349168777466, Total_reward: 104.0
Epoch: 90, Loss: 1.6565382480621338, Total_reward: 148.0
Epoch: 100, Loss: 1.7054846286773682, Total_reward: 115.0
Epoch: 110, Loss: 2.677795648574829, Total_reward: 192.0
Epoch: 120, Loss: 3.3150508403778076, Total_reward: 211.0
Epoch: 130, Loss: 2.9282712936401367, Total_reward: 114.0
Epoch: 140, Loss: 3.6799063682556152, Total_reward: 149.0
Epoch: 150, Loss: 3.5925657749176025, Total_reward: 162.0
Epoch: 160, Loss: 4.279272079467773, Total_reward: 95.0
Epoch: 170, Loss: 2.5829029083251953, Total_reward: 100.0
Epoch: 180, Loss: 2.7993147373199463, Total_reward: 119.0
Epoch: 190, Loss: 4.907169342041016, Total_reward: 97.0
Epoch: 200, Loss: 7.821091651916504, Total_reward: 113.0
Epoch: 210, Loss: 7.269420623779297, Total_reward: 145.0
Epoch: 220, Loss: 4.631052017211914, Total_reward: 124.0
Epoch: 230, Loss: 7.77956485748291, Total_reward: 109.0
Epoch: 240, Loss: 7.620573043823242, Total_reward: 123.0
Epoch: 250, Loss: 5.955843448638916, Total_reward: 125.0
Epoch: 260, Loss: 5.8865180015563965, Total_reward: 116.0
Epoch: 270, Loss: 8.847759246826172, Total_reward: 107.0
Epoch: 280, Loss: 11.403955459594727, Total_reward: 98.0
Epoch: 290, Loss: 9.645373344421387, Total_reward: 116.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.011401847004890442, Total_reward: 99.0
Epoch: 20, Loss: 0.98677659034729, Total_reward: 101.0
Epoch: 30, Loss: 0.6667917966842651, Total_reward: 125.0
Epoch: 40, Loss: 0.1765376180410385, Total_reward: 121.0
Epoch: 50, Loss: 3.1765048503875732, Total_reward: 113.0
Epoch: 60, Loss: 12.062264442443848, Total_reward: 133.0
Epoch: 70, Loss: 11.18277645111084, Total_reward: 150.0
Epoch: 80, Loss: 8.675381660461426, Total_reward: 101.0
Epoch: 90, Loss: 9.031028747558594, Total_reward: 113.0
Epoch: 100, Loss: 14.339503288269043, Total_reward: 109.0
Epoch: 110, Loss: 11.398469924926758, Total_reward: 176.0
Epoch: 120, Loss: 19.72612762451172, Total_reward: 107.0
Epoch: 130, Loss: 15.051192283630371, Total_reward: 105.0
Epoch: 140, Loss: 17.23719024658203, Total_reward: 117.0
Epoch: 150, Loss: 11.871428489685059, Total_reward: 119.0
Epoch: 160, Loss: 22.754404067993164, Total_reward: 149.0
Epoch: 170, Loss: 19.78274154663086, Total_reward: 138.0
Epoch: 180, Loss: 15.075902938842773, Total_reward: 129.0
Epoch: 190, Loss: 30.70880126953125, Total_reward: 97.0
Epoch: 200, Loss: 22.529674530029297, Total_reward: 114.0
Epoch: 210, Loss: 33.22688293457031, Total_reward: 101.0
Epoch: 220, Loss: 8.283275604248047, Total_reward: 137.0
Epoch: 230, Loss: 7.972842693328857, Total_reward: 113.0
Epoch: 240, Loss: 9.168107986450195, Total_reward: 101.0
Epoch: 250, Loss: 22.743242263793945, Total_reward: 88.0
Epoch: 260, Loss: 22.98968505859375, Total_reward: 97.0
Epoch: 270, Loss: 24.526844024658203, Total_reward: 102.0
Epoch: 280, Loss: 9.861440658569336, Total_reward: 109.0
Epoch: 290, Loss: 11.949708938598633, Total_reward: 109.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 20, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.0012495963601395488, Total_reward: 98.0
Epoch: 20, Loss: 0.0957658439874649, Total_reward: 82.0
Epoch: 30, Loss: 0.36880531907081604, Total_reward: 107.0
Epoch: 40, Loss: 0.36419084668159485, Total_reward: 112.0
Epoch: 50, Loss: 0.40760573744773865, Total_reward: 107.0
Epoch: 60, Loss: 0.6214751601219177, Total_reward: 146.0
Epoch: 70, Loss: 1.9828866720199585, Total_reward: 104.0
Epoch: 80, Loss: 1.1836698055267334, Total_reward: 103.0
Epoch: 90, Loss: 1.4257336854934692, Total_reward: 139.0
Epoch: 100, Loss: 1.585817813873291, Total_reward: 177.0
Epoch: 110, Loss: 2.3373868465423584, Total_reward: 138.0
Epoch: 120, Loss: 1.1845535039901733, Total_reward: 202.0
Epoch: 130, Loss: 1.970235824584961, Total_reward: 207.0
Epoch: 140, Loss: 1.4151811599731445, Total_reward: 176.0
Epoch: 150, Loss: 2.3012332916259766, Total_reward: 149.0
Epoch: 160, Loss: 2.382918357849121, Total_reward: 127.0
Epoch: 170, Loss: 5.716768264770508, Total_reward: 131.0
Epoch: 180, Loss: 1.3568394184112549, Total_reward: 141.0
Epoch: 190, Loss: 2.9778411388397217, Total_reward: 155.0
Epoch: 200, Loss: 4.267113208770752, Total_reward: 109.0
Epoch: 210, Loss: 1.216392159461975, Total_reward: 123.0
Epoch: 220, Loss: 4.83349609375, Total_reward: 115.0
Epoch: 230, Loss: 7.883454322814941, Total_reward: 92.0
Epoch: 240, Loss: 7.795549392700195, Total_reward: 96.0
Epoch: 250, Loss: 6.972442626953125, Total_reward: 205.0
Epoch: 260, Loss: 4.411044120788574, Total_reward: 185.0
Epoch: 270, Loss: 6.573342800140381, Total_reward: 189.0
Epoch: 280, Loss: 7.737298011779785, Total_reward: 123.0
Epoch: 290, Loss: 13.9788818359375, Total_reward: 161.0
Average reward over 10 runs (invisible): 9.3
result : 9.3
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.4135650098323822, Total_reward: 96.0
Epoch: 20, Loss: 0.8438353538513184, Total_reward: 97.0
Epoch: 30, Loss: 2.0900092124938965, Total_reward: 87.0
Epoch: 40, Loss: 2.686379909515381, Total_reward: 83.0
Epoch: 50, Loss: 5.318663120269775, Total_reward: 118.0
Epoch: 60, Loss: 5.531141757965088, Total_reward: 134.0
Epoch: 70, Loss: 5.0021562576293945, Total_reward: 108.0
Epoch: 80, Loss: 2.737539052963257, Total_reward: 105.0
Epoch: 90, Loss: 5.812145709991455, Total_reward: 96.0
Epoch: 100, Loss: 4.636507034301758, Total_reward: 83.0
Epoch: 110, Loss: 6.637838840484619, Total_reward: 125.0
Epoch: 120, Loss: 12.723404884338379, Total_reward: 138.0
Epoch: 130, Loss: 15.334646224975586, Total_reward: 106.0
Epoch: 140, Loss: 7.877384185791016, Total_reward: 127.0
Epoch: 150, Loss: 13.571572303771973, Total_reward: 140.0
Epoch: 160, Loss: 11.045056343078613, Total_reward: 124.0
Epoch: 170, Loss: 9.475250244140625, Total_reward: 123.0
Epoch: 180, Loss: 18.14796257019043, Total_reward: 112.0
Epoch: 190, Loss: 9.216141700744629, Total_reward: 146.0
Epoch: 200, Loss: 7.895534038543701, Total_reward: 211.0
Epoch: 210, Loss: 8.837179183959961, Total_reward: 166.0
Epoch: 220, Loss: 20.662240982055664, Total_reward: 178.0
Epoch: 230, Loss: 13.152582168579102, Total_reward: 129.0
Epoch: 240, Loss: 18.031721115112305, Total_reward: 127.0
Epoch: 250, Loss: 18.002485275268555, Total_reward: 147.0
Epoch: 260, Loss: 13.07101821899414, Total_reward: 116.0
Epoch: 270, Loss: 15.174898147583008, Total_reward: 198.0
Epoch: 280, Loss: 21.475048065185547, Total_reward: 199.0
Epoch: 290, Loss: 10.481684684753418, Total_reward: 225.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.0034297173842787743, Total_reward: 116.0
Epoch: 20, Loss: 0.36184290051460266, Total_reward: 119.0
Epoch: 30, Loss: 0.4971737265586853, Total_reward: 127.0
Epoch: 40, Loss: 0.7292473316192627, Total_reward: 89.0
Epoch: 50, Loss: 1.6319613456726074, Total_reward: 132.0
Epoch: 60, Loss: 1.4562643766403198, Total_reward: 102.0
Epoch: 70, Loss: 1.4593145847320557, Total_reward: 147.0
Epoch: 80, Loss: 0.6295451521873474, Total_reward: 110.0
Epoch: 90, Loss: 0.989559531211853, Total_reward: 192.0
Epoch: 100, Loss: 3.128763437271118, Total_reward: 110.0
Epoch: 110, Loss: 1.5528266429901123, Total_reward: 117.0
Epoch: 120, Loss: 3.2067253589630127, Total_reward: 141.0
Epoch: 130, Loss: 3.289462089538574, Total_reward: 161.0
Epoch: 140, Loss: 2.6873555183410645, Total_reward: 156.0
Epoch: 150, Loss: 3.5122129917144775, Total_reward: 182.0
Epoch: 160, Loss: 2.7574079036712646, Total_reward: 145.0
Epoch: 170, Loss: 1.9425426721572876, Total_reward: 130.0
Epoch: 180, Loss: 6.056989669799805, Total_reward: 165.0
Epoch: 190, Loss: 3.852173089981079, Total_reward: 153.0
Epoch: 200, Loss: 7.496544361114502, Total_reward: 184.0
Epoch: 210, Loss: 6.431377410888672, Total_reward: 193.0
Epoch: 220, Loss: 4.664710998535156, Total_reward: 195.0
Epoch: 230, Loss: 7.537989139556885, Total_reward: 103.0
Epoch: 240, Loss: 7.224591255187988, Total_reward: 171.0
Epoch: 250, Loss: 5.294630527496338, Total_reward: 109.0
Epoch: 260, Loss: 9.496817588806152, Total_reward: 98.0
Epoch: 270, Loss: 3.7121195793151855, Total_reward: 130.0
Epoch: 280, Loss: 6.003213405609131, Total_reward: 211.0
Epoch: 290, Loss: 11.467151641845703, Total_reward: 172.0
Average reward over 10 runs (invisible): 9.7
result : 9.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.5528836250305176, Total_reward: 94.0
Epoch: 20, Loss: 0.1537332385778427, Total_reward: 86.0
Epoch: 30, Loss: 2.294224977493286, Total_reward: 86.0
Epoch: 40, Loss: 1.3035786151885986, Total_reward: 159.0
Epoch: 50, Loss: 3.430474042892456, Total_reward: 114.0
Epoch: 60, Loss: 3.931907892227173, Total_reward: 139.0
Epoch: 70, Loss: 2.209705352783203, Total_reward: 100.0
Epoch: 80, Loss: 5.371578216552734, Total_reward: 111.0
Epoch: 90, Loss: 13.16089153289795, Total_reward: 136.0
Epoch: 100, Loss: 6.9697771072387695, Total_reward: 114.0
Epoch: 110, Loss: 10.715498924255371, Total_reward: 157.0
Epoch: 120, Loss: 10.27785587310791, Total_reward: 151.0
Epoch: 130, Loss: 5.455642223358154, Total_reward: 122.0
Epoch: 140, Loss: 3.0258374214172363, Total_reward: 165.0
Epoch: 150, Loss: 5.6591973304748535, Total_reward: 107.0
Epoch: 160, Loss: 18.96291160583496, Total_reward: 145.0
Epoch: 170, Loss: 13.652399063110352, Total_reward: 151.0
Epoch: 180, Loss: 15.60401439666748, Total_reward: 148.0
Epoch: 190, Loss: 21.58594512939453, Total_reward: 114.0
Epoch: 200, Loss: 23.200586318969727, Total_reward: 134.0
Epoch: 210, Loss: 20.753883361816406, Total_reward: 131.0
Epoch: 220, Loss: 17.028440475463867, Total_reward: 88.0
Epoch: 230, Loss: 6.548985958099365, Total_reward: 89.0
Epoch: 240, Loss: 28.53878402709961, Total_reward: 89.0
Epoch: 250, Loss: 23.98373794555664, Total_reward: 80.0
Epoch: 260, Loss: 24.64209747314453, Total_reward: 89.0
Epoch: 270, Loss: 19.6279239654541, Total_reward: 83.0
Epoch: 280, Loss: 15.410102844238281, Total_reward: 96.0
Epoch: 290, Loss: 15.956294059753418, Total_reward: 83.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.06087625026702881, Total_reward: 101.0
Epoch: 20, Loss: 0.15554803609848022, Total_reward: 118.0
Epoch: 30, Loss: 0.3980022668838501, Total_reward: 90.0
Epoch: 40, Loss: 0.4633006751537323, Total_reward: 102.0
Epoch: 50, Loss: 1.6884794235229492, Total_reward: 107.0
Epoch: 60, Loss: 1.4151698350906372, Total_reward: 137.0
Epoch: 70, Loss: 1.052544116973877, Total_reward: 109.0
Epoch: 80, Loss: 0.8950512409210205, Total_reward: 111.0
Epoch: 90, Loss: 1.9253766536712646, Total_reward: 109.0
Epoch: 100, Loss: 0.8488576412200928, Total_reward: 138.0
Epoch: 110, Loss: 1.663291573524475, Total_reward: 214.0
Epoch: 120, Loss: 1.6414588689804077, Total_reward: 139.0
Epoch: 130, Loss: 4.551301002502441, Total_reward: 160.0
Epoch: 140, Loss: 2.619988441467285, Total_reward: 145.0
Epoch: 150, Loss: 2.5985851287841797, Total_reward: 87.0
Epoch: 160, Loss: 4.5418524742126465, Total_reward: 101.0
Epoch: 170, Loss: 4.521094799041748, Total_reward: 166.0
Epoch: 180, Loss: 1.0994915962219238, Total_reward: 119.0
Epoch: 190, Loss: 4.50348424911499, Total_reward: 86.0
Epoch: 200, Loss: 7.149417877197266, Total_reward: 96.0
Epoch: 210, Loss: 6.525698661804199, Total_reward: 119.0
Epoch: 220, Loss: 7.819526195526123, Total_reward: 118.0
Epoch: 230, Loss: 4.453639507293701, Total_reward: 94.0
Epoch: 240, Loss: 4.232844829559326, Total_reward: 88.0
Epoch: 250, Loss: 7.739490985870361, Total_reward: 183.0
Epoch: 260, Loss: 5.233733177185059, Total_reward: 193.0
Epoch: 270, Loss: 2.4293456077575684, Total_reward: 220.0
Epoch: 280, Loss: 3.7491207122802734, Total_reward: 154.0
Epoch: 290, Loss: 3.293147087097168, Total_reward: 144.0
Average reward over 10 runs (invisible): 12.9
result : 12.9
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.0632004663348198, Total_reward: 101.0
Epoch: 20, Loss: 1.2326271533966064, Total_reward: 101.0
Epoch: 30, Loss: 3.553266763687134, Total_reward: 112.0
Epoch: 40, Loss: 3.9665908813476562, Total_reward: 106.0
Epoch: 50, Loss: 4.436031818389893, Total_reward: 143.0
Epoch: 60, Loss: 5.680524826049805, Total_reward: 120.0
Epoch: 70, Loss: 5.003096580505371, Total_reward: 161.0
Epoch: 80, Loss: 4.563930034637451, Total_reward: 181.0
Epoch: 90, Loss: 3.799086093902588, Total_reward: 142.0
Epoch: 100, Loss: 6.606140613555908, Total_reward: 92.0
Epoch: 110, Loss: 10.23950481414795, Total_reward: 136.0
Epoch: 120, Loss: 3.59185791015625, Total_reward: 120.0
Epoch: 130, Loss: 12.031685829162598, Total_reward: 120.0
Epoch: 140, Loss: 11.011505126953125, Total_reward: 97.0
Epoch: 150, Loss: 12.25992202758789, Total_reward: 102.0
Epoch: 160, Loss: 7.035776138305664, Total_reward: 117.0
Epoch: 170, Loss: 12.8301420211792, Total_reward: 96.0
Epoch: 180, Loss: 8.313804626464844, Total_reward: 87.0
Epoch: 190, Loss: 8.131449699401855, Total_reward: 99.0
Epoch: 200, Loss: 13.066085815429688, Total_reward: 172.0
Epoch: 210, Loss: 9.412672996520996, Total_reward: 122.0
Epoch: 220, Loss: 9.547988891601562, Total_reward: 184.0
Epoch: 230, Loss: 11.171060562133789, Total_reward: 180.0
Epoch: 240, Loss: 9.240989685058594, Total_reward: 131.0
Epoch: 250, Loss: 6.802040100097656, Total_reward: 122.0
Epoch: 260, Loss: 12.094021797180176, Total_reward: 93.0
Epoch: 270, Loss: 20.95866584777832, Total_reward: 84.0
Epoch: 280, Loss: 19.842411041259766, Total_reward: 146.0
Epoch: 290, Loss: 17.624431610107422, Total_reward: 86.0
Average reward over 10 runs (invisible): 21.6
result : 21.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.00551188550889492, Total_reward: 115.0
Epoch: 20, Loss: 0.09605885297060013, Total_reward: 151.0
Epoch: 30, Loss: 0.10063587874174118, Total_reward: 179.0
Epoch: 40, Loss: 0.22667646408081055, Total_reward: 112.0
Epoch: 50, Loss: 0.01628962531685829, Total_reward: 228.0
Epoch: 60, Loss: 0.24115775525569916, Total_reward: 190.0
Epoch: 70, Loss: 1.475311040878296, Total_reward: 184.0
Epoch: 80, Loss: 0.974740207195282, Total_reward: 196.0
Epoch: 90, Loss: 1.6576592922210693, Total_reward: 177.0
Epoch: 100, Loss: 2.373119831085205, Total_reward: 185.0
Epoch: 110, Loss: 1.5887315273284912, Total_reward: 207.0
Epoch: 120, Loss: 1.5546634197235107, Total_reward: 189.0
Epoch: 130, Loss: 2.0848848819732666, Total_reward: 135.0
Epoch: 140, Loss: 2.2244796752929688, Total_reward: 127.0
Epoch: 150, Loss: 2.594968795776367, Total_reward: 185.0
Epoch: 160, Loss: 3.9169692993164062, Total_reward: 129.0
Epoch: 170, Loss: 4.76403284072876, Total_reward: 152.0
Epoch: 180, Loss: 4.8972883224487305, Total_reward: 180.0
Epoch: 190, Loss: 3.6617190837860107, Total_reward: 169.0
Epoch: 200, Loss: 5.960609436035156, Total_reward: 178.0
Epoch: 210, Loss: 6.569480895996094, Total_reward: 136.0
Epoch: 220, Loss: 4.033787250518799, Total_reward: 135.0
Epoch: 230, Loss: 6.144952297210693, Total_reward: 119.0
Epoch: 240, Loss: 4.761730670928955, Total_reward: 143.0
Epoch: 250, Loss: 4.34306526184082, Total_reward: 122.0
Epoch: 260, Loss: 9.883617401123047, Total_reward: 149.0
Epoch: 270, Loss: 5.113422393798828, Total_reward: 149.0
Epoch: 280, Loss: 2.3895721435546875, Total_reward: 155.0
Epoch: 290, Loss: 4.585441589355469, Total_reward: 160.0
Average reward over 10 runs (invisible): 15.4
result : 15.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.26143792271614075, Total_reward: 124.0
Epoch: 20, Loss: 1.104897141456604, Total_reward: 101.0
Epoch: 30, Loss: 2.215740919113159, Total_reward: 140.0
Epoch: 40, Loss: 4.911004066467285, Total_reward: 103.0
Epoch: 50, Loss: 7.499752998352051, Total_reward: 149.0
Epoch: 60, Loss: 3.4508633613586426, Total_reward: 117.0
Epoch: 70, Loss: 9.425948143005371, Total_reward: 93.0
Epoch: 80, Loss: 9.661327362060547, Total_reward: 146.0
Epoch: 90, Loss: 8.785199165344238, Total_reward: 117.0
Epoch: 100, Loss: 2.787764310836792, Total_reward: 168.0
Epoch: 110, Loss: 6.1415534019470215, Total_reward: 271.0
Epoch: 120, Loss: 8.511788368225098, Total_reward: 136.0
Epoch: 130, Loss: 19.245115280151367, Total_reward: 125.0
Epoch: 140, Loss: 11.391897201538086, Total_reward: 139.0
Epoch: 150, Loss: 10.859408378601074, Total_reward: 120.0
Epoch: 160, Loss: 17.690196990966797, Total_reward: 131.0
Epoch: 170, Loss: 22.11154556274414, Total_reward: 169.0
Epoch: 180, Loss: 13.164643287658691, Total_reward: 154.0
Epoch: 190, Loss: 16.06234359741211, Total_reward: 175.0
Epoch: 200, Loss: 16.604230880737305, Total_reward: 191.0
Epoch: 210, Loss: 8.182680130004883, Total_reward: 151.0
Epoch: 220, Loss: 9.7137451171875, Total_reward: 129.0
Epoch: 230, Loss: 11.433079719543457, Total_reward: 231.0
Epoch: 240, Loss: 11.717906951904297, Total_reward: 182.0
Epoch: 250, Loss: 13.702523231506348, Total_reward: 120.0
Epoch: 260, Loss: 21.567270278930664, Total_reward: 91.0
Epoch: 270, Loss: 9.660837173461914, Total_reward: 126.0
Epoch: 280, Loss: 6.636106491088867, Total_reward: 127.0
Epoch: 290, Loss: 14.116539001464844, Total_reward: 110.0
Average reward over 10 runs (invisible): 12.7
result : 12.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.05180776119232178, Total_reward: 115.0
Epoch: 20, Loss: 0.1688406616449356, Total_reward: 165.0
Epoch: 30, Loss: 0.2201043963432312, Total_reward: 107.0
Epoch: 40, Loss: 0.16651836037635803, Total_reward: 154.0
Epoch: 50, Loss: 0.6673604249954224, Total_reward: 117.0
Epoch: 60, Loss: 0.6308441162109375, Total_reward: 181.0
Epoch: 70, Loss: 1.103720784187317, Total_reward: 180.0
Epoch: 80, Loss: 0.8581010103225708, Total_reward: 114.0
Epoch: 90, Loss: 1.490173578262329, Total_reward: 150.0
Epoch: 100, Loss: 1.2414734363555908, Total_reward: 145.0
Epoch: 110, Loss: 1.6893378496170044, Total_reward: 163.0
Epoch: 120, Loss: 4.6132612228393555, Total_reward: 200.0
Epoch: 130, Loss: 2.0406901836395264, Total_reward: 104.0
Epoch: 140, Loss: 2.1769652366638184, Total_reward: 178.0
Epoch: 150, Loss: 4.0774407386779785, Total_reward: 158.0
Epoch: 160, Loss: 0.806768000125885, Total_reward: 117.0
Epoch: 170, Loss: 1.3625171184539795, Total_reward: 114.0
Epoch: 180, Loss: 6.322932720184326, Total_reward: 125.0
Epoch: 190, Loss: 1.9399583339691162, Total_reward: 118.0
Epoch: 200, Loss: 6.06037712097168, Total_reward: 97.0
Epoch: 210, Loss: 5.35185432434082, Total_reward: 183.0
Epoch: 220, Loss: 3.3730292320251465, Total_reward: 104.0
Epoch: 230, Loss: 7.0962629318237305, Total_reward: 115.0
Epoch: 240, Loss: 5.460343837738037, Total_reward: 167.0
Epoch: 250, Loss: 7.140808582305908, Total_reward: 151.0
Epoch: 260, Loss: 5.150371074676514, Total_reward: 140.0
Epoch: 270, Loss: 5.478988170623779, Total_reward: 122.0
Epoch: 280, Loss: 1.721877932548523, Total_reward: 122.0
Epoch: 290, Loss: 7.786824703216553, Total_reward: 174.0
Average reward over 10 runs (invisible): 38.5
result : 38.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.15291918814182281, Total_reward: 101.0
Epoch: 20, Loss: 1.2529504299163818, Total_reward: 150.0
Epoch: 30, Loss: 1.6094536781311035, Total_reward: 110.0
Epoch: 40, Loss: 2.6784493923187256, Total_reward: 136.0
Epoch: 50, Loss: 1.683066964149475, Total_reward: 117.0
Epoch: 60, Loss: 9.734633445739746, Total_reward: 107.0
Epoch: 70, Loss: 9.549419403076172, Total_reward: 102.0
Epoch: 80, Loss: 10.881890296936035, Total_reward: 83.0
Epoch: 90, Loss: 11.244933128356934, Total_reward: 87.0
Epoch: 100, Loss: 14.925216674804688, Total_reward: 120.0
Epoch: 110, Loss: 24.072986602783203, Total_reward: 94.0
Epoch: 120, Loss: 16.468791961669922, Total_reward: 89.0
Epoch: 130, Loss: 24.62302589416504, Total_reward: 86.0
Epoch: 140, Loss: 30.215930938720703, Total_reward: 100.0
Epoch: 150, Loss: 24.21404457092285, Total_reward: 85.0
Epoch: 160, Loss: 16.15855598449707, Total_reward: 104.0
Epoch: 170, Loss: 12.859023094177246, Total_reward: 126.0
Epoch: 180, Loss: 23.300155639648438, Total_reward: 107.0
Epoch: 190, Loss: 13.38707160949707, Total_reward: 101.0
Epoch: 200, Loss: 12.123550415039062, Total_reward: 97.0
Epoch: 210, Loss: 12.28242301940918, Total_reward: 92.0
Epoch: 220, Loss: 13.287372589111328, Total_reward: 121.0
Epoch: 230, Loss: 16.48558235168457, Total_reward: 159.0
Epoch: 240, Loss: 9.115793228149414, Total_reward: 172.0
Epoch: 250, Loss: 15.058589935302734, Total_reward: 165.0
Epoch: 260, Loss: 4.476208209991455, Total_reward: 135.0
Epoch: 270, Loss: 11.55423641204834, Total_reward: 153.0
Epoch: 280, Loss: 17.73904800415039, Total_reward: 105.0
Epoch: 290, Loss: 5.171330451965332, Total_reward: 134.0
Average reward over 10 runs (invisible): 9.4
result : 9.4
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.18631751835346222, Total_reward: 98.0
Epoch: 20, Loss: 0.05753586068749428, Total_reward: 88.0
Epoch: 30, Loss: 0.20661166310310364, Total_reward: 119.0
Epoch: 40, Loss: 0.009923421777784824, Total_reward: 107.0
Epoch: 50, Loss: 0.4175386130809784, Total_reward: 108.0
Epoch: 60, Loss: 0.9772170782089233, Total_reward: 146.0
Epoch: 70, Loss: 0.5464295744895935, Total_reward: 125.0
Epoch: 80, Loss: 1.5048538446426392, Total_reward: 144.0
Epoch: 90, Loss: 1.003517508506775, Total_reward: 179.0
Epoch: 100, Loss: 2.280782461166382, Total_reward: 172.0
Epoch: 110, Loss: 2.1718015670776367, Total_reward: 171.0
Epoch: 120, Loss: 2.5565598011016846, Total_reward: 163.0
Epoch: 130, Loss: 1.3757500648498535, Total_reward: 111.0
Epoch: 140, Loss: 2.8549866676330566, Total_reward: 138.0
Epoch: 150, Loss: 0.9381004571914673, Total_reward: 213.0
Epoch: 160, Loss: 3.3310933113098145, Total_reward: 183.0
Epoch: 170, Loss: 0.6248750686645508, Total_reward: 191.0
Epoch: 180, Loss: 2.1217832565307617, Total_reward: 196.0
Epoch: 190, Loss: 2.8065989017486572, Total_reward: 165.0
Epoch: 200, Loss: 2.5118649005889893, Total_reward: 181.0
Epoch: 210, Loss: 1.5864214897155762, Total_reward: 136.0
Epoch: 220, Loss: 5.700740814208984, Total_reward: 125.0
Epoch: 230, Loss: 4.365874290466309, Total_reward: 147.0
Epoch: 240, Loss: 5.0759992599487305, Total_reward: 157.0
Epoch: 250, Loss: 14.185114860534668, Total_reward: 183.0
Epoch: 260, Loss: 8.177122116088867, Total_reward: 163.0
Epoch: 270, Loss: 4.21626615524292, Total_reward: 191.0
Epoch: 280, Loss: 4.4522294998168945, Total_reward: 144.0
Epoch: 290, Loss: 5.358255386352539, Total_reward: 198.0
Average reward over 10 runs (invisible): 67.6
result : 67.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.04061933606863022, Total_reward: 161.0
Epoch: 20, Loss: 0.6049666404724121, Total_reward: 125.0
Epoch: 30, Loss: 0.17003998160362244, Total_reward: 116.0
Epoch: 40, Loss: 2.4318249225616455, Total_reward: 171.0
Epoch: 50, Loss: 6.092123508453369, Total_reward: 131.0
Epoch: 60, Loss: 5.775520324707031, Total_reward: 139.0
Epoch: 70, Loss: 4.392404556274414, Total_reward: 160.0
Epoch: 80, Loss: 4.812177658081055, Total_reward: 122.0
Epoch: 90, Loss: 10.248544692993164, Total_reward: 170.0
Epoch: 100, Loss: 2.935102939605713, Total_reward: 98.0
Epoch: 110, Loss: 4.063673973083496, Total_reward: 135.0
Epoch: 120, Loss: 5.102968692779541, Total_reward: 127.0
Epoch: 130, Loss: 9.303855895996094, Total_reward: 121.0
Epoch: 140, Loss: 15.477945327758789, Total_reward: 136.0
Epoch: 150, Loss: 8.60853099822998, Total_reward: 170.0
Epoch: 160, Loss: 14.340367317199707, Total_reward: 127.0
Epoch: 170, Loss: 20.02939796447754, Total_reward: 126.0
Epoch: 180, Loss: 13.375795364379883, Total_reward: 99.0
Epoch: 190, Loss: 15.654801368713379, Total_reward: 98.0
Epoch: 200, Loss: 33.71442794799805, Total_reward: 134.0
Epoch: 210, Loss: 28.5194034576416, Total_reward: 100.0
Epoch: 220, Loss: 36.89224624633789, Total_reward: 115.0
Epoch: 230, Loss: 24.735694885253906, Total_reward: 89.0
Epoch: 240, Loss: 10.824783325195312, Total_reward: 99.0
Epoch: 250, Loss: 20.701927185058594, Total_reward: 124.0
Epoch: 260, Loss: 16.867704391479492, Total_reward: 106.0
Epoch: 270, Loss: 11.681548118591309, Total_reward: 113.0
Epoch: 280, Loss: 24.418882369995117, Total_reward: 105.0
Epoch: 290, Loss: 6.124563694000244, Total_reward: 115.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.023849138990044594, Total_reward: 97.0
Epoch: 20, Loss: 0.007724221795797348, Total_reward: 86.0
Epoch: 30, Loss: 0.08623788505792618, Total_reward: 87.0
Epoch: 40, Loss: 0.11439139395952225, Total_reward: 90.0
Epoch: 50, Loss: 0.19925808906555176, Total_reward: 97.0
Epoch: 60, Loss: 0.39394617080688477, Total_reward: 90.0
Epoch: 70, Loss: 1.0190861225128174, Total_reward: 132.0
Epoch: 80, Loss: 1.0943628549575806, Total_reward: 92.0
Epoch: 90, Loss: 0.35760724544525146, Total_reward: 117.0
Epoch: 100, Loss: 1.4329098463058472, Total_reward: 145.0
Epoch: 110, Loss: 2.329601526260376, Total_reward: 91.0
Epoch: 120, Loss: 1.768212080001831, Total_reward: 119.0
Epoch: 130, Loss: 2.5746915340423584, Total_reward: 168.0
Epoch: 140, Loss: 3.447307586669922, Total_reward: 108.0
Epoch: 150, Loss: 3.117509603500366, Total_reward: 115.0
Epoch: 160, Loss: 1.963343620300293, Total_reward: 93.0
Epoch: 170, Loss: 4.97079610824585, Total_reward: 134.0
Epoch: 180, Loss: 1.7476918697357178, Total_reward: 109.0
Epoch: 190, Loss: 5.503091812133789, Total_reward: 86.0
Epoch: 200, Loss: 4.263257026672363, Total_reward: 96.0
Epoch: 210, Loss: 4.060883045196533, Total_reward: 107.0
Epoch: 220, Loss: 4.00971794128418, Total_reward: 120.0
Epoch: 230, Loss: 4.540207386016846, Total_reward: 119.0
Epoch: 240, Loss: 2.704896926879883, Total_reward: 136.0
Epoch: 250, Loss: 10.161788940429688, Total_reward: 134.0
Epoch: 260, Loss: 5.089138031005859, Total_reward: 116.0
Epoch: 270, Loss: 3.6199917793273926, Total_reward: 82.0
Epoch: 280, Loss: 6.241161346435547, Total_reward: 95.0
Epoch: 290, Loss: 2.6339824199676514, Total_reward: 103.0
Average reward over 10 runs (invisible): 9.1
result : 9.1
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.042833972722291946, Total_reward: 111.0
Epoch: 20, Loss: 1.439026951789856, Total_reward: 101.0
Epoch: 30, Loss: 2.759921073913574, Total_reward: 129.0
Epoch: 40, Loss: 5.013144016265869, Total_reward: 106.0
Epoch: 50, Loss: 6.8772430419921875, Total_reward: 121.0
Epoch: 60, Loss: 7.259726047515869, Total_reward: 129.0
Epoch: 70, Loss: 16.949480056762695, Total_reward: 144.0
Epoch: 80, Loss: 16.081878662109375, Total_reward: 116.0
Epoch: 90, Loss: 12.938529014587402, Total_reward: 141.0
Epoch: 100, Loss: 21.36410140991211, Total_reward: 104.0
Epoch: 110, Loss: 24.190759658813477, Total_reward: 93.0
Epoch: 120, Loss: 13.394708633422852, Total_reward: 117.0
Epoch: 130, Loss: 12.975868225097656, Total_reward: 157.0
Epoch: 140, Loss: 31.5477294921875, Total_reward: 115.0
Epoch: 150, Loss: 27.415422439575195, Total_reward: 169.0
Epoch: 160, Loss: 26.11416244506836, Total_reward: 114.0
Epoch: 170, Loss: 17.478729248046875, Total_reward: 127.0
Epoch: 180, Loss: 20.809324264526367, Total_reward: 90.0
Epoch: 190, Loss: 31.46611785888672, Total_reward: 92.0
Epoch: 200, Loss: 20.361204147338867, Total_reward: 126.0
Epoch: 210, Loss: 27.642131805419922, Total_reward: 106.0
Epoch: 220, Loss: 27.61945343017578, Total_reward: 127.0
Epoch: 230, Loss: 9.600223541259766, Total_reward: 102.0
Epoch: 240, Loss: 18.73060417175293, Total_reward: 114.0
Epoch: 250, Loss: 22.31303596496582, Total_reward: 92.0
Epoch: 260, Loss: 27.92629623413086, Total_reward: 100.0
Epoch: 270, Loss: 18.1934814453125, Total_reward: 109.0
Epoch: 280, Loss: 12.709310531616211, Total_reward: 126.0
Epoch: 290, Loss: 16.184478759765625, Total_reward: 83.0
Average reward over 10 runs (invisible): 9.8
result : 9.8
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.0015296984929591417, Total_reward: 124.0
Epoch: 20, Loss: 0.04362811893224716, Total_reward: 84.0
Epoch: 30, Loss: 0.14898838102817535, Total_reward: 115.0
Epoch: 40, Loss: 0.292330265045166, Total_reward: 116.0
Epoch: 50, Loss: 0.6347470283508301, Total_reward: 176.0
Epoch: 60, Loss: 0.6763209104537964, Total_reward: 152.0
Epoch: 70, Loss: 0.6859104037284851, Total_reward: 146.0
Epoch: 80, Loss: 1.5491454601287842, Total_reward: 109.0
Epoch: 90, Loss: 1.5105578899383545, Total_reward: 127.0
Epoch: 100, Loss: 1.316482424736023, Total_reward: 137.0
Epoch: 110, Loss: 2.073645830154419, Total_reward: 156.0
Epoch: 120, Loss: 0.9407841563224792, Total_reward: 121.0
Epoch: 130, Loss: 1.2434747219085693, Total_reward: 85.0
Epoch: 140, Loss: 1.764512062072754, Total_reward: 136.0
Epoch: 150, Loss: 5.979693412780762, Total_reward: 100.0
Epoch: 160, Loss: 4.476444721221924, Total_reward: 99.0
Epoch: 170, Loss: 3.7997584342956543, Total_reward: 172.0
Epoch: 180, Loss: 2.492089033126831, Total_reward: 109.0
Epoch: 190, Loss: 3.8442115783691406, Total_reward: 114.0
Epoch: 200, Loss: 4.05006217956543, Total_reward: 106.0
Epoch: 210, Loss: 4.583223819732666, Total_reward: 104.0
Epoch: 220, Loss: 9.151729583740234, Total_reward: 88.0
Epoch: 230, Loss: 3.709256172180176, Total_reward: 140.0
Epoch: 240, Loss: 9.431402206420898, Total_reward: 140.0
Epoch: 250, Loss: 5.450076580047607, Total_reward: 137.0
Epoch: 260, Loss: 6.495185852050781, Total_reward: 120.0
Epoch: 270, Loss: 9.445855140686035, Total_reward: 103.0
Epoch: 280, Loss: 0.8774755597114563, Total_reward: 90.0
Epoch: 290, Loss: 7.038339614868164, Total_reward: 115.0
Average reward over 10 runs (invisible): 9.0
result : 9.0
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.15931087732315063, Total_reward: 118.0
Epoch: 20, Loss: 0.06847071647644043, Total_reward: 129.0
Epoch: 30, Loss: 1.5409648418426514, Total_reward: 103.0
Epoch: 40, Loss: 5.911698818206787, Total_reward: 99.0
Epoch: 50, Loss: 4.755616664886475, Total_reward: 94.0
Epoch: 60, Loss: 12.353879928588867, Total_reward: 89.0
Epoch: 70, Loss: 11.508157730102539, Total_reward: 123.0
Epoch: 80, Loss: 28.211524963378906, Total_reward: 94.0
Epoch: 90, Loss: 11.817304611206055, Total_reward: 106.0
Epoch: 100, Loss: 13.288755416870117, Total_reward: 142.0
Epoch: 110, Loss: 7.628235816955566, Total_reward: 120.0
Epoch: 120, Loss: 8.731721878051758, Total_reward: 118.0
Epoch: 130, Loss: 15.593214988708496, Total_reward: 165.0
Epoch: 140, Loss: 29.702911376953125, Total_reward: 125.0
Epoch: 150, Loss: 12.81572437286377, Total_reward: 110.0
Epoch: 160, Loss: 11.340165138244629, Total_reward: 170.0
Epoch: 170, Loss: 7.947617530822754, Total_reward: 107.0
Epoch: 180, Loss: 26.814599990844727, Total_reward: 145.0
Epoch: 190, Loss: 16.570125579833984, Total_reward: 129.0
Epoch: 200, Loss: 28.095172882080078, Total_reward: 154.0
Epoch: 210, Loss: 10.01132869720459, Total_reward: 112.0
Epoch: 220, Loss: 20.27909278869629, Total_reward: 127.0
Epoch: 230, Loss: 8.903966903686523, Total_reward: 136.0
Epoch: 240, Loss: 40.91276931762695, Total_reward: 95.0
Epoch: 250, Loss: 39.73273468017578, Total_reward: 141.0
Epoch: 260, Loss: 27.85379981994629, Total_reward: 136.0
Epoch: 270, Loss: 13.588268280029297, Total_reward: 126.0
Epoch: 280, Loss: 15.962042808532715, Total_reward: 115.0
Epoch: 290, Loss: 14.8118896484375, Total_reward: 155.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 64, 'max_action_by_epoch': 40, 'replay_memory_size': 1000, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.00925766583532095, Total_reward: 101.0
Epoch: 20, Loss: 0.07744286954402924, Total_reward: 86.0
Epoch: 30, Loss: 0.2945282757282257, Total_reward: 121.0
Epoch: 40, Loss: 0.5574164390563965, Total_reward: 166.0
Epoch: 50, Loss: 0.6493411064147949, Total_reward: 123.0
Epoch: 60, Loss: 1.3329449892044067, Total_reward: 131.0
Epoch: 70, Loss: 0.5071420669555664, Total_reward: 151.0
Epoch: 80, Loss: 1.1071714162826538, Total_reward: 116.0
Epoch: 90, Loss: 1.9175065755844116, Total_reward: 220.0
Epoch: 100, Loss: 2.2828023433685303, Total_reward: 141.0
Epoch: 110, Loss: 1.3630692958831787, Total_reward: 119.0
Epoch: 120, Loss: 4.040429592132568, Total_reward: 88.0
Epoch: 130, Loss: 4.133093357086182, Total_reward: 167.0
Epoch: 140, Loss: 1.51046884059906, Total_reward: 157.0
Epoch: 150, Loss: 5.540719985961914, Total_reward: 150.0
Epoch: 160, Loss: 4.310822010040283, Total_reward: 121.0
Epoch: 170, Loss: 4.4620184898376465, Total_reward: 112.0
Epoch: 180, Loss: 6.139885902404785, Total_reward: 119.0
Epoch: 190, Loss: 6.129884719848633, Total_reward: 105.0
Epoch: 200, Loss: 3.7707557678222656, Total_reward: 105.0
Epoch: 210, Loss: 3.3398101329803467, Total_reward: 103.0
Epoch: 220, Loss: 5.695509910583496, Total_reward: 131.0
Epoch: 230, Loss: 8.254039764404297, Total_reward: 119.0
Epoch: 240, Loss: 9.372109413146973, Total_reward: 97.0
Epoch: 250, Loss: 8.551868438720703, Total_reward: 107.0
Epoch: 260, Loss: 7.187903881072998, Total_reward: 148.0
Epoch: 270, Loss: 6.731806755065918, Total_reward: 146.0
Epoch: 280, Loss: 10.701340675354004, Total_reward: 133.0
Epoch: 290, Loss: 4.386928081512451, Total_reward: 155.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1555812805891037, Total_reward: 95.0
Epoch: 20, Loss: 0.9699068069458008, Total_reward: 105.0
Epoch: 30, Loss: 1.5144532918930054, Total_reward: 132.0
Epoch: 40, Loss: 3.3263401985168457, Total_reward: 95.0
Epoch: 50, Loss: 4.183804035186768, Total_reward: 99.0
Epoch: 60, Loss: 2.9083094596862793, Total_reward: 140.0
Epoch: 70, Loss: 6.056016445159912, Total_reward: 127.0
Epoch: 80, Loss: 6.336509704589844, Total_reward: 106.0
Epoch: 90, Loss: 7.482484817504883, Total_reward: 85.0
Epoch: 100, Loss: 9.40697956085205, Total_reward: 102.0
Epoch: 110, Loss: 8.170757293701172, Total_reward: 147.0
Epoch: 120, Loss: 8.31755256652832, Total_reward: 135.0
Epoch: 130, Loss: 4.904997825622559, Total_reward: 114.0
Epoch: 140, Loss: 8.791421890258789, Total_reward: 151.0
Epoch: 150, Loss: 15.960321426391602, Total_reward: 137.0
Epoch: 160, Loss: 6.898839950561523, Total_reward: 121.0
Epoch: 170, Loss: 14.521503448486328, Total_reward: 114.0
Epoch: 180, Loss: 14.944080352783203, Total_reward: 129.0
Epoch: 190, Loss: 11.046893119812012, Total_reward: 119.0
Epoch: 200, Loss: 6.744424343109131, Total_reward: 127.0
Epoch: 210, Loss: 18.074514389038086, Total_reward: 128.0
Epoch: 220, Loss: 13.529337882995605, Total_reward: 127.0
Epoch: 230, Loss: 13.811223030090332, Total_reward: 156.0
Epoch: 240, Loss: 13.41356372833252, Total_reward: 127.0
Epoch: 250, Loss: 17.48248291015625, Total_reward: 163.0
Epoch: 260, Loss: 17.605382919311523, Total_reward: 154.0
Epoch: 270, Loss: 23.391735076904297, Total_reward: 170.0
Epoch: 280, Loss: 9.60679817199707, Total_reward: 166.0
Epoch: 290, Loss: 8.24580192565918, Total_reward: 166.0
Average reward over 10 runs (invisible): 30.5
result : 30.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.15991944074630737, Total_reward: 103.0
Epoch: 20, Loss: 0.10658949613571167, Total_reward: 83.0
Epoch: 30, Loss: 0.5050827264785767, Total_reward: 83.0
Epoch: 40, Loss: 0.4867701530456543, Total_reward: 86.0
Epoch: 50, Loss: 1.3510105609893799, Total_reward: 104.0
Epoch: 60, Loss: 1.174206018447876, Total_reward: 130.0
Epoch: 70, Loss: 1.6323356628417969, Total_reward: 129.0
Epoch: 80, Loss: 1.5095009803771973, Total_reward: 109.0
Epoch: 90, Loss: 1.4618499279022217, Total_reward: 103.0
Epoch: 100, Loss: 2.606839179992676, Total_reward: 91.0
Epoch: 110, Loss: 2.0595834255218506, Total_reward: 96.0
Epoch: 120, Loss: 3.1319046020507812, Total_reward: 96.0
Epoch: 130, Loss: 3.1354503631591797, Total_reward: 134.0
Epoch: 140, Loss: 3.251340389251709, Total_reward: 131.0
Epoch: 150, Loss: 2.9958744049072266, Total_reward: 96.0
Epoch: 160, Loss: 3.5643150806427, Total_reward: 104.0
Epoch: 170, Loss: 4.984647750854492, Total_reward: 103.0
Epoch: 180, Loss: 4.142994403839111, Total_reward: 96.0
Epoch: 190, Loss: 5.194700717926025, Total_reward: 135.0
Epoch: 200, Loss: 1.9960899353027344, Total_reward: 170.0
Epoch: 210, Loss: 5.249046325683594, Total_reward: 103.0
Epoch: 220, Loss: 4.954007148742676, Total_reward: 107.0
Epoch: 230, Loss: 3.7514595985412598, Total_reward: 166.0
Epoch: 240, Loss: 5.7186808586120605, Total_reward: 101.0
Epoch: 250, Loss: 5.701835632324219, Total_reward: 150.0
Epoch: 260, Loss: 2.9226043224334717, Total_reward: 152.0
Epoch: 270, Loss: 3.607614040374756, Total_reward: 122.0
Epoch: 280, Loss: 2.161717653274536, Total_reward: 107.0
Epoch: 290, Loss: 3.688610553741455, Total_reward: 105.0
Average reward over 10 runs (invisible): 9.2
result : 9.2
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.33834731578826904, Total_reward: 120.0
Epoch: 20, Loss: 0.6911917924880981, Total_reward: 130.0
Epoch: 30, Loss: 2.1124820709228516, Total_reward: 137.0
Epoch: 40, Loss: 2.046086072921753, Total_reward: 150.0
Epoch: 50, Loss: 5.2531352043151855, Total_reward: 130.0
Epoch: 60, Loss: 7.208559036254883, Total_reward: 96.0
Epoch: 70, Loss: 3.8267998695373535, Total_reward: 148.0
Epoch: 80, Loss: 9.584582328796387, Total_reward: 126.0
Epoch: 90, Loss: 4.513370513916016, Total_reward: 140.0
Epoch: 100, Loss: 4.338310241699219, Total_reward: 187.0
Epoch: 110, Loss: 2.3428564071655273, Total_reward: 150.0
Epoch: 120, Loss: 9.061942100524902, Total_reward: 127.0
Epoch: 130, Loss: 6.84052848815918, Total_reward: 135.0
Epoch: 140, Loss: 16.778390884399414, Total_reward: 125.0
Epoch: 150, Loss: 18.100494384765625, Total_reward: 153.0
Epoch: 160, Loss: 14.951804161071777, Total_reward: 158.0
Epoch: 170, Loss: 16.085371017456055, Total_reward: 143.0
Epoch: 180, Loss: 13.956226348876953, Total_reward: 144.0
Epoch: 190, Loss: 26.293851852416992, Total_reward: 167.0
Epoch: 200, Loss: 15.221090316772461, Total_reward: 154.0
Epoch: 210, Loss: 32.742706298828125, Total_reward: 144.0
Epoch: 220, Loss: 20.303531646728516, Total_reward: 172.0
Epoch: 230, Loss: 23.78358268737793, Total_reward: 140.0
Epoch: 240, Loss: 9.103355407714844, Total_reward: 164.0
Epoch: 250, Loss: 13.42807674407959, Total_reward: 136.0
Epoch: 260, Loss: 45.16194152832031, Total_reward: 119.0
Epoch: 270, Loss: 30.039142608642578, Total_reward: 165.0
Epoch: 280, Loss: 25.816265106201172, Total_reward: 180.0
Epoch: 290, Loss: 18.214534759521484, Total_reward: 138.0
Average reward over 10 runs (invisible): 9.5
result : 9.5
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 0, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.08385542035102844, Total_reward: 113.0
Epoch: 20, Loss: 0.08334977179765701, Total_reward: 117.0
Epoch: 30, Loss: 0.33241161704063416, Total_reward: 143.0
Epoch: 40, Loss: 0.3771111071109772, Total_reward: 108.0
Epoch: 50, Loss: 1.1752820014953613, Total_reward: 102.0
Epoch: 60, Loss: 1.131357192993164, Total_reward: 96.0
Epoch: 70, Loss: 1.9404304027557373, Total_reward: 87.0
Epoch: 80, Loss: 2.0548253059387207, Total_reward: 85.0
Epoch: 90, Loss: 1.6230319738388062, Total_reward: 87.0
Epoch: 100, Loss: 1.4773234128952026, Total_reward: 95.0
Epoch: 110, Loss: 2.246802806854248, Total_reward: 156.0
Epoch: 120, Loss: 1.6257907152175903, Total_reward: 169.0
Epoch: 130, Loss: 2.0505530834198, Total_reward: 166.0
Epoch: 140, Loss: 1.1435397863388062, Total_reward: 154.0
Epoch: 150, Loss: 1.966043472290039, Total_reward: 168.0
Epoch: 160, Loss: 1.6849137544631958, Total_reward: 154.0
Epoch: 170, Loss: 4.510571479797363, Total_reward: 126.0
Epoch: 180, Loss: 3.9149811267852783, Total_reward: 103.0
Epoch: 190, Loss: 3.8474860191345215, Total_reward: 156.0
Epoch: 200, Loss: 4.13828706741333, Total_reward: 173.0
Epoch: 210, Loss: 3.506248712539673, Total_reward: 171.0
Epoch: 220, Loss: 5.115988254547119, Total_reward: 104.0
Epoch: 230, Loss: 2.5114407539367676, Total_reward: 161.0
Epoch: 240, Loss: 4.001709461212158, Total_reward: 168.0
Epoch: 250, Loss: 3.24349308013916, Total_reward: 177.0
Epoch: 260, Loss: 2.354475975036621, Total_reward: 156.0
Epoch: 270, Loss: 3.5661420822143555, Total_reward: 184.0
Epoch: 280, Loss: 6.489562034606934, Total_reward: 174.0
Epoch: 290, Loss: 1.310933232307434, Total_reward: 170.0
Average reward over 10 runs (invisible): 25.7
result : 25.7
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.14376337826251984, Total_reward: 124.0
Epoch: 20, Loss: 0.9074844121932983, Total_reward: 187.0
Epoch: 30, Loss: 1.849188208580017, Total_reward: 186.0
Epoch: 40, Loss: 3.0593178272247314, Total_reward: 155.0
Epoch: 50, Loss: 3.8033907413482666, Total_reward: 299.0
Epoch: 60, Loss: 4.94912052154541, Total_reward: 220.0
Epoch: 70, Loss: 8.134371757507324, Total_reward: 113.0
Epoch: 80, Loss: 7.449919700622559, Total_reward: 175.0
Epoch: 90, Loss: 4.808788299560547, Total_reward: 222.0
Epoch: 100, Loss: 14.321439743041992, Total_reward: 142.0
Epoch: 110, Loss: 16.50987434387207, Total_reward: 117.0
Epoch: 120, Loss: 18.93145751953125, Total_reward: 108.0
Epoch: 130, Loss: 23.956602096557617, Total_reward: 99.0
Epoch: 140, Loss: 21.344772338867188, Total_reward: 119.0
Epoch: 150, Loss: 16.6451416015625, Total_reward: 173.0
Epoch: 160, Loss: 17.22072410583496, Total_reward: 191.0
Epoch: 170, Loss: 14.525527954101562, Total_reward: 199.0
Epoch: 180, Loss: 12.539002418518066, Total_reward: 178.0
Epoch: 190, Loss: 7.1446757316589355, Total_reward: 202.0
Epoch: 200, Loss: 18.16742515563965, Total_reward: 103.0
Epoch: 210, Loss: 26.098421096801758, Total_reward: 101.0
Epoch: 220, Loss: 23.369083404541016, Total_reward: 94.0
Epoch: 230, Loss: 25.93651580810547, Total_reward: 104.0
Epoch: 240, Loss: 14.64736557006836, Total_reward: 141.0
Epoch: 250, Loss: 13.854385375976562, Total_reward: 144.0
Epoch: 260, Loss: 18.603527069091797, Total_reward: 139.0
Epoch: 270, Loss: 17.686199188232422, Total_reward: 154.0
Epoch: 280, Loss: 14.523355484008789, Total_reward: 190.0
Epoch: 290, Loss: 16.576353073120117, Total_reward: 193.0
Average reward over 10 runs (invisible): 22.6
result : 22.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 200, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 20, 'update_epsilon_decay': 1}
Epoch: 10, Loss: 0.1781729906797409, Total_reward: 89.0
Epoch: 20, Loss: 0.21831616759300232, Total_reward: 90.0
Epoch: 30, Loss: 0.5254400968551636, Total_reward: 120.0
Epoch: 40, Loss: 0.6671816110610962, Total_reward: 143.0
Epoch: 50, Loss: 1.1877264976501465, Total_reward: 113.0
Epoch: 60, Loss: 1.3971461057662964, Total_reward: 142.0
Epoch: 70, Loss: 1.7171019315719604, Total_reward: 163.0
Epoch: 80, Loss: 2.01261568069458, Total_reward: 133.0
Epoch: 90, Loss: 1.59877347946167, Total_reward: 113.0
Epoch: 100, Loss: 0.7691527605056763, Total_reward: 124.0
Epoch: 110, Loss: 1.5201921463012695, Total_reward: 172.0
Epoch: 120, Loss: 2.5595569610595703, Total_reward: 165.0
Epoch: 130, Loss: 2.375526189804077, Total_reward: 122.0
Epoch: 140, Loss: 3.0085949897766113, Total_reward: 128.0
Epoch: 150, Loss: 7.199738502502441, Total_reward: 115.0
Epoch: 160, Loss: 4.027144432067871, Total_reward: 154.0
Epoch: 170, Loss: 2.847931385040283, Total_reward: 172.0
Epoch: 180, Loss: 3.996953010559082, Total_reward: 184.0
Epoch: 190, Loss: 3.284893035888672, Total_reward: 195.0
Epoch: 200, Loss: 3.773334264755249, Total_reward: 151.0
Epoch: 210, Loss: 3.0525457859039307, Total_reward: 160.0
Epoch: 220, Loss: 4.0871148109436035, Total_reward: 158.0
Epoch: 230, Loss: 5.329475402832031, Total_reward: 213.0
Epoch: 240, Loss: 3.896383285522461, Total_reward: 196.0
Epoch: 250, Loss: 8.118389129638672, Total_reward: 146.0
Epoch: 260, Loss: 6.649258136749268, Total_reward: 142.0
Epoch: 270, Loss: 6.1520514488220215, Total_reward: 176.0
Epoch: 280, Loss: 6.284187316894531, Total_reward: 194.0
Epoch: 290, Loss: 3.741150140762329, Total_reward: 235.0
Average reward over 10 runs (invisible): 9.6
result : 9.6
_________________
Testing with params: {'fixed_learning_rate': 0.01, 'epsilon': 0.01, 'batch_size': 128, 'max_action_by_epoch': 20, 'replay_memory_size': 500, 'increment_actions': 1, 'run_to_fill_replay': 300, 'max_epoch': 300, 'horizon': 1, 'update_q_step': 5, 'update_epsilon_decay': 1}
